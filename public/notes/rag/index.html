<!doctype html>
<html
  lang="en-us"
  dir="ltr"
>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<link rel="stylesheet" href="http://localhost:1313/css/styles.min.b9043231ddc756d7f5562ab1d7340f4281ad35df8c158a62ccfd66848c674e5e.css">
<meta charset="utf-8" />
<meta name="language" content="en" />
<meta name="viewport" content="width=device-width" />
<title>
    Fundamentals of Retrieval Augmented Generation (RAG) | Yuan Meng
</title>
  <meta name="description" content="If you ask 100 ML engineers about their career goals, 90 of them will say they want to work on LLMs someday. If you ask which part of LLMs they want to work on, probably 80 out of those 90 will say pretraining, post-training, or whatever is perceived as ‚Äúcore modeling work.‚Äù Quite likely they never could. The remaining 10 may or may not land jobs on applied research engineering teams at {OpenAI, Anthropic, xAI, GDM}, Perplexity, Glean, Anysphere, or the likes, building AI products (e.g., chatbots, web/enterprise search, etc.) that people commonly use. Retrieval-Augmented Generation (RAG) and, later, AI agents are top technologies behind popular AI applications." />
<meta property="og:url" content="http://localhost:1313/notes/rag/">
  <meta property="og:site_name" content="Yuan Meng">
  <meta property="og:title" content="Fundamentals of Retrieval Augmented Generation (RAG)">
  <meta property="og:description" content="If you ask 100 ML engineers about their career goals, 90 of them will say they want to work on LLMs someday. If you ask which part of LLMs they want to work on, probably 80 out of those 90 will say pretraining, post-training, or whatever is perceived as ‚Äúcore modeling work.‚Äù Quite likely they never could. The remaining 10 may or may not land jobs on applied research engineering teams at {OpenAI, Anthropic, xAI, GDM}, Perplexity, Glean, Anysphere, or the likes, building AI products (e.g., chatbots, web/enterprise search, etc.) that people commonly use. Retrieval-Augmented Generation (RAG) and, later, AI agents are top technologies behind popular AI applications.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-10-27T00:00:00+00:00">


  <meta itemprop="name" content="Fundamentals of Retrieval Augmented Generation (RAG)">
  <meta itemprop="description" content="If you ask 100 ML engineers about their career goals, 90 of them will say they want to work on LLMs someday. If you ask which part of LLMs they want to work on, probably 80 out of those 90 will say pretraining, post-training, or whatever is perceived as ‚Äúcore modeling work.‚Äù Quite likely they never could. The remaining 10 may or may not land jobs on applied research engineering teams at {OpenAI, Anthropic, xAI, GDM}, Perplexity, Glean, Anysphere, or the likes, building AI products (e.g., chatbots, web/enterprise search, etc.) that people commonly use. Retrieval-Augmented Generation (RAG) and, later, AI agents are top technologies behind popular AI applications.">
  <meta itemprop="datePublished" content="2025-10-27T00:00:00+00:00">
  <meta itemprop="wordCount" content="4347">
  <meta itemprop="keywords" content="Llm,Search,Rag">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Fundamentals of Retrieval Augmented Generation (RAG)">
  <meta name="twitter:description" content="If you ask 100 ML engineers about their career goals, 90 of them will say they want to work on LLMs someday. If you ask which part of LLMs they want to work on, probably 80 out of those 90 will say pretraining, post-training, or whatever is perceived as ‚Äúcore modeling work.‚Äù Quite likely they never could. The remaining 10 may or may not land jobs on applied research engineering teams at {OpenAI, Anthropic, xAI, GDM}, Perplexity, Glean, Anysphere, or the likes, building AI products (e.g., chatbots, web/enterprise search, etc.) that people commonly use. Retrieval-Augmented Generation (RAG) and, later, AI agents are top technologies behind popular AI applications.">

<link rel="canonical" href="http://localhost:1313/notes/rag/" />

    <link rel="stylesheet" href="/css/index.css" />
    <link rel="stylesheet" href="/css/stranger.css" />


      <script src="/js/main.js" defer></script>
  

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org/",
  "@id": "http://localhost:1313/notes/rag/",
  "@type": "BlogPosting",
  "articleSection": [
    "Llm",
    "Search",
    "Rag"
  ],
  "author": {
    "@type": "Person",
    "email": "mycaptainmy@gmail.com",
    "name": "Yuan Meng",
    "url": "http://localhost:1313/about/"
  },
  "copyrightNotice": "Yuan Meng",
  "datePublished": "2025-10-27",
  "description": "If you ask 100 ML engineers about their career goals, 90 of them will say they want to work on LLMs someday. If you ask which part of LLMs they want to work on, probably 80 out of those 90 will say pretraining, post-training, or whatever is perceived as ‚Äúcore modeling work.‚Äù Quite likely they never could. The remaining 10 may or may not land jobs on applied research engineering teams at {OpenAI, Anthropic, xAI, GDM}, Perplexity, Glean, Anysphere, or the likes, building AI products (e.g., chatbots, web/enterprise search, etc.) that people commonly use. Retrieval-Augmented Generation (RAG) and, later, AI agents are top technologies behind popular AI applications.",
  "headline": "Fundamentals of Retrieval Augmented Generation (RAG)",
  "isPartOf": {
    "@id": "http://localhost:1313/notes/",
    "@type": "Blog",
    "name": "Notes"
  },
  "mainEntityOfPage": "http://localhost:1313/notes/rag/",
  "name": "Fundamentals of Retrieval Augmented Generation (RAG)",
  "timeRequired": "PT21M",
  "url": "http://localhost:1313/notes/rag/",
  "wordCount": 4347
}
</script>


  </head>
  <body>
    <div class="container mx-auto flex max-w-prose flex-col space-y-10 p-4 md:p-6">
      <header class="flex flex-row items-center justify-between">
        <div>
  <a id="skip-nav" class="sr-only" href="#maincontent">Skip to main content</a>
  <a class="font-semibold" href="/">Yuan Meng</a>
</div>

  <nav>
    <ul class="flex flex-row items-center justify-end space-x-4">
    <li>
      <a href="/about/">About</a
      >
    </li>
    <li>
      <a href="/posts/">Posts</a
      >
    </li>
    <li>
      <a aria-current="true" class="ancestor" href="/notes/">Notes</a
      >
    </li>
    </ul>
  </nav>


      </header>
      <main class="prose prose-slate relative md:prose-lg prose-h1:text-[2em]" id="maincontent">
        <article class="main">
    <header>
      <h1 class="!mb-1">Fundamentals of Retrieval Augmented Generation (RAG)</h1><div class="flex flex-row items-center space-x-4">
          <time class="text-sm italic opacity-80" datetime="2025-10-27T00:00:00&#43;00:00">October 27, 2025</time>
        </div>
    </header>

    
    
      Reading time: 21 minutes
    

    
    
      <div class="toc-container">
        <span id="toc-toggle">
          <span id="toc-icon">‚ñ∂</span> 
          <span>Table of Contents</span>
        </span>
        <nav id="TableOfContents" class="toc-content">
          <nav id="TableOfContents">
  <ul>
    <li><a href="#the-structure-of-a-rag">The Structure of a RAG</a></li>
    <li><a href="#before-a-search-is-issued">Before a Search Is Issued</a>
      <ul>
        <li><a href="#indexing">Indexing</a>
          <ul>
            <li><a href="#chunk-raw-documents">Chunk Raw Documents</a></li>
            <li><a href="#store-embeddings-in-vector-databases">Store Embeddings in Vector Databases</a></li>
          </ul>
        </li>
        <li><a href="#query-analysis">Query Analysis</a>
          <ul>
            <li><a href="#query-rewriting">Query Rewriting</a></li>
            <li><a href="#query-construction">Query Construction</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#retriever">Retriever</a>
      <ul>
        <li><a href="#hybrid-search">Hybrid Search</a>
          <ul>
            <li><a href="#lexical-retrieval">Lexical Retrieval</a></li>
            <li><a href="#embedding-based-retrieval">Embedding-Based Retrieval</a></li>
            <li><a href="#metadata-filtering">Metadata Filtering</a></li>
          </ul>
        </li>
        <li><a href="#result-fusion">Result Fusion</a></li>
        <li><a href="#reranking">Reranking</a></li>
        <li><a href="#retriever-evaluation">Retriever Evaluation</a></li>
      </ul>
    </li>
    <li><a href="#generation">Generation</a>
      <ul>
        <li><a href="#prompt-template">Prompt Template</a></li>
        <li><a href="#pick-an-llm">Pick an LLM</a></li>
        <li><a href="#decoding-strategies">Decoding Strategies</a></li>
        <li><a href="#generation-evaluation">Generation Evaluation</a></li>
      </ul>
    </li>
    <li><a href="#fine-tuning">Fine-Tuning</a>
      <ul>
        <li><a href="#embeddings-for-retrieval">Embeddings for Retrieval</a></li>
        <li><a href="#llms-for-generation">LLMs for Generation</a></li>
      </ul>
    </li>
    <li><a href="#put-rag-in-production">Put RAG in Production</a></li>
    <li><a href="#learn-more">Learn More</a></li>
  </ul>
</nav>
        </nav>
      </div>

      <script>
        
        document.addEventListener('DOMContentLoaded', function () {
          var tocToggle = document.getElementById('toc-toggle');
          var tocContent = document.getElementById('TableOfContents');
          var tocIcon = document.getElementById('toc-icon');
          tocToggle.addEventListener('click', function () {
            if (tocContent.style.display === 'none' || tocContent.style.display === '') {
              tocContent.style.display = 'block';
              tocIcon.textContent = '‚ñº'; 
            } else {
              tocContent.style.display = 'none';
              tocIcon.textContent = '‚ñ∂'; 
            }
          });
        });
      </script>
    

    
    <div class="content">
      <p>If you ask 100 ML engineers about their career goals, 90 of them will say they want to work on LLMs someday. If you ask which part of LLMs they want to work on, probably 80 out of those 90 will say pretraining, post-training, or whatever is perceived as &ldquo;core modeling work.&rdquo; Quite likely they never could. The remaining 10 may or may not land jobs on <span style="background-color: #D9CEFF">applied research engineering</span> teams at <code>{OpenAI, Anthropic, xAI, GDM}</code>, Perplexity, Glean, Anysphere, or the likes, building AI products (e.g., chatbots, web/enterprise search, etc.) that people commonly use. <a href="https://python.langchain.com/docs/concepts/rag/">Retrieval-Augmented Generation</a> (RAG) and, later, <a href="https://lilianweng.github.io/posts/2023-06-23-agent/">AI agents</a> are top technologies behind popular AI applications.</p>
<p>In this post, I&rsquo;ll summarize RAG fundamentals from LangChain&rsquo;s RAG from Scratch (<a href="https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x">YouTube</a>, tutorials <a href="https://python.langchain.com/docs/tutorials/rag/">1</a> &amp; <a href="https://python.langchain.com/docs/tutorials/qa_chat_history/">2</a>, <a href="https://github.com/langchain-ai/rag-from-scratch">repo</a>), DeepLearning.AI&rsquo;s <a href="https://www.coursera.org/learn/retrieval-augmented-generation-rag">RAG course</a>, and some classic or recent RAG + Agentic Search papers.</p>
<h2 id="the-structure-of-a-rag" class="scroll-mt-8 group">
  The Structure of a RAG
  
    <a href="#the-structure-of-a-rag"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p><em>Why RAG?</em> Out of the box, an LLM isn&rsquo;t well-suited to generate information it didn&rsquo;t have access to during training&mdash;such as recent events, a company&rsquo;s proprietary knowledge base, personal data, and so on. RAG retrieves missing information the LLM needs in order to generate accurate, up-to-date, and context-aware responses.</p>
<figure><img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F45603646e979c62349ce27744a940abf30200d57-3840x2160.png&amp;w=3840&amp;q=75"
    alt="A standard RAG system (source: Anthropic)." width="1800"><figcaption>
      <p>A standard RAG system (source: <a href="https://www.anthropic.com/engineering/contextual-retrieval">Anthropic</a>).</p>
    </figcaption>
</figure>

<ul>
<li><strong>Retriever</strong>: The query is first routed to the retriever, which retrieves relevant documents from the knowledge base and ranks them relative to the prompt to select the top-k.</li>
<li><strong>Generation</strong>: The retrieved documents are used to augment the prompt, which is then sent to an LLM to generate a response.</li>
</ul>
<p>The system could be more complex&mdash;for instance, we can use a routing LLM to decide whether to skip the retriever and generate a direct LLM response. The first paper from the much hyped Meta Superintelligence Lab is a RAG paper called <em>REFRAG: Rethinking RAG based Decoding</em> (<a href="https://arxiv.org/abs/2509.01092">Lin et al., 2025</a>), which aims to make RAG cheaper and faster. This shows how central RAG is to LLM applications.</p>
<h2 id="before-a-search-is-issued" class="scroll-mt-8 group">
  Before a Search Is Issued
  
    <a href="#before-a-search-is-issued"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p>In an <a href="/posts/ltr/" class="backlink">old post</a>
  
  , I wrote about how traditional search engines work. Before we could issue any search, we need to index all documents so later they could be easily found. When a query comes in, we might need to transform it somehow (e.g., autocompletion, spell checking, intent classification, rewriting) to optimize search results. It turns out that in LLM-based search systems such as RAG, indexing and query analysis are still instrumental to search engine performance.</p>
<h3 id="indexing" class="scroll-mt-8 group">
  Indexing
  
    <a href="#indexing"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>From what kind of databases are we retrieving information? In case of natural language documents, we usually store them in two formats:</p>
<ul>
<li><strong><a href="https://www.geeksforgeeks.org/dbms/inverted-index/">Inverted index</a></strong> (for keyword-based search): Analyze documents into tokens (e.g., ElasticSearch <a href="https://www.elastic.co/docs/manage-data/data-store/text-analysis/anatomy-of-an-analyzer">analyzer</a>) üëâ for each token, store a list of documents in which it appears (like the end of textbooks)</li>
<li><strong>Vector database</strong> (for embedding-based retrieval): Chunk documents into meaningful pieces üëâ for each chunk, use an LLM to create a dense embedding and we store its embedding</li>
</ul>
<p>For structured data, we can store them in relational (e.g., Snowflake, MySQL) or graph (e.g., Neo4j) databases and use GQL or SQL to query.</p>
<p>For traditional search engine indexing, I highly recommend the <a href="https://www.oreilly.com/library/view/relevant-search/9781617292774/">Relevant Search</a> book. Amazing how nothing beats its comprehensiveness, when it still uses Python 2.7. Below we&rsquo;ll focus on more &ldquo;fashionable&rdquo; techniques designed for vector databases.</p>
<h4 id="chunk-raw-documents" class="scroll-mt-8 group">
  Chunk Raw Documents
  
    <a href="#chunk-raw-documents"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<blockquote>
<p>As a rule of thumb, if the chunk of text makes sense without the surrounding context to a human, it will make sense to the language model as well. &mdash; <a href="https://www.pinecone.io/learn/chunking-strategies/"><em>Chunking Strategies for LLM Applications</em></a> by Pinecone</p>
</blockquote>
<p>If a document is a book, for instance, it would be too coarse to represent the entire book as a single embedding.</p>
<ul>
<li><strong>Lack granularity</strong>: It&rsquo;d be hard to retrieve specific topics, chapters, or pages from the book. Important information may get lost in a sea of irrelevant information.</li>
<li><strong>Context window explosion</strong>: Even if we deem the whole book as relevant and successfully retrieve it, it will blow up the LLM context window when we plug it into the prompt.</li>
</ul>
<p>We can chunk documents into smaller pieces. But how? How about we split the book into words? Good luck knowing what <code>dog</code> or <code>the</code> means without context. Chunking is critical for a RAG system&rsquo;s performance. Below are common techniques for text inputs:</p>
<ul>
<li><strong>Fixed-size chunking</strong>: We can split the document into chunks containing the same number of tokens (e.g., 250). It&rsquo;s best to have token overlaps between adjacent chunks so the context flows.</li>
<li><strong>Recursive character splitting</strong>: We can split the document on one (<code>&quot;.&quot;</code>) or several special characters (e.g., <code>[&quot;\n\n&quot;, &quot;\n&quot;, &quot; &quot;, &quot;&quot;]</code> in LangChain&rsquo;s <a href="https://python.langchain.com/docs/how_to/recursive_text_splitter/">RecursiveCharacterTextSplitter</a>). Chunks may end on more natural places, not the middle of sentences.</li>
<li><strong>Semantic chunking</strong>: We can start a chunk with a sentence &mdash; if the next sentence has high cosine similarity with the current sentence, include it in the chunk; otherwise, start a new chunk.</li>
<li><strong>LLM-based chunking</strong>: We can prompt an LLM to create chunks from a document. Nowaways, this method is becoming increasingly more cost-effective and widely used.</li>
<li><strong>Context-aware chunking</strong>: This is orthogonal to above methods &mdash; regardless of how we created a chunk, we can use an LLM to add context to it &mdash; Anthropic has a <a href="https://www.anthropic.com/engineering/contextual-retrieval">great article</a> on context retrieval. A good example is, on its own, the chunk <code>&quot;The company's revenue grew by 3% over the previous quarter.&quot;</code> seems meaningless when a user searches for a specific company&rsquo;s earnings. After all, what company is it even talking about? üòÇ To solve this problem, we could use an LLM to contextualize it into <code>&quot;This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter.&quot;</code>. With costs of LLMs going down, this method has become more cost-effective today.</li>
</ul>
<p>Things can get more complicated when the document is a PDF, image, Markdown/LaTeX/HTML/DOCX file, or code file, as special chunking techniques are needed to preserve structural information. Check out Pinecone&rsquo;s <a href="https://www.pinecone.io/learn/chunking-strategies/">blogpost</a> for more details. TL;DR: different languages have their own structural symbols&mdash;e.g., headers and body text may appear in different parts of a PDF page, HTML uses tags to mark sections, and Markdown and LaTeX have their own section organizations. You can customize your splitter based on language-specific symbols.</p>
<h4 id="store-embeddings-in-vector-databases" class="scroll-mt-8 group">
  Store Embeddings in Vector Databases
  
    <a href="#store-embeddings-in-vector-databases"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<p>We can use an embedding model (pretrained or in-house models trained using contrastive objectives) to generate embeddings for chunks we created and store them in a vector database. If storage cost is a concern, we can apply quantization to condense raw embeddings at the element level (e.g., map each float element into an int8 or even an int4 representation) or the vector level (e.g., map each dense vector into a binary vector via vector or product quantization).</p>
<p>In some cases, quantization can speed up top-k retrieval&mdash;e.g., if queries and documents are binary vectors, we can use Hamming distance instead of inner product to select the top k, reducing the complexity of each comparison from $O(d)$ to $O(1)$, where $d$ is the vector dimension. (I first read about this trick in an <a href="https://arxiv.org/pdf/2108.04468">Alibaba paper</a>, but I remember it was based on a LinkedIn paper whose title I forgot.)</p>
<p>See my EBR <a href="/posts/ebr/" class="backlink">blogpost</a>
  
   for more details on storage optimization.</p>
<h3 id="query-analysis" class="scroll-mt-8 group">
  Query Analysis
  
    <a href="#query-analysis"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Raw inputs are typically natural language queries from the user. Depending on where we query data from, we may need to <strong>construct</strong> natural language queries into SQL (for relational databases) or GQL (for graph databases) for structured databases, or <strong>rewrite</strong> the query to improve natural language documents that we could return.</p>
<h4 id="query-rewriting" class="scroll-mt-8 group">
  Query Rewriting
  
    <a href="#query-rewriting"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<p>I remember back in 2022 as a new grad, my onboarding project at DoorDash was on query expansion &mdash; at that time, Starbucks wasn&rsquo;t on DoorDash so users searching for <code>starbucks</code> would see null results. However, if they were sleepy and needed coffee, wouldn&rsquo;t they want to see other coffee shops? Mapping a query to a type of stores to expand search results is a form of query expansion. LangChain has an awesome <a href="https://python.langchain.com/docs/concepts/retrieval/">post</a> that summarizes many forms of query rewriting:</p>
<ul>
<li><strong>Query clarification</strong>: Rephrase ambiguous, poorly worded, or otherwise confusing queries for clarity
<ul>
<li>E.g., a patient went on a tangent to describe how they hurt themself on a skiing trip starting from booking an Airbnb and getting to the resort; in the end, they asked whether they should worry about the symptoms üëâ you could just extract the symptoms and search them in a medical database</li>
</ul>
</li>
<li><strong>Semantic understanding</strong>: Identify the query intent (e.g., shopping or finding information), so we could go beyond literal matching</li>
<li><strong>Query expansion</strong>: Generate related terms or concepts to broaden the search scope, such as from <code>starbucks</code> to <code>coffee shops</code> or <code>breakfast</code> &mdash; useful when Recall is very low</li>
<li><strong>Complex query handling</strong>: Break down multi-part queries into simpler sub-queries &mdash; examples techniques including
<ul>
<li><strong>Multi-query</strong>: Rewrite a compound query into several simple sub-queries üëâ retrieve documents for each sub-query</li>
<li><strong>Decomposition</strong>: Decompose a complex problem into subproblems (e.g., the user asks the LLM to debug an issue and there are 5 things to investigate) üëâ retrieve documents to help resolve each subproblem; combine the solution to the previous problem and retrieved information for the current problem to resolve the current problem</li>
<li><strong>Step-back</strong> (<a href="https://arxiv.org/abs/2310.06117">Zheng et al., 2024</a>): Ask the LLM to derive high-level concepts and first principles from detailed examples and use those concepts and principles to guide reasoning</li>
<li><strong>HyDE</strong> (<a href="https://arxiv.org/abs/2212.10496">Gao et al., 2022</a>): Convert a hard-to-handle query into hypothetical documents and use hypothetical document embeddings (HyDE) to retrieve real documents</li>
</ul>
</li>
</ul>
<p>Which method to use depends on how the system underperforms (e.g., low recall? low precision? too much detail? too little detail?&hellip;). We can control the method by tweaking the system prompt.</p>
<h4 id="query-construction" class="scroll-mt-8 group">
  Query Construction
  
    <a href="#query-construction"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<p>If we need to query from a relational or a graph database, we need to translate a natural language query into a structured query language. There are many <a href="https://python.langchain.com/docs/tutorials/sql_qa/">text-to-SQL</a> and <a href="https://python.langchain.com/docs/tutorials/graph/">text-to-Cypher</a> (Cypher is a graph query language) models at your disposal. You can also create metadata filters from the query (e.g., title, author, year, region, language).</p>
<h2 id="retriever" class="scroll-mt-8 group">
  Retriever
  
    <a href="#retriever"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="hybrid-search" class="scroll-mt-8 group">
  Hybrid Search
  
    <a href="#hybrid-search"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<figure><img src="https://www.dropbox.com/scl/fi/m9ts8k5sgebb7cacmvaec/Screenshot-2025-10-24-at-7.20.39-PM.png?rlkey=4mj913dxxuvn09jkufp8gcp4e&amp;st=thy25rnj&amp;raw=1"
    alt="An illustration of hybrid search (source: DeepLearning.AI RAG course)." width="1800"><figcaption>
      <p>An illustration of hybrid search (source: <a href="https://learn.deeplearning.ai/courses/retrieval-augmented-generation/information">DeepLearning.AI RAG</a> course).</p>
    </figcaption>
</figure>

<h4 id="lexical-retrieval" class="scroll-mt-8 group">
  Lexical Retrieval
  
    <a href="#lexical-retrieval"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<p>Traditional search engines &ldquo;know&rdquo; a document is relevant if many query tokens appear in it. The two most famous algorithms for computing query-document lexical similarity are <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> and <a href="https://en.wikipedia.org/wiki/Okapi_BM25">BM25</a>.</p>
<p>Below is a toy TF-IDF retriever implementation based on this formula:</p>
<p>$${\displaystyle (1+\log f_{t,d})\cdot \log {\frac {N}{n_{t}}}}.$$</p>
<figure class="codeblock not-prose relative scroll-mt-8" id="codeblock-01">
  <aside
    class="absolute right-0 top-0 hidden rounded-bl-sm rounded-tr-sm bg-white/10 px-2 py-1 text-white/70 transition-opacity md:inline-block"
  >
    <div class="codeblock-meta flex max-w-xs flex-row items-center space-x-3">
      <div class="small-caps shrink cursor-default truncate font-mono text-xs" aria-hidden="true">
        <span class="relative">python3</span>
      </div>
      <div>
        <clipboard-copy
          type="button"
          aria-label="Copy code to clipboard"
          title="Copy code to clipboard"
          class="block cursor-pointer transition-colors hover:text-sky-400"
          target="#codeblock-01 code"
        >
          <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-width="2"
  stroke-linecap="round"
  stroke-linejoin="round"
  class="lucide lucide-clipboard h-4 w-4"
  viewBox="0 0 24 24"
>
  <rect width="8" height="4" x="8" y="2" rx="1" ry="1" />
  <path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2" />
</svg>

        </clipboard-copy>
      </div>
      <div>
        <a
          href="#codeblock-01"
          class="block"
          aria-label="Link to this code block"
          title="Link to this code block"
        >
          <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

        </a>
      </div>
    </div>
  </aside>
  <p class="sr-only">python3 code snippet start</p>
  <div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python3" data-lang="python3"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">heapq</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TFIDFRetriver</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;process doc corpus and precompute tf-idf of each doc&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">docs</span> <span class="o">=</span> <span class="n">docs</span>  <span class="c1"># remember original docs</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>  <span class="c1"># number of docs</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># document frequency: in how many DISTINCT docs does term appear?</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># {term : # of unique docs}</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>  <span class="c1"># track if term has already been counted in this doc</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">term</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">[</span><span class="n">term</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">term</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># inverse document frequency: log(N / n_t)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># n_t is df[term]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">idf</span> <span class="o">=</span> <span class="p">{</span><span class="n">term</span><span class="p">:</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">/</span> <span class="n">df_val</span><span class="p">)</span> <span class="k">for</span> <span class="n">term</span><span class="p">,</span> <span class="n">df_val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># {term : index}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">stoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">term</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">term</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idf</span><span class="o">.</span><span class="n">keys</span><span class="p">())}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stoi</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># precompute each document&#39;s tf-idf vector using (1 + log f_{t,d}) * idf</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">doc_tfidf</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">docs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">vec</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># raw term frequency in this doc</span>
</span></span><span class="line"><span class="cl">            <span class="n">tf</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="n">tf</span><span class="p">[</span><span class="n">term</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># fill tf-idf weights</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">term</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">term</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stoi</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">term</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># (1 + log f_{t,d})</span>
</span></span><span class="line"><span class="cl">                <span class="n">tf_weight</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="n">vec</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf_weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">idf</span><span class="p">[</span><span class="n">term</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">doc_tfidf</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">vec</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_query_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;vectorize query into tf-idf vector with same formula&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">vec</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># raw term frequency in query</span>
</span></span><span class="line"><span class="cl">        <span class="n">tf</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">query</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">tf</span><span class="p">[</span><span class="n">term</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">term</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">term</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stoi</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">continue</span>
</span></span><span class="line"><span class="cl">            <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">term</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># (1 + log f_{t,d}) where d is &#34;the query bag&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="n">tf_weight</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">vec</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf_weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">idf</span><span class="p">[</span><span class="n">term</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">vec</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_dot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;compute dot product between 2 vectors&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">total</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">total</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">return_doc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;return top-k documents with highest tf-idf scores&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">query_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_query_vector</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">heap</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># min-heap of (score, doc_idx)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">doc_idx</span><span class="p">,</span> <span class="n">doc_vec</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">doc_tfidf</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dot</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">doc_vec</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">heapq</span><span class="o">.</span><span class="n">heappush</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">doc_idx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">topk</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">heapq</span><span class="o">.</span><span class="n">heappop</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># sort from best to worst</span>
</span></span><span class="line"><span class="cl">        <span class="n">heap</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_idx</span> <span class="ow">in</span> <span class="n">heap</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="n">doc_idx</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">results</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># test case</span>
</span></span><span class="line"><span class="cl"><span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;the cat sat on the mat&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;the dog sat on the log&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;the cat chased the mouse&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;dogs and cats can be friends&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">query</span> <span class="o">=</span> <span class="s2">&#34;cat sat on mat&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tfidf_retriver</span> <span class="o">=</span> <span class="n">TFIDFRetriver</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">results</span> <span class="o">=</span> <span class="n">tfidf_retriver</span><span class="o">.</span><span class="n">return_doc</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># printed results: [(3.3631710974274096, &#39;the cat sat on the mat&#39;), (0.9609060278364028, &#39;the dog sat on the log&#39;)]</span></span></span></code></pre></div>
  <p class="sr-only">python3 code snippet end</p>

  
</figure>
<p>TF-IDF suffers from a lack of document length normalization (e.g., longer documents tend to include the same query token more times) and term frequency saturation (e.g., a document with 20 &ldquo;pizza&rdquo; matches is seen as 2x as relevant as one with 10 &ldquo;pizza&rdquo; matches). A convoluted algorithm, BM25, was created to fix these issues:</p>
<p>$${\displaystyle {\text{score}}(D,Q)=\sum_{i=1}^{n}{\text{IDF}}(q_{i})\cdot {\frac {f(q_{i},D)\cdot (k_{1}+1)}{f(q_{i},D)+k_{1}\cdot \left(1-b+b\cdot {\frac {|D|}{\text{avgdl}}}\right)}}}.$$</p>
<p>The issue with lexical retrieval is, if I search <code>Taylor Swift husband</code>, I may not see Travis Kelce üå≤ results, since there are no term matches. Embedding-based retrieval comes to our rescue.</p>
<h4 id="embedding-based-retrieval" class="scroll-mt-8 group">
  Embedding-Based Retrieval
  
    <a href="#embedding-based-retrieval"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<p>We could use a pretrained LLM to embed query and documents, or we could train our own query and document embeddings. Below are common architectures for learning query and document embeddings:</p>
<figure><img src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/Bi_vs_Cross-Encoder.png"
    alt="Bi-Encoder vs. Cross-Encoder (source: Sentence Transformers doc." width="1800"><figcaption>
      <p>Bi-Encoder vs. Cross-Encoder (source: Sentence Transformers <a href="https://sbert.net/examples/cross_encoder/applications/README.html">doc</a>.</p>
    </figcaption>
</figure>

<ul>
<li><strong>Bi-encoder</strong>: Embed query and doc separately using two identical towers üëâ compute cosine similarity between outputs.</li>
<li><strong>Cross-encoder</strong>: Concatenate query and document and embed the concatenated input üëâ feed the output to a binary classification head to predict a <code>[0, 1]</code> relevance score.</li>
<li><strong>ColBERT</strong>: Bi-encoder doesn&rsquo;t have any query-document interactions whereas cross-encoder performs an early fusion from the start; as a result, bi-encoder is fast but has low quality while cross-encoder has good quality but is slow üëâ to enjoy the best of both worlds, <a href="https://arxiv.org/abs/2004.12832">ColBERT</a> performs a late fusion via MaxSum: for each query token, compute cosine similarity with each document, only keep the maximum score, and sum the scores over all query tokens to get the final relevance score.</li>
</ul>
<figure><img src="https://www.dropbox.com/scl/fi/w5ydcd7zfm4d0tl0s4ryu/Screenshot-2025-10-25-at-5.02.19-PM.png?rlkey=uig147bizg5wvb6bz5aukqq2u&amp;st=vk9p8zx7&amp;raw=1"
    alt="Architecture of ColBERT with late query-document fusion." width="1800"><figcaption>
      <p>Architecture of ColBERT with late query-document fusion.</p>
    </figcaption>
</figure>

<p>Due to its inefficiency, rarely would we use the cross-encoder for retrieval. Moreover, since cross-encoders concatenate queries and documents before embedding, we can&rsquo;t precompute document embeddings offline, which is a hard requirement for low latency. Bi-encoders and ColBERT are therefore good choices for retrieval.</p>
<p>The brute-force way is, for a query, compute its similarity (e.g., inner product, cosine) with each document and return top k. This approach scales poorly with a large document corpus. In practice, people usually use approximate nearest neighbor (ANN) search to speed up search. To learn more, you can read the <a href="https://www.yuan-meng.com/posts/ebr/#approximate-retrieval-algorithms">ANN part</a> in my blogpost, and a much more comprehensive summary in Lilian Weng&rsquo;s <a href="https://lilianweng.github.io/posts/2023-06-23-agent/#maximum-inner-product-search-mips">blogpost</a>. TL;DR is we need to reduce search space by:</p>
<ul>
<li><strong>Locality sensitive hash</strong>: Hash each vector into multiple buckets üëâ only search in buckets where the query is assigned to</li>
<li><strong>Graph traversal</strong> (e.g., HNSW): Performs random walks on the document graph, hoping to minimize query-document distances</li>
<li><strong>Clustering</strong> (e.g., FAISS): Cluster documents based on their embeddings and only search in clusters where the query is in</li>
</ul>
<h4 id="metadata-filtering" class="scroll-mt-8 group">
  Metadata Filtering
  
    <a href="#metadata-filtering"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<p>We can further filter retrieved results based on metadata, such as title, author, creation data, region, etc.. This is not a true retrieval method, but an approach to purge unwanted or irrelevant retrieved results.</p>
<h3 id="result-fusion" class="scroll-mt-8 group">
  Result Fusion
  
    <a href="#result-fusion"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>We can combine results retrieved from different sources using methods such as the <a href="https://opensearch.org/blog/introducing-reciprocal-rank-fusion-hybrid-search/">reciprocal rank fusion (RRF)</a>.</p>
<p>There are many knobs we can tune when combing different resources.</p>
<ul>
<li><strong>The &ldquo;k&rdquo; parameter in RRF</strong>: RRF rewards documents highly ranked in each list&mdash;$\frac{1}{k + \mathrm{rank_1}} + \frac{1}{k + \mathrm{rank_2}} + \frac{1}{k + \ldots + \mathrm{rank_n}}$, where $\mathrm{rank_i}$ is a document&rsquo;s ranking in the $i$th list. The greater the k, the less a single high ranking matters. RRF cares about rankings, not scores.</li>
<li><strong>Weight of keyword vs. semantic search</strong>: Based on your understanding of the &ldquo;ideal&rdquo; system, you can make keyword matches more important than EBR, or the reverse. You can also tune the weights based on downstream evaluation results.</li>
</ul>
<h3 id="reranking" class="scroll-mt-8 group">
  Reranking
  
    <a href="#reranking"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>After selecting documents with highest combined scores, we need to rerank them by relevance and further cap the result length. At this stage, we can use the more expensive but performance cross-encoder.</p>
<h3 id="retriever-evaluation" class="scroll-mt-8 group">
  Retriever Evaluation
  
    <a href="#retriever-evaluation"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Results from the retriever can be evaluated end to end (e.g., whether the final generation is &ldquo;good&rdquo;, whatever that means) or on their own. We need the latter for debugging and logging. When evaluating the retriever alone, please read the <a href="https://www.yuan-meng.com/posts/ltr/#what-is-the-right-order">evaluation section</a> in my LTR post:</p>
<ul>
<li><strong>Rank-unaware metrics</strong>: Recall@k, Precision@k</li>
<li><strong>Rank-aware metrics</strong>: nDCG@k (how much top k differs from &ldquo;ideal ranking&rdquo;), MRR (how early the first relevant result appears), MAP@k (how much relevant results are concentrated at the top)</li>
</ul>
<h2 id="generation" class="scroll-mt-8 group">
  Generation
  
    <a href="#generation"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p>In traditional search, top k documents are returned to the user on a search result page. In RAG, however, they are plugged into a prompt to an LLM and it&rsquo;s the LLM that returns the user-facing response.</p>
<h3 id="prompt-template" class="scroll-mt-8 group">
  Prompt Template
  
    <a href="#prompt-template"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>A prompt typically consists of the following components:</p>
<ul>
<li><strong>System prompt</strong>: Instructions on how an LLM should behave
<ul>
<li><strong>Role</strong>: It controls the desired tone and personality of the LLM as well as what procedures it should follow
<ul>
<li>For RAG, we can ask the LLM to (1) only use retrieved documents to answer, (2) judge whether a document is relevant, and (3) cite sources in responses</li>
</ul>
</li>
<li><strong>Be careful about the token usage!</strong> System prompts are added to every prompt &mdash; think twice before using in-context learning (i.e., providing one or few examples); in-context learning can even be detrimental to reasoning models</li>
</ul>
</li>
<li><strong>Conversation history</strong>: Multi-turn conversations require the LLM need to remember previous conversations (i.e., past user messages and assistant responses); we can trim older messages no longer needed, a practice called &ldquo;context pruning&rdquo;</li>
<li><strong>Retrieved information</strong>: The content of retrieved documents (clean up or summarize, if need be) and their sources and metadata</li>
<li><strong>User prompt</strong>: The user&rsquo;s query with the question they want to ask</li>
</ul>
<h3 id="pick-an-llm" class="scroll-mt-8 group">
  Pick an LLM
  
    <a href="#pick-an-llm"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Which LLM to use depends on your task. The LLM Arena <a href="https://lmarena.ai/leaderboard">leaderboard</a> ranks models by performance in each domain (e.g., coding, writing, multimodal). When choosing an LLM, compare these key specs:</p>
<ul>
<li><strong>Context window</strong>: How much text the model can process at once. Bigger is better for long documents or multi-turn conversations.</li>
<li><strong>Training cutoff</strong>: The most recent date of data the model was trained on. Newer is better for up-to-date knowledge.</li>
<li><strong>Time to first token</strong> (TTFT): Delay before the model starts responding. Lower is better for responsiveness.</li>
<li><strong>Tokens per second</strong> (TPS): How fast the model generates text. Higher is better for throughput.</li>
</ul>
<h3 id="decoding-strategies" class="scroll-mt-8 group">
  Decoding Strategies
  
    <a href="#decoding-strategies"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Whichever LLM you choose, you can usually adjust its decoding strategy to control how it generates responses &mdash;</p>
<ul>
<li><strong>Greedy decoding</strong>: Always pick the most likely next token. Best for deterministic tasks (e.g., math, code), but gets repetitive or dull.</li>
<li><strong>Top-k sampling</strong>: Sample from top k tokens. Good for creative writing or brainstorming where you want controlled randomness.</li>
<li><strong>Top-p (nucleus) sampling</strong>: Sample from the smallest set of tokens whose cumulative probability exceeds p. Smoother and more adaptive than top-k; ideal for natural dialogue or storytelling.</li>
</ul>
<p>You can also tweak token logits to change their probabilities:</p>
<ul>
<li><strong>Repetition penalty</strong>: Reduces the probability of tokens already generated. Prevents loops and redundancy in long outputs.</li>
<li><strong>Logit bias</strong>: Manually adjust the probability of specific tokens. Useful for steering style or enforcing constraints (e.g., suppressing profanity, forcing certain keywords).</li>
<li><strong>Temperature</strong>: Lower = more deterministic, higher = more diverse. Use low temperature for precision, high for creativity.</li>
</ul>
<h3 id="generation-evaluation" class="scroll-mt-8 group">
  Generation Evaluation
  
    <a href="#generation-evaluation"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>At a high level, the responsibility of a RAG system is to incorporate relevant information found by the retriever and discard irrelevant information in order to generate <strong>relevant</strong>, <strong>faithful</strong> responses.</p>
<p>In the early days, the RAG triad was a popular evaluation framework:</p>
<ul>
<li><strong>Answer relevance</strong>: Whether the response is relevant to the query</li>
<li><strong>Context relevance</strong>: Whether context is relevant to the query</li>
<li><strong>Groundedness</strong>: Whether the response is supported by context</li>
</ul>
<figure><img src="https://truera.com/wp-content/uploads/2024/03/TruEra-The-Rag-Triad-1.png"
    alt="The RAG triad for evaluating relevance (source: TruEra)." width="600"><figcaption>
      <p>The RAG triad for evaluating relevance (source: <a href="https://truera.com/ai-quality-education/generative-ai-rags/what-is-the-rag-triad/">TruEra</a>).</p>
    </figcaption>
</figure>

<p>RAG evaluation has become more comprehensive now. For instance, in addition to the triad, LlamaIndex also <a href="https://developers.llamaindex.ai/python/framework/module_guides/evaluating/">outlined</a> several other metrics:</p>
<ul>
<li><strong>Correctness</strong>: Whether the generated answer matches that of the reference answer given the query (requires labels).</li>
<li><strong>Semantic similarity</strong>: Whether the predicted answer is semantically similar to the reference answer (requires labels).</li>
<li><strong>Faithfulness</strong>: Evaluates if the answer is faithful to the retrieved contexts (in other words, whether if there&rsquo;s hallucination).
<ul>
<li><em>Tip</em>: To reduce hallucination, prompt the LLM to cite sources. To detect hallucination automatically, compare multiple generations ‚Äî factual claims remain stable, while hallucinations tend to vary.</li>
</ul>
</li>
<li><strong>Guideline adherence</strong>: Whether the predicted answer adheres to specific guidelines.</li>
</ul>
<p>Judging correctness and semantic similarity typically requires human evaluation, since a human needs to provide a reference answer to the query and compare it with the LLM-generate response.</p>
<p>For other metrics, LLM-as-a-judge is a scalable alternative: We can use another LLM to evaluate the relevance (answer + context), groundedness, and faithfulness of LLM-generated responses. Be aware of biases: A model usually prefers other models in its family!</p>
<h2 id="fine-tuning" class="scroll-mt-8 group">
  Fine-Tuning
  
    <a href="#fine-tuning"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="embeddings-for-retrieval" class="scroll-mt-8 group">
  Embeddings for Retrieval
  
    <a href="#embeddings-for-retrieval"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Quite likely an out-of-box embedding model doesn&rsquo;t understand your documents and queries well, in which case we can fine-tune embeddings for better retrieval. For this kind of &ldquo;metric learning&rdquo; problems, a typical solution is to find positive query-document pairs, sample negative pairs, and do some sort of contrastive representation learning (see Lilian Weng&rsquo;s <a href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">blogpost</a> for a comprehensive review).</p>
<p>In recommender systems, positive samples come free (e.g., engagements you wish to predict, like clicks or conversions) and negatives can be sampled (see <a href="/posts/ltr/" class="backlink">my post</a>
  
  ) from the same batch (batch negative sampling), the corpus (random negative sampling), a combination of both (mixed negative sampling), or hard-to-predict samples during training (online hard negative sampling). For RAG, we don&rsquo;t have click labels (since documents aren&rsquo;t returned to users directly) or the time/budget to annotate many query-document pairs.</p>
<p>A common solution is to use an LLM to generate example queries for your documents and treat synthetic query-document pairs as positives. You can use an LLM-as-a-judge to filter out low-quality queries. In the end, you can sample in-batch negatives for contrastive learning. This process is summarize in a Databricks <a href="https://www.databricks.com/blog/improving-retrieval-and-rag-embedding-model-finetuning">blogpost</a>. While fine-tuning can improve retrieval, do start from a strong position by selecting a strong model from the <a href="https://huggingface.co/spaces/mteb/leaderboard">Embedding Leaderboard</a>.</p>
<h3 id="llms-for-generation" class="scroll-mt-8 group">
  LLMs for Generation
  
    <a href="#llms-for-generation"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Your LLM may be trained to generate a response from a prompt, but not necessarily (1) <em>judging</em> which documents are most relevant to the prompt and (2) only <em>selecting</em> them to generate a response. <a href="https://arxiv.org/abs/2403.10131">Retrieval Augmented FineTuning</a> (RAFT) is a fine-tuning recipe that teaches an LLM to ignore documents that don&rsquo;t help answering the question.</p>
<figure><img src="https://www.dropbox.com/scl/fi/hlmkdvpozme82873a02rp/Screenshot-2025-10-27-at-7.04.56-PM.png?rlkey=4dymunsmiqrm7mqtpnlpn1tdd&amp;st=0ca27g47&amp;raw=1"
    alt="RAFT." width="600"><figcaption>
      <p>RAFT.</p>
    </figcaption>
</figure>

<p>The training data for RAFT consists of three parts: (1) a question ($\mathbf{Q}$), (2) a set of documents, including &ldquo;golden&rdquo; documents ($\mathbf{D^*}$) with answer-relevant information and &ldquo;distractor&rdquo; documents ($\mathbf{D}$) without, and (3) a chain-of-thought style answer ($\mathbf{A}$) generated from a golden document. In $(1-P)$ of the data, there&rsquo;s no golden documents. The LLM is trained to generate the given answer using given documents. RAFT improves response quality (see <a href="https://www.yuan-meng.com/notes/rag/#generation-evaluation">&ldquo;Generation Evaluation&rdquo;</a>) within the fine-tuned domain. Ablation studies showed that including the data without golden documents made the LLM more robust.</p>
<h2 id="put-rag-in-production" class="scroll-mt-8 group">
  Put RAG in Production
  
    <a href="#put-rag-in-production"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p>Productionizing RAG is like productionizing any search engine&mdash;you need to define and track key metrics to measure system performance:</p>
<ul>
<li><strong>System metrics</strong>: latency (p50, p90, p95, p99 end-to-end and per component), throughput, memory and compute usage, success rate, timeout rate, null search rate, and so on.</li>
<li><strong>Quality metrics</strong>: retrieval metrics (e.g., MRR, MAP@k, nDCG@k, P@k, R@k), generation metrics (e.g., response &lt;&gt; query relevance, context &lt;&gt; query relevance, response &lt;&gt; context groundedness, correctness, semantic similarity, faithfulness, guideline adherence), handling of unexpected or adversarial queries, etc..</li>
<li><strong>Security and safety</strong>: prompt injection and data leakage prevention, access control and audit logging, PII redaction, hallucination rate, jailbreak detection, and content moderation.</li>
</ul>
<p>In both online experiments and daily monitoring, you should log per-component as well as end-to-end performance. For debugging and observability purposes, it&rsquo;s useful to log as much as you can:</p>
<ul>
<li>The raw user query (before parsing or analysis)</li>
<li>The rewritten or analyzed query sent to the retriever</li>
<li>The initial prompt without retrieved documents</li>
<li>The retrieved chunks (top-k before and after reranking)</li>
<li>The final prompt assembled for the LLM</li>
<li>The LLM-generated response</li>
</ul>
<p>Most quality metrics can come from using an LLM-as-a-judge. For metrics like correctness or faithfulness, you can periodically sample outputs for human-in-the-loop evaluation. Any significant degradation should trigger alerts for on-call engineers and model owners to investigate, ideally with dashboards that trace issues back to individual components (retriever, reranker, generator, etc.) or data sources.</p>
<h2 id="learn-more" class="scroll-mt-8 group">
  Learn More
  
    <a href="#learn-more"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p>I feel RAG is one of those things you can explain in two sentences but the actual system you attempt to build can fail in 500 ways. Otherwise, there&rsquo;s no need to hire so many engineers to work on it :)</p>
<p>Even when using basic RAG, you might ask, how to best analyze the types of queries your users are most likely to search (some UX insights could help)? How do you chunk the documents you have? Exactly what documents do you have? If retrieving from multiple sources, against what metrics do you tune their relative weights? How do you identify which component is under-performing? If embeddings are no good, how do you fine-tune? What about the LLM? To name a few.</p>
<p>Nowaways, many companies are shifting towards <a href="https://arxiv.org/abs/2501.05366">Agentic Search</a> &mdash; rather than following a set procedure, an &ldquo;agent&rdquo; can decide whether to retrieve documents or directly generate a response, whether to search again in case the response is irrelevant, and so on. To learn more, you can check out tutorials form <a href="https://developers.llamaindex.ai/python/framework/understanding/rag/">LLamaIndex</a>, <a href="https://python.langchain.com/docs/tutorials/agents/">LangChain</a>, Pinecone (<a href="https://www.pinecone.io/learn/retrieval-augmented-generation/">RAG</a>, <a href="https://www.pinecone.io/learn/series/langchain/langchain-agents/">Agent</a>), or skim a book (e.g., <a href="https://www.manning.com/books/a-simple-guide-to-retrieval-augmented-generation">A Simple Guide to Retrieval Augmented Generation</a>) for a more comprehensive overview.</p>

    </div>
  </article>

  
    <aside class="not-prose flex flex-col space-y-8 border-t pt-6">
    <section class="flex flex-col space-y-4">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-shapes h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="M8.3 10a.7.7 0 0 1-.626-1.079L11.4 3a.7.7 0 0 1 1.198-.043L16.3 8.9a.7.7 0 0 1-.572 1.1Z"
  />
  <rect width="7" height="7" x="3" y="14" rx="1" />
  <circle cx="17.5" cy="17.5" r="3.5" />
</svg>

        <span>Categories</span>
      </h2>

      <ul class="ml-6 flex flex-row flex-wrap items-center space-x-2">
          <li>
            <a href="/categories/llm/" class="taxonomy category">llm</a>
          </li>
          <li>
            <a href="/categories/search/" class="taxonomy category">search</a>
          </li>
          <li>
            <a href="/categories/rag/" class="taxonomy category">rag</a>
          </li>
      </ul>
    </section>
    <section class="flex flex-col space-y-4" aria-hidden="true">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-chart-network h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="m13.11 7.664 1.78 2.672M14.162 12.788l-3.324 1.424M20 4l-6.06 1.515M3 3v16a2 2 0 0 0 2 2h16"
  />
  <circle cx="12" cy="6" r="2" />
  <circle cx="16" cy="12" r="2" />
  <circle cx="9" cy="15" r="2" />
</svg>

        <span>Graph</span>
      </h2>

      <content-network-graph
  class="h-64 ml-6"
  data-endpoint="/graph/index.json"
  page="/notes/rag/"
></content-network-graph>

    </section>
    <section class="flex flex-col space-y-4">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-newspaper h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="M4 22h16a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v16a2 2 0 0 1-2 2Zm0 0a2 2 0 0 1-2-2v-9c0-1.1.9-2 2-2h2M18 14h-8M15 18h-5"
  />
  <path d="M10 6h8v4h-8V6Z" />
</svg>

        <span>Posts</span>
      </h2>
        <section class="flex flex-col space-y-1">
          <h3 class="flex flex-row items-center space-x-2 text-sm font-semibold">
            <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-arrow-up-from-dot h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path d="m5 9 7-7 7 7M12 16V2" />
  <circle cx="12" cy="21" r="1" />
</svg>

            <span>Outgoing</span>
          </h3>

          <ol class="not-prose ml-6">
    <li>
      <article class="flex flex-row items-center">
        <header class="grow">
          <h3>
            <a
              href="/posts/ltr/"
              class="truncate text-sm underline decoration-slate-300 decoration-2 underline-offset-4 hover:decoration-inherit"
              title="An Evolution of Learning to Rank"
              >An Evolution of Learning to Rank</a
            >
          </h3>
        </header>
          <ul class="flex flex-row items-center justify-end space-x-2">
              <li>
                <a
                  href="/categories/search/"
                  class="taxonomy"
                  title="Posts and notes on Search"
                  >Search</a
                >
              </li>
              <li>
                <a
                  href="/categories/information-retrieval/"
                  class="taxonomy"
                  title="Posts and notes on Information retrieval"
                  >Information retrieval</a
                >
              </li>
              <li>
                <a
                  href="/categories/learning-to-rank/"
                  class="taxonomy"
                  title="Posts and notes on Learning to rank"
                  >Learning to rank</a
                >
              </li>
          </ul>
      </article>
    </li>
    <li>
      <article class="flex flex-row items-center">
        <header class="grow">
          <h3>
            <a
              href="/posts/ebr/"
              class="truncate text-sm underline decoration-slate-300 decoration-2 underline-offset-4 hover:decoration-inherit"
              title="An Introduction to Embedding-Based Retrieval"
              >An Introduction to Embedding-Based Retrieval</a
            >
          </h3>
        </header>
          <ul class="flex flex-row items-center justify-end space-x-2">
              <li>
                <a
                  href="/categories/embedding/"
                  class="taxonomy"
                  title="Posts and notes on Embedding"
                  >Embedding</a
                >
              </li>
              <li>
                <a
                  href="/categories/information-retrieval/"
                  class="taxonomy"
                  title="Posts and notes on Information retrieval"
                  >Information retrieval</a
                >
              </li>
              <li>
                <a
                  href="/categories/vector-based-search/"
                  class="taxonomy"
                  title="Posts and notes on Vector-based search"
                  >Vector-based search</a
                >
              </li>
          </ul>
      </article>
    </li>
    <li>
      <article class="flex flex-row items-center">
        <header class="grow">
          <h3>
            <a
              href="/posts/ltr/"
              class="truncate text-sm underline decoration-slate-300 decoration-2 underline-offset-4 hover:decoration-inherit"
              title="An Evolution of Learning to Rank"
              >An Evolution of Learning to Rank</a
            >
          </h3>
        </header>
          <ul class="flex flex-row items-center justify-end space-x-2">
              <li>
                <a
                  href="/categories/search/"
                  class="taxonomy"
                  title="Posts and notes on Search"
                  >Search</a
                >
              </li>
              <li>
                <a
                  href="/categories/information-retrieval/"
                  class="taxonomy"
                  title="Posts and notes on Information retrieval"
                  >Information retrieval</a
                >
              </li>
              <li>
                <a
                  href="/categories/learning-to-rank/"
                  class="taxonomy"
                  title="Posts and notes on Learning to rank"
                  >Learning to rank</a
                >
              </li>
          </ul>
      </article>
    </li>
</ol>

        </section>
    </section>
</aside>

      </main>
      <footer class="mt-20 border-t border-neutral-100 pt-2 text-xs">
        
<section class="items-top flex flex-row justify-between opacity-70">
  <div class="flex flex-col space-y-2">
      <p>Copyright &copy; 2026, Yuan Meng.</p>
      <div
        xmlns:cc="https://creativecommons.org/ns#"
        xmlns:dct="http://purl.org/dc/terms/"
        about="https://creativecommons.org"
      >
        Content is available under
        <a href="https://creativecommons.org/licenses/by-sa/4.0/" rel="license" class="inline-block" title="Creative Commons Attribution-ShareAlike 4.0 International"
          >CC BY-SA 4.0</a
        >
        unless otherwise noted.
      </div>
        <div
          class="mt-2 flex items-center space-x-2 fill-slate-400 hover:fill-slate-600 motion-safe:transition-colors"
        >
          <div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
  aria-hidden="true"
>
  <title>Creative Commons</title>
  <circle fill="transparent" cx="37.785" cy="28.501" r="28.836" />
  <path
    d="M37.441-3.5c8.951 0 16.572 3.125 22.857 9.372 3.008 3.009 5.295 6.448 6.857 10.314 1.561 3.867 2.344 7.971 2.344 12.314 0 4.381-.773 8.486-2.314 12.313-1.543 3.828-3.82 7.21-6.828 10.143-3.123 3.085-6.666 5.448-10.629 7.086-3.961 1.638-8.057 2.457-12.285 2.457s-8.276-.808-12.143-2.429c-3.866-1.618-7.333-3.961-10.4-7.027-3.067-3.066-5.4-6.524-7-10.372S5.5 32.767 5.5 28.5c0-4.229.809-8.295 2.428-12.2 1.619-3.905 3.972-7.4 7.057-10.486C21.08-.394 28.565-3.5 37.441-3.5zm.116 5.772c-7.314 0-13.467 2.553-18.458 7.657-2.515 2.553-4.448 5.419-5.8 8.6a25.204 25.204 0 0 0-2.029 9.972c0 3.429.675 6.734 2.029 9.913 1.353 3.183 3.285 6.021 5.8 8.516 2.514 2.496 5.351 4.399 8.515 5.715a25.652 25.652 0 0 0 9.943 1.971c3.428 0 6.75-.665 9.973-1.999 3.219-1.335 6.121-3.257 8.713-5.771 4.99-4.876 7.484-10.99 7.484-18.344 0-3.543-.648-6.895-1.943-10.057-1.293-3.162-3.18-5.98-5.654-8.458-5.146-5.143-11.335-7.715-18.573-7.715zm-.401 20.915-4.287 2.229c-.458-.951-1.019-1.619-1.685-2-.667-.38-1.286-.571-1.858-.571-2.856 0-4.286 1.885-4.286 5.657 0 1.714.362 3.084 1.085 4.113.724 1.029 1.791 1.544 3.201 1.544 1.867 0 3.181-.915 3.944-2.743l3.942 2c-.838 1.563-2 2.791-3.486 3.686-1.484.896-3.123 1.343-4.914 1.343-2.857 0-5.163-.875-6.915-2.629-1.752-1.752-2.628-4.19-2.628-7.313 0-3.048.886-5.466 2.657-7.257 1.771-1.79 4.009-2.686 6.715-2.686 3.963-.002 6.8 1.541 8.515 4.627zm18.457 0-4.229 2.229c-.457-.951-1.02-1.619-1.686-2-.668-.38-1.307-.571-1.914-.571-2.857 0-4.287 1.885-4.287 5.657 0 1.714.363 3.084 1.086 4.113.723 1.029 1.789 1.544 3.201 1.544 1.865 0 3.18-.915 3.941-2.743l4 2c-.875 1.563-2.057 2.791-3.541 3.686a9.233 9.233 0 0 1-4.857 1.343c-2.896 0-5.209-.875-6.941-2.629-1.736-1.752-2.602-4.19-2.602-7.313 0-3.048.885-5.466 2.658-7.257 1.77-1.79 4.008-2.686 6.713-2.686 3.962-.002 6.783 1.541 8.458 4.627z"
  />
</svg>
</div><div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
>
  <title>Credit must be given to the creator</title>
  <circle fill="transparent" cx="37.637" cy="28.806" r="28.276" />
  <path
    d="M37.443-3.5c8.988 0 16.57 3.085 22.742 9.257C66.393 11.967 69.5 19.548 69.5 28.5c0 8.991-3.049 16.476-9.145 22.456-6.476 6.363-14.113 9.544-22.912 9.544-8.649 0-16.153-3.144-22.514-9.43C8.644 44.784 5.5 37.262 5.5 28.5c0-8.761 3.144-16.342 9.429-22.742C21.101-.415 28.604-3.5 37.443-3.5zm.114 5.772c-7.276 0-13.428 2.553-18.457 7.657-5.22 5.334-7.829 11.525-7.829 18.572 0 7.086 2.59 13.22 7.77 18.398 5.181 5.182 11.352 7.771 18.514 7.771 7.123 0 13.334-2.607 18.629-7.828 5.029-4.838 7.543-10.952 7.543-18.343 0-7.276-2.553-13.465-7.656-18.571-5.104-5.104-11.276-7.656-18.514-7.656zm8.572 18.285v13.085h-3.656v15.542h-9.944V33.643h-3.656V20.557c0-.572.2-1.057.599-1.457.401-.399.887-.6 1.457-.6h13.144c.533 0 1.01.2 1.428.6.417.4.628.886.628 1.457zm-13.087-8.228c0-3.008 1.485-4.514 4.458-4.514s4.457 1.504 4.457 4.514c0 2.971-1.486 4.457-4.457 4.457s-4.458-1.486-4.458-4.457z"
  />
</svg>
</div><div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
>
  <title>Adaptations must be shared under the same terms</title>
  <circle fill="transparent" cx="36.944" cy="28.631" r="29.105" />
  <path
    d="M37.443-3.5c8.951 0 16.531 3.105 22.742 9.315C66.393 11.987 69.5 19.548 69.5 28.5c0 8.954-3.049 16.457-9.145 22.514-6.437 6.324-14.076 9.486-22.912 9.486-8.649 0-16.153-3.143-22.514-9.429C8.644 44.786 5.5 37.264 5.5 28.501c0-8.723 3.144-16.285 9.429-22.685C21.138-.395 28.643-3.5 37.443-3.5zm.114 5.772c-7.276 0-13.428 2.572-18.457 7.715-5.22 5.296-7.829 11.467-7.829 18.513 0 7.125 2.59 13.257 7.77 18.4 5.181 5.182 11.352 7.771 18.514 7.771 7.123 0 13.334-2.609 18.629-7.828 5.029-4.876 7.543-10.99 7.543-18.343 0-7.313-2.553-13.485-7.656-18.513-5.067-5.145-11.239-7.715-18.514-7.715zM23.271 23.985c.609-3.924 2.189-6.962 4.742-9.114 2.552-2.152 5.656-3.228 9.314-3.228 5.027 0 9.029 1.62 12 4.856 2.971 3.238 4.457 7.391 4.457 12.457 0 4.915-1.543 9-4.627 12.256-3.088 3.256-7.086 4.886-12.002 4.886-3.619 0-6.743-1.085-9.371-3.257-2.629-2.172-4.209-5.257-4.743-9.257H31.1c.19 3.886 2.533 5.829 7.029 5.829 2.246 0 4.057-.972 5.428-2.914 1.373-1.942 2.059-4.534 2.059-7.771 0-3.391-.629-5.971-1.885-7.743-1.258-1.771-3.066-2.657-5.43-2.657-4.268 0-6.667 1.885-7.2 5.656h2.343l-6.342 6.343-6.343-6.343 2.512.001z"
  />
</svg>
</div>
        </div>

  </div>
    <div>
      <a
        href="https://github.com/michenriksen/hugo-theme-til"
        title="Today I Learned &#8212; A Hugo theme by Michael Henriksen"
        data-theme-version="0.4.0"
        >theme: til</a
      >
    </div>
</section>

      </footer>
    </div>

    
    <button id="back-to-top" title="Go to top">‚òùÔ∏è</button>


    
    

    
    <script src="/js/back-to-top.js"></script>

     
    <script src="/js/cat-cursor.js" defer></script>
  </body>
</html>
