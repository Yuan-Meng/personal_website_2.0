<!doctype html>
<html
  lang="en-us"
  dir="ltr"
>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<link rel="stylesheet" href="http://localhost:1313/css/styles.min.29149e7eece4eab92c5f2dc32ab7ccaad6427a19dd21db0153b88b4ccb8f3645.css">
<meta charset="utf-8" />
<meta name="language" content="en" />
<meta name="viewport" content="width=device-width" />
<title>
    Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race | Yuan Meng
</title>
  <meta name="description" content="Coming soon in Octoberâ€¦
References Stable Literature 1.0 Nathan Lambertâ€™s RLHF book, a valiant attempt to capture a â€œstable literatureâ€ in an evolving LLM post-training battlefield Kevin Murphyâ€™s Reinforcement Learning: An Overview ğŸ‘‰ Chapter 6 talks about RL &amp; LLM Sutton and Bartoâ€™s Reinforcement Learning: An Introduction (2nd Edition) ğŸ‘‰ the OG RL book updated with new stuff Norvig and Russellâ€™s Artificial Intelligence: A Modern Approach ğŸ‘‰ Berkeley CS 188 textbook &amp; the most popular one in the world â€” an overview of intelligent agents with a heavy emphasis on RL Metaâ€™s comprehensive lit review on RL algorithms for LLMs ğŸ‘‰ Understanding Reinforcement Learning for Model Training, and future directions with GRAPE (2025) by Patel, arXiv. Extensive review on reasoning models ğŸ‘‰ A Survey of Reinforcement Learning for Large Reasoning Models (2025) by Zhang et al., arXiv. Evolving Literature RLHF for Language Models The hottest AI startup Thinking Machines sells post-training as a service ğŸ‘‰ product: Tinker; blog: LoRA Without Regret Cursorâ€™s blogpost on their wickedly effective RL models ğŸ‘‰ Improving Cursor Tab with online RL Nathan Lambertâ€™s blogpost Interconnects ğŸ‘‰ frequently writes about reasoning models, agents, post-training, etc. Hugging Faceâ€™s post-training course smol-course ğŸ‘‰ hands-on course on language model fine-tuning basics Fei-Fei Li and teamâ€™s AI agent paper ğŸ‘‰ Agent AI: Surveying the Horizons of Multimodal Interaction (2024) by Durante et al., arXiv. NVIDIA uses RL in pretraining ğŸ‘‰ RLP: Reinforcement as a Pretraining Objective (2025) by Hatamizadeh et al., arXiv. Metaâ€™s new self-play training strategy for Llama ğŸ‘‰ Language Self-Play For Data-Free Training (2025) by Kuba et al., arXiv. RLFH for RecSys OneRec-V2 improves Generative Recommendation post-training ğŸ‘‰ OneRec-V2 Technical Report (2025) by Zhou et al., arXiv. Shopeeâ€™s OnePiece integrates RL with traditional cascade RecSys ğŸ‘‰ OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System (2025) by Dai et al., arXiv. Douyin improves user understanding with reasoning ğŸ‘‰ Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling (2025) by Li et al., arXiv. Pinterest uses RL to tune value models of multi-objective rankers ğŸ‘‰ Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest (2025) by Xiao et al., arXiv. " />
<meta property="og:url" content="http://localhost:1313/posts/rlhf/">
  <meta property="og:site_name" content="Yuan Meng">
  <meta property="og:title" content="Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race">
  <meta property="og:description" content="Coming soon in Octoberâ€¦
References Stable Literature 1.0 Nathan Lambertâ€™s RLHF book, a valiant attempt to capture a â€œstable literatureâ€ in an evolving LLM post-training battlefield Kevin Murphyâ€™s Reinforcement Learning: An Overview ğŸ‘‰ Chapter 6 talks about RL &amp; LLM Sutton and Bartoâ€™s Reinforcement Learning: An Introduction (2nd Edition) ğŸ‘‰ the OG RL book updated with new stuff Norvig and Russellâ€™s Artificial Intelligence: A Modern Approach ğŸ‘‰ Berkeley CS 188 textbook &amp; the most popular one in the world â€” an overview of intelligent agents with a heavy emphasis on RL Metaâ€™s comprehensive lit review on RL algorithms for LLMs ğŸ‘‰ Understanding Reinforcement Learning for Model Training, and future directions with GRAPE (2025) by Patel, arXiv. Extensive review on reasoning models ğŸ‘‰ A Survey of Reinforcement Learning for Large Reasoning Models (2025) by Zhang et al., arXiv. Evolving Literature RLHF for Language Models The hottest AI startup Thinking Machines sells post-training as a service ğŸ‘‰ product: Tinker; blog: LoRA Without Regret Cursorâ€™s blogpost on their wickedly effective RL models ğŸ‘‰ Improving Cursor Tab with online RL Nathan Lambertâ€™s blogpost Interconnects ğŸ‘‰ frequently writes about reasoning models, agents, post-training, etc. Hugging Faceâ€™s post-training course smol-course ğŸ‘‰ hands-on course on language model fine-tuning basics Fei-Fei Li and teamâ€™s AI agent paper ğŸ‘‰ Agent AI: Surveying the Horizons of Multimodal Interaction (2024) by Durante et al., arXiv. NVIDIA uses RL in pretraining ğŸ‘‰ RLP: Reinforcement as a Pretraining Objective (2025) by Hatamizadeh et al., arXiv. Metaâ€™s new self-play training strategy for Llama ğŸ‘‰ Language Self-Play For Data-Free Training (2025) by Kuba et al., arXiv. RLFH for RecSys OneRec-V2 improves Generative Recommendation post-training ğŸ‘‰ OneRec-V2 Technical Report (2025) by Zhou et al., arXiv. Shopeeâ€™s OnePiece integrates RL with traditional cascade RecSys ğŸ‘‰ OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System (2025) by Dai et al., arXiv. Douyin improves user understanding with reasoning ğŸ‘‰ Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling (2025) by Li et al., arXiv. Pinterest uses RL to tune value models of multi-objective rankers ğŸ‘‰ Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest (2025) by Xiao et al., arXiv.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-05T00:00:00+00:00">


  <meta itemprop="name" content="Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race">
  <meta itemprop="description" content="Coming soon in Octoberâ€¦
References Stable Literature 1.0 Nathan Lambertâ€™s RLHF book, a valiant attempt to capture a â€œstable literatureâ€ in an evolving LLM post-training battlefield Kevin Murphyâ€™s Reinforcement Learning: An Overview ğŸ‘‰ Chapter 6 talks about RL &amp; LLM Sutton and Bartoâ€™s Reinforcement Learning: An Introduction (2nd Edition) ğŸ‘‰ the OG RL book updated with new stuff Norvig and Russellâ€™s Artificial Intelligence: A Modern Approach ğŸ‘‰ Berkeley CS 188 textbook &amp; the most popular one in the world â€” an overview of intelligent agents with a heavy emphasis on RL Metaâ€™s comprehensive lit review on RL algorithms for LLMs ğŸ‘‰ Understanding Reinforcement Learning for Model Training, and future directions with GRAPE (2025) by Patel, arXiv. Extensive review on reasoning models ğŸ‘‰ A Survey of Reinforcement Learning for Large Reasoning Models (2025) by Zhang et al., arXiv. Evolving Literature RLHF for Language Models The hottest AI startup Thinking Machines sells post-training as a service ğŸ‘‰ product: Tinker; blog: LoRA Without Regret Cursorâ€™s blogpost on their wickedly effective RL models ğŸ‘‰ Improving Cursor Tab with online RL Nathan Lambertâ€™s blogpost Interconnects ğŸ‘‰ frequently writes about reasoning models, agents, post-training, etc. Hugging Faceâ€™s post-training course smol-course ğŸ‘‰ hands-on course on language model fine-tuning basics Fei-Fei Li and teamâ€™s AI agent paper ğŸ‘‰ Agent AI: Surveying the Horizons of Multimodal Interaction (2024) by Durante et al., arXiv. NVIDIA uses RL in pretraining ğŸ‘‰ RLP: Reinforcement as a Pretraining Objective (2025) by Hatamizadeh et al., arXiv. Metaâ€™s new self-play training strategy for Llama ğŸ‘‰ Language Self-Play For Data-Free Training (2025) by Kuba et al., arXiv. RLFH for RecSys OneRec-V2 improves Generative Recommendation post-training ğŸ‘‰ OneRec-V2 Technical Report (2025) by Zhou et al., arXiv. Shopeeâ€™s OnePiece integrates RL with traditional cascade RecSys ğŸ‘‰ OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System (2025) by Dai et al., arXiv. Douyin improves user understanding with reasoning ğŸ‘‰ Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling (2025) by Li et al., arXiv. Pinterest uses RL to tune value models of multi-objective rankers ğŸ‘‰ Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest (2025) by Xiao et al., arXiv.">
  <meta itemprop="datePublished" content="2025-10-05T00:00:00+00:00">
  <meta itemprop="wordCount" content="358">
  <meta itemprop="keywords" content="Reinforcement learning from human feedback,Large language models,Recommender systems">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race">
  <meta name="twitter:description" content="Coming soon in Octoberâ€¦
References Stable Literature 1.0 Nathan Lambertâ€™s RLHF book, a valiant attempt to capture a â€œstable literatureâ€ in an evolving LLM post-training battlefield Kevin Murphyâ€™s Reinforcement Learning: An Overview ğŸ‘‰ Chapter 6 talks about RL &amp; LLM Sutton and Bartoâ€™s Reinforcement Learning: An Introduction (2nd Edition) ğŸ‘‰ the OG RL book updated with new stuff Norvig and Russellâ€™s Artificial Intelligence: A Modern Approach ğŸ‘‰ Berkeley CS 188 textbook &amp; the most popular one in the world â€” an overview of intelligent agents with a heavy emphasis on RL Metaâ€™s comprehensive lit review on RL algorithms for LLMs ğŸ‘‰ Understanding Reinforcement Learning for Model Training, and future directions with GRAPE (2025) by Patel, arXiv. Extensive review on reasoning models ğŸ‘‰ A Survey of Reinforcement Learning for Large Reasoning Models (2025) by Zhang et al., arXiv. Evolving Literature RLHF for Language Models The hottest AI startup Thinking Machines sells post-training as a service ğŸ‘‰ product: Tinker; blog: LoRA Without Regret Cursorâ€™s blogpost on their wickedly effective RL models ğŸ‘‰ Improving Cursor Tab with online RL Nathan Lambertâ€™s blogpost Interconnects ğŸ‘‰ frequently writes about reasoning models, agents, post-training, etc. Hugging Faceâ€™s post-training course smol-course ğŸ‘‰ hands-on course on language model fine-tuning basics Fei-Fei Li and teamâ€™s AI agent paper ğŸ‘‰ Agent AI: Surveying the Horizons of Multimodal Interaction (2024) by Durante et al., arXiv. NVIDIA uses RL in pretraining ğŸ‘‰ RLP: Reinforcement as a Pretraining Objective (2025) by Hatamizadeh et al., arXiv. Metaâ€™s new self-play training strategy for Llama ğŸ‘‰ Language Self-Play For Data-Free Training (2025) by Kuba et al., arXiv. RLFH for RecSys OneRec-V2 improves Generative Recommendation post-training ğŸ‘‰ OneRec-V2 Technical Report (2025) by Zhou et al., arXiv. Shopeeâ€™s OnePiece integrates RL with traditional cascade RecSys ğŸ‘‰ OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System (2025) by Dai et al., arXiv. Douyin improves user understanding with reasoning ğŸ‘‰ Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling (2025) by Li et al., arXiv. Pinterest uses RL to tune value models of multi-objective rankers ğŸ‘‰ Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest (2025) by Xiao et al., arXiv.">

<link rel="canonical" href="http://localhost:1313/posts/rlhf/" />

    <link rel="stylesheet" href="/css/index.css" />


      <script src="/js/main.js" defer></script>
  

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org/",
  "@id": "http://localhost:1313/posts/rlhf/",
  "@type": "BlogPosting",
  "articleSection": [
    "Reinforcement learning from human feedback",
    "Large language models",
    "Recommender systems"
  ],
  "author": {
    "@type": "Person",
    "email": "mycaptainmy@gmail.com",
    "name": "Yuan Meng",
    "url": "http://localhost:1313/about/"
  },
  "copyrightNotice": "Yuan Meng",
  "datePublished": "2025-10-05",
  "description": "Coming soon in Octoberâ€¦\nReferences Stable Literature 1.0 Nathan Lambertâ€™s RLHF book, a valiant attempt to capture a â€œstable literatureâ€ in an evolving LLM post-training battlefield Kevin Murphyâ€™s Reinforcement Learning: An Overview ğŸ‘‰ Chapter 6 talks about RL \u0026 LLM Sutton and Bartoâ€™s Reinforcement Learning: An Introduction (2nd Edition) ğŸ‘‰ the OG RL book updated with new stuff Norvig and Russellâ€™s Artificial Intelligence: A Modern Approach ğŸ‘‰ Berkeley CS 188 textbook \u0026 the most popular one in the world â€” an overview of intelligent agents with a heavy emphasis on RL Metaâ€™s comprehensive lit review on RL algorithms for LLMs ğŸ‘‰ Understanding Reinforcement Learning for Model Training, and future directions with GRAPE (2025) by Patel, arXiv. Extensive review on reasoning models ğŸ‘‰ A Survey of Reinforcement Learning for Large Reasoning Models (2025) by Zhang et al., arXiv. Evolving Literature RLHF for Language Models The hottest AI startup Thinking Machines sells post-training as a service ğŸ‘‰ product: Tinker; blog: LoRA Without Regret Cursorâ€™s blogpost on their wickedly effective RL models ğŸ‘‰ Improving Cursor Tab with online RL Nathan Lambertâ€™s blogpost Interconnects ğŸ‘‰ frequently writes about reasoning models, agents, post-training, etc. Hugging Faceâ€™s post-training course smol-course ğŸ‘‰ hands-on course on language model fine-tuning basics Fei-Fei Li and teamâ€™s AI agent paper ğŸ‘‰ Agent AI: Surveying the Horizons of Multimodal Interaction (2024) by Durante et al., arXiv. NVIDIA uses RL in pretraining ğŸ‘‰ RLP: Reinforcement as a Pretraining Objective (2025) by Hatamizadeh et al., arXiv. Metaâ€™s new self-play training strategy for Llama ğŸ‘‰ Language Self-Play For Data-Free Training (2025) by Kuba et al., arXiv. RLFH for RecSys OneRec-V2 improves Generative Recommendation post-training ğŸ‘‰ OneRec-V2 Technical Report (2025) by Zhou et al., arXiv. Shopeeâ€™s OnePiece integrates RL with traditional cascade RecSys ğŸ‘‰ OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System (2025) by Dai et al., arXiv. Douyin improves user understanding with reasoning ğŸ‘‰ Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling (2025) by Li et al., arXiv. Pinterest uses RL to tune value models of multi-objective rankers ğŸ‘‰ Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest (2025) by Xiao et al., arXiv. ",
  "headline": "Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race",
  "isPartOf": {
    "@id": "http://localhost:1313/posts/",
    "@type": "Blog",
    "name": "Posts"
  },
  "mainEntityOfPage": "http://localhost:1313/posts/rlhf/",
  "name": "Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race",
  "timeRequired": "PT2M",
  "url": "http://localhost:1313/posts/rlhf/",
  "wordCount": 358
}
</script>


  </head>
  <body>
    <div class="container mx-auto flex max-w-prose flex-col space-y-10 p-4 md:p-6">
      <header class="flex flex-row items-center justify-between">
        <div>
  <a id="skip-nav" class="sr-only" href="#maincontent">Skip to main content</a>
  <a class="font-semibold" href="/">Yuan Meng</a>
</div>

  <nav>
    <ul class="flex flex-row items-center justify-end space-x-4">
    <li>
      <a href="/about/">About</a
      >
    </li>
    <li>
      <a aria-current="true" class="ancestor" href="/posts/">Posts</a
      >
    </li>
    <li>
      <a href="/notes/">Notes</a
      >
    </li>
    </ul>
  </nav>


      </header>
      <main class="prose prose-slate relative md:prose-lg prose-h1:text-[2em]" id="maincontent">
        <article class="main">
    <header>
      <h1 class="!mb-1">Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race</h1><div class="flex flex-row items-center space-x-4">
          <time class="text-sm italic opacity-80" datetime="2025-10-05T00:00:00&#43;00:00">October 5, 2025</time>
        </div>
    </header>

    
    
      Reading time: 2 minutes
    

    
    
      <div class="toc-container">
        <span id="toc-toggle">
          <span id="toc-icon">â–¶</span> 
          <span>Table of Contents</span>
        </span>
        <nav id="TableOfContents" class="toc-content">
          <nav id="TableOfContents">
  <ul>
    <li><a href="#references">References</a>
      <ul>
        <li><a href="#stable-literature-10">Stable Literature 1.0</a></li>
        <li><a href="#evolving-literature">Evolving Literature</a>
          <ul>
            <li><a href="#rlhf-for-language-models">RLHF for Language Models</a></li>
            <li><a href="#rlfh-for-recsys">RLFH for RecSys</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
        </nav>
      </div>

      <script>
        
        document.addEventListener('DOMContentLoaded', function () {
          var tocToggle = document.getElementById('toc-toggle');
          var tocContent = document.getElementById('TableOfContents');
          var tocIcon = document.getElementById('toc-icon');
          tocToggle.addEventListener('click', function () {
            if (tocContent.style.display === 'none' || tocContent.style.display === '') {
              tocContent.style.display = 'block';
              tocIcon.textContent = 'â–¼'; 
            } else {
              tocContent.style.display = 'none';
              tocIcon.textContent = 'â–¶'; 
            }
          });
        });
      </script>
    

    
    <div class="content">
      <p>Coming soon in October&hellip;</p>
<h2 id="references" class="scroll-mt-8 group">
  References
  
    <a href="#references"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="stable-literature-10" class="scroll-mt-8 group">
  Stable Literature 1.0
  
    <a href="#stable-literature-10"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<ol>
<li>Nathan Lambert&rsquo;s <a href="https://rlhfbook.com/">RLHF book</a>, a valiant attempt to capture a &ldquo;stable literature&rdquo; in an evolving LLM post-training battlefield</li>
<li>Kevin Murphy&rsquo;s <a href="https://arxiv.org/abs/2412.05265">Reinforcement Learning: An Overview</a> ğŸ‘‰ Chapter 6 talks about RL &amp; LLM</li>
<li>Sutton and Barto&rsquo;s <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Reinforcement Learning: An Introduction (2nd Edition)</a> ğŸ‘‰ the OG RL book updated with new stuff</li>
<li>Norvig and Russell&rsquo;s <a href="https://aima.cs.berkeley.edu/">Artificial Intelligence: A Modern Approach</a> ğŸ‘‰ Berkeley CS 188 textbook &amp; the most popular one in the world &mdash; an overview of intelligent agents with a heavy emphasis on RL</li>
<li>Meta&rsquo;s comprehensive lit review on RL algorithms for LLMs ğŸ‘‰ <a href="https://arxiv.org/abs/2509.04501"><em>Understanding Reinforcement Learning for Model Training, and future directions with GRAPE</em></a> (2025) by Patel, <em>arXiv</em>.</li>
<li>Extensive review on reasoning models ğŸ‘‰ <a href="https://arxiv.org/abs/2509.08827"><em>A Survey of Reinforcement Learning for Large Reasoning Models</em></a> (2025) by Zhang et al., <em>arXiv</em>.</li>
</ol>
<h3 id="evolving-literature" class="scroll-mt-8 group">
  Evolving Literature
  
    <a href="#evolving-literature"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<h4 id="rlhf-for-language-models" class="scroll-mt-8 group">
  RLHF for Language Models
  
    <a href="#rlhf-for-language-models"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<ol start="7">
<li>The hottest AI startup Thinking Machines sells post-training as a service ğŸ‘‰ product: <a href="https://thinkingmachines.ai/">Tinker</a>; blog: <a href="https://thinkingmachines.ai/blog/lora/">LoRA Without Regret</a></li>
<li>Cursor&rsquo;s blogpost on their wickedly effective RL models ğŸ‘‰ <a href="https://cursor.com/en-US/blog/tab-rl"><em>Improving Cursor Tab with online RL</em></a></li>
<li>Nathan Lambert&rsquo;s blogpost <a href="https://www.interconnects.ai/"><em>Interconnects</em></a> ğŸ‘‰ frequently writes about reasoning models, agents, post-training, etc.</li>
<li>Hugging Face&rsquo;s post-training course <a href="https://huggingface.co/learn/smol-course/en/unit0/1">smol-course</a> ğŸ‘‰ hands-on course on language model fine-tuning basics</li>
<li>Fei-Fei Li and team&rsquo;s AI agent paper ğŸ‘‰ <a href="https://arxiv.org/abs/2401.03568"><em>Agent AI: Surveying the Horizons of Multimodal Interaction</em></a> (2024) by Durante et al., <em>arXiv</em>.</li>
<li>NVIDIA uses RL in pretraining ğŸ‘‰ <a href="https://arxiv.org/abs/2510.01265"><em>RLP: Reinforcement as a Pretraining Objective</em></a> (2025) by Hatamizadeh et al., <em>arXiv</em>.</li>
<li>Meta&rsquo;s new self-play training strategy for Llama ğŸ‘‰ <a href="https://arxiv.org/abs/2509.07414"><em>Language Self-Play For Data-Free Training</em></a> (2025) by Kuba et al., <em>arXiv</em>.</li>
</ol>
<h4 id="rlfh-for-recsys" class="scroll-mt-8 group">
  RLFH for RecSys
  
    <a href="#rlfh-for-recsys"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<ol start="14">
<li>OneRec-V2 improves <a href="/posts/generative_recommendation/" class="backlink">Generative Recommendation</a>
  
   post-training ğŸ‘‰ <a href="https://arxiv.org/abs/2508.20900"><em>OneRec-V2 Technical Report</em></a> (2025) by Zhou et al., <em>arXiv</em>.</li>
<li>Shopee&rsquo;s OnePiece integrates RL with traditional cascade RecSys ğŸ‘‰ <a href="https://arxiv.org/abs/2509.18091"><em>OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System</em></a> (2025) by Dai et al., <em>arXiv</em>.</li>
<li>Douyin improves user understanding with reasoning ğŸ‘‰ <a href="https://arxiv.org/abs/2509.18864"><em>Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling</em></a> (2025) by Li et al., <em>arXiv</em>.</li>
<li>Pinterest uses RL to tune value models of multi-objective rankers ğŸ‘‰ <a href="https://arxiv.org/abs/2509.05292"><em>Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest</em></a> (2025) by Xiao et al., <em>arXiv</em>.</li>
</ol>

    </div>
  </article>

  
    <aside class="not-prose flex flex-col space-y-8 border-t pt-6">
    <section class="flex flex-col space-y-4">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-shapes h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="M8.3 10a.7.7 0 0 1-.626-1.079L11.4 3a.7.7 0 0 1 1.198-.043L16.3 8.9a.7.7 0 0 1-.572 1.1Z"
  />
  <rect width="7" height="7" x="3" y="14" rx="1" />
  <circle cx="17.5" cy="17.5" r="3.5" />
</svg>

        <span>Categories</span>
      </h2>

      <ul class="ml-6 flex flex-row flex-wrap items-center space-x-2">
          <li>
            <a href="/categories/reinforcement-learning-from-human-feedback/" class="taxonomy category">reinforcement learning from human feedback</a>
          </li>
          <li>
            <a href="/categories/large-language-models/" class="taxonomy category">large language models</a>
          </li>
          <li>
            <a href="/categories/recommender-systems/" class="taxonomy category">recommender systems</a>
          </li>
      </ul>
    </section>
    <section class="flex flex-col space-y-4" aria-hidden="true">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-chart-network h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="m13.11 7.664 1.78 2.672M14.162 12.788l-3.324 1.424M20 4l-6.06 1.515M3 3v16a2 2 0 0 0 2 2h16"
  />
  <circle cx="12" cy="6" r="2" />
  <circle cx="16" cy="12" r="2" />
  <circle cx="9" cy="15" r="2" />
</svg>

        <span>Graph</span>
      </h2>

      <content-network-graph
  class="h-64 ml-6"
  data-endpoint="/graph/index.json"
  page="/posts/rlhf/"
></content-network-graph>

    </section>
    <section class="flex flex-col space-y-4">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-newspaper h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="M4 22h16a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v16a2 2 0 0 1-2 2Zm0 0a2 2 0 0 1-2-2v-9c0-1.1.9-2 2-2h2M18 14h-8M15 18h-5"
  />
  <path d="M10 6h8v4h-8V6Z" />
</svg>

        <span>Posts</span>
      </h2>
        <section class="flex flex-col space-y-1">
          <h3 class="flex flex-row items-center space-x-2 text-sm font-semibold">
            <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-arrow-up-from-dot h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path d="m5 9 7-7 7 7M12 16V2" />
  <circle cx="12" cy="21" r="1" />
</svg>

            <span>Outgoing</span>
          </h3>

          <ol class="not-prose ml-6">
    <li>
      <article class="flex flex-row items-center">
        <header class="grow">
          <h3>
            <a
              href="/posts/generative_recommendation/"
              class="truncate text-sm underline decoration-slate-300 decoration-2 underline-offset-4 hover:decoration-inherit"
              title="Is Generative Recommendation the ChatGPT Moment of RecSys?"
              >Is Generative Recommendation the ChatGPT Moment of RecSys?</a
            >
          </h3>
        </header>
          <ul class="flex flex-row items-center justify-end space-x-2">
              <li>
                <a
                  href="/categories/generative-recommendation/"
                  class="taxonomy"
                  title="Posts and notes on Generative recommendation"
                  >Generative recommendation</a
                >
              </li>
              <li>
                <a
                  href="/categories/large-language-models/"
                  class="taxonomy"
                  title="Posts and notes on Large language models"
                  >Large language models</a
                >
              </li>
          </ul>
      </article>
    </li>
</ol>

        </section>
    </section>
</aside>

      </main>
      <footer class="mt-20 border-t border-neutral-100 pt-2 text-xs">
        
<section class="items-top flex flex-row justify-between opacity-70">
  <div class="flex flex-col space-y-2">
      <p>Copyright &copy; 2025, Yuan Meng.</p>
      <div
        xmlns:cc="https://creativecommons.org/ns#"
        xmlns:dct="http://purl.org/dc/terms/"
        about="https://creativecommons.org"
      >
        Content is available under
        <a href="https://creativecommons.org/licenses/by-sa/4.0/" rel="license" class="inline-block" title="Creative Commons Attribution-ShareAlike 4.0 International"
          >CC BY-SA 4.0</a
        >
        unless otherwise noted.
      </div>
        <div
          class="mt-2 flex items-center space-x-2 fill-slate-400 hover:fill-slate-600 motion-safe:transition-colors"
        >
          <div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
  aria-hidden="true"
>
  <title>Creative Commons</title>
  <circle fill="transparent" cx="37.785" cy="28.501" r="28.836" />
  <path
    d="M37.441-3.5c8.951 0 16.572 3.125 22.857 9.372 3.008 3.009 5.295 6.448 6.857 10.314 1.561 3.867 2.344 7.971 2.344 12.314 0 4.381-.773 8.486-2.314 12.313-1.543 3.828-3.82 7.21-6.828 10.143-3.123 3.085-6.666 5.448-10.629 7.086-3.961 1.638-8.057 2.457-12.285 2.457s-8.276-.808-12.143-2.429c-3.866-1.618-7.333-3.961-10.4-7.027-3.067-3.066-5.4-6.524-7-10.372S5.5 32.767 5.5 28.5c0-4.229.809-8.295 2.428-12.2 1.619-3.905 3.972-7.4 7.057-10.486C21.08-.394 28.565-3.5 37.441-3.5zm.116 5.772c-7.314 0-13.467 2.553-18.458 7.657-2.515 2.553-4.448 5.419-5.8 8.6a25.204 25.204 0 0 0-2.029 9.972c0 3.429.675 6.734 2.029 9.913 1.353 3.183 3.285 6.021 5.8 8.516 2.514 2.496 5.351 4.399 8.515 5.715a25.652 25.652 0 0 0 9.943 1.971c3.428 0 6.75-.665 9.973-1.999 3.219-1.335 6.121-3.257 8.713-5.771 4.99-4.876 7.484-10.99 7.484-18.344 0-3.543-.648-6.895-1.943-10.057-1.293-3.162-3.18-5.98-5.654-8.458-5.146-5.143-11.335-7.715-18.573-7.715zm-.401 20.915-4.287 2.229c-.458-.951-1.019-1.619-1.685-2-.667-.38-1.286-.571-1.858-.571-2.856 0-4.286 1.885-4.286 5.657 0 1.714.362 3.084 1.085 4.113.724 1.029 1.791 1.544 3.201 1.544 1.867 0 3.181-.915 3.944-2.743l3.942 2c-.838 1.563-2 2.791-3.486 3.686-1.484.896-3.123 1.343-4.914 1.343-2.857 0-5.163-.875-6.915-2.629-1.752-1.752-2.628-4.19-2.628-7.313 0-3.048.886-5.466 2.657-7.257 1.771-1.79 4.009-2.686 6.715-2.686 3.963-.002 6.8 1.541 8.515 4.627zm18.457 0-4.229 2.229c-.457-.951-1.02-1.619-1.686-2-.668-.38-1.307-.571-1.914-.571-2.857 0-4.287 1.885-4.287 5.657 0 1.714.363 3.084 1.086 4.113.723 1.029 1.789 1.544 3.201 1.544 1.865 0 3.18-.915 3.941-2.743l4 2c-.875 1.563-2.057 2.791-3.541 3.686a9.233 9.233 0 0 1-4.857 1.343c-2.896 0-5.209-.875-6.941-2.629-1.736-1.752-2.602-4.19-2.602-7.313 0-3.048.885-5.466 2.658-7.257 1.77-1.79 4.008-2.686 6.713-2.686 3.962-.002 6.783 1.541 8.458 4.627z"
  />
</svg>
</div><div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
>
  <title>Credit must be given to the creator</title>
  <circle fill="transparent" cx="37.637" cy="28.806" r="28.276" />
  <path
    d="M37.443-3.5c8.988 0 16.57 3.085 22.742 9.257C66.393 11.967 69.5 19.548 69.5 28.5c0 8.991-3.049 16.476-9.145 22.456-6.476 6.363-14.113 9.544-22.912 9.544-8.649 0-16.153-3.144-22.514-9.43C8.644 44.784 5.5 37.262 5.5 28.5c0-8.761 3.144-16.342 9.429-22.742C21.101-.415 28.604-3.5 37.443-3.5zm.114 5.772c-7.276 0-13.428 2.553-18.457 7.657-5.22 5.334-7.829 11.525-7.829 18.572 0 7.086 2.59 13.22 7.77 18.398 5.181 5.182 11.352 7.771 18.514 7.771 7.123 0 13.334-2.607 18.629-7.828 5.029-4.838 7.543-10.952 7.543-18.343 0-7.276-2.553-13.465-7.656-18.571-5.104-5.104-11.276-7.656-18.514-7.656zm8.572 18.285v13.085h-3.656v15.542h-9.944V33.643h-3.656V20.557c0-.572.2-1.057.599-1.457.401-.399.887-.6 1.457-.6h13.144c.533 0 1.01.2 1.428.6.417.4.628.886.628 1.457zm-13.087-8.228c0-3.008 1.485-4.514 4.458-4.514s4.457 1.504 4.457 4.514c0 2.971-1.486 4.457-4.457 4.457s-4.458-1.486-4.458-4.457z"
  />
</svg>
</div><div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
>
  <title>Adaptations must be shared under the same terms</title>
  <circle fill="transparent" cx="36.944" cy="28.631" r="29.105" />
  <path
    d="M37.443-3.5c8.951 0 16.531 3.105 22.742 9.315C66.393 11.987 69.5 19.548 69.5 28.5c0 8.954-3.049 16.457-9.145 22.514-6.437 6.324-14.076 9.486-22.912 9.486-8.649 0-16.153-3.143-22.514-9.429C8.644 44.786 5.5 37.264 5.5 28.501c0-8.723 3.144-16.285 9.429-22.685C21.138-.395 28.643-3.5 37.443-3.5zm.114 5.772c-7.276 0-13.428 2.572-18.457 7.715-5.22 5.296-7.829 11.467-7.829 18.513 0 7.125 2.59 13.257 7.77 18.4 5.181 5.182 11.352 7.771 18.514 7.771 7.123 0 13.334-2.609 18.629-7.828 5.029-4.876 7.543-10.99 7.543-18.343 0-7.313-2.553-13.485-7.656-18.513-5.067-5.145-11.239-7.715-18.514-7.715zM23.271 23.985c.609-3.924 2.189-6.962 4.742-9.114 2.552-2.152 5.656-3.228 9.314-3.228 5.027 0 9.029 1.62 12 4.856 2.971 3.238 4.457 7.391 4.457 12.457 0 4.915-1.543 9-4.627 12.256-3.088 3.256-7.086 4.886-12.002 4.886-3.619 0-6.743-1.085-9.371-3.257-2.629-2.172-4.209-5.257-4.743-9.257H31.1c.19 3.886 2.533 5.829 7.029 5.829 2.246 0 4.057-.972 5.428-2.914 1.373-1.942 2.059-4.534 2.059-7.771 0-3.391-.629-5.971-1.885-7.743-1.258-1.771-3.066-2.657-5.43-2.657-4.268 0-6.667 1.885-7.2 5.656h2.343l-6.342 6.343-6.343-6.343 2.512.001z"
  />
</svg>
</div>
        </div>

  </div>
    <div>
      <a
        href="https://github.com/michenriksen/hugo-theme-til"
        title="Today I Learned &#8212; A Hugo theme by Michael Henriksen"
        data-theme-version="0.4.0"
        >theme: til</a
      >
    </div>
</section>

      </footer>
    </div>

    
    <button id="back-to-top" title="Go to top">â˜ï¸</button>


    
    

    
    <script src="/js/back-to-top.js"></script>

     
    <script src="/js/cat-cursor.js" defer></script>
  </body>
</html>
