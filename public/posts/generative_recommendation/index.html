<!doctype html>
<html
  lang="en-us"
  dir="ltr"
>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<link rel="stylesheet" href="http://localhost:1313/css/styles.min.29149e7eece4eab92c5f2dc32ab7ccaad6427a19dd21db0153b88b4ccb8f3645.css">
<meta charset="utf-8" />
<meta name="language" content="en" />
<meta name="viewport" content="width=device-width" />
<title>
    Is Generative Recommendation the ChatGPT Moment of RecSys? | Yuan Meng
</title>
  <meta name="description" content=" Has the Tide Turned? From DLRM to GM For nearly a decade, recommender systems have remained largely the same (sidenote: It used to be (still is?) the case that if you&#39;re familiar with the cascade pipeline and the most popular L1 (e.g., two-tower models and embedding-based retrieval) and L2 (e.g., &#34;Embedding-MLP&#34; style `pAction` models, sequence modeling) architectures, you&#39;re golden in almost every ML system design interview. Perhaps a year from now, GenRec talents and experience will be what top companies seek instead.) . It‚Äôs hard to even imagine a system without a cascade pipeline in the iconic YouTube paper, which retrieves tens of thousands of candidates from a massive corpus, trims them down to hundreds of relevant items using a lightweight ranker (L1), selects the top dozen using a heavy ranker (L2), and makes adjustments based on policy and business logic (L3). Architecture-wise, the L2 ranker hasn‚Äôt drifted far from the seminal Deep &amp; Wide network, which embeds input features, passes them through interaction modules, and transforms representations for task heads (e.g., clicks, purchase, video watch). Upgrades to feature interaction (e.g., DCN-v2, MaskNet) and multi-task learning (e.g., MMoE, PLE) culminated in Meta‚Äôs DHEN, which combines multiple interaction modules and experts to push the limits of this ‚ÄúDeep Learning Recommender System‚Äù (DLRM) paradigm.
Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style ‚ÄòEmbedding &amp; Interaction &amp; Expert‚Äô model architectures.
In 2025, the tide seems to have finally turned after Meta‚Äôs HSTU delivered perhaps the biggest offline/online metric and serving efficiency gains in recent years ‚Äî other top companies such as Google (sidenote: Google DeepMind published TIGER a year before HSTU, but it was used for retrieval. Meta may have been the major influence behind using Generative Recommendation for both retrieval and ranking.) , Netflix, Kuaishou, ByteDance, Xiaohongshu, Tencent, Baidu, Alibaba, JD.com, and Meituan are starting to embrace a new ‚ÄúGenerative Recommendation‚Äù (GM) paradigm for retrieval and ranking, reframing the discriminative pAction prediction task as a generative task, akin to token predictions in language modeling." />
<meta property="og:url" content="http://localhost:1313/posts/generative_recommendation/">
  <meta property="og:site_name" content="Yuan Meng">
  <meta property="og:title" content="Is Generative Recommendation the ChatGPT Moment of RecSys?">
  <meta property="og:description" content="Has the Tide Turned? From DLRM to GM For nearly a decade, recommender systems have remained largely the same (sidenote: It used to be (still is?) the case that if you&#39;re familiar with the cascade pipeline and the most popular L1 (e.g., two-tower models and embedding-based retrieval) and L2 (e.g., &#34;Embedding-MLP&#34; style `pAction` models, sequence modeling) architectures, you&#39;re golden in almost every ML system design interview. Perhaps a year from now, GenRec talents and experience will be what top companies seek instead.) . It‚Äôs hard to even imagine a system without a cascade pipeline in the iconic YouTube paper, which retrieves tens of thousands of candidates from a massive corpus, trims them down to hundreds of relevant items using a lightweight ranker (L1), selects the top dozen using a heavy ranker (L2), and makes adjustments based on policy and business logic (L3). Architecture-wise, the L2 ranker hasn‚Äôt drifted far from the seminal Deep &amp; Wide network, which embeds input features, passes them through interaction modules, and transforms representations for task heads (e.g., clicks, purchase, video watch). Upgrades to feature interaction (e.g., DCN-v2, MaskNet) and multi-task learning (e.g., MMoE, PLE) culminated in Meta‚Äôs DHEN, which combines multiple interaction modules and experts to push the limits of this ‚ÄúDeep Learning Recommender System‚Äù (DLRM) paradigm.
Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style ‚ÄòEmbedding &amp; Interaction &amp; Expert‚Äô model architectures.
In 2025, the tide seems to have finally turned after Meta‚Äôs HSTU delivered perhaps the biggest offline/online metric and serving efficiency gains in recent years ‚Äî other top companies such as Google (sidenote: Google DeepMind published TIGER a year before HSTU, but it was used for retrieval. Meta may have been the major influence behind using Generative Recommendation for both retrieval and ranking.) , Netflix, Kuaishou, ByteDance, Xiaohongshu, Tencent, Baidu, Alibaba, JD.com, and Meituan are starting to embrace a new ‚ÄúGenerative Recommendation‚Äù (GM) paradigm for retrieval and ranking, reframing the discriminative pAction prediction task as a generative task, akin to token predictions in language modeling.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-07-29T00:00:00+00:00">


  <meta itemprop="name" content="Is Generative Recommendation the ChatGPT Moment of RecSys?">
  <meta itemprop="description" content="Has the Tide Turned? From DLRM to GM For nearly a decade, recommender systems have remained largely the same (sidenote: It used to be (still is?) the case that if you&#39;re familiar with the cascade pipeline and the most popular L1 (e.g., two-tower models and embedding-based retrieval) and L2 (e.g., &#34;Embedding-MLP&#34; style `pAction` models, sequence modeling) architectures, you&#39;re golden in almost every ML system design interview. Perhaps a year from now, GenRec talents and experience will be what top companies seek instead.) . It‚Äôs hard to even imagine a system without a cascade pipeline in the iconic YouTube paper, which retrieves tens of thousands of candidates from a massive corpus, trims them down to hundreds of relevant items using a lightweight ranker (L1), selects the top dozen using a heavy ranker (L2), and makes adjustments based on policy and business logic (L3). Architecture-wise, the L2 ranker hasn‚Äôt drifted far from the seminal Deep &amp; Wide network, which embeds input features, passes them through interaction modules, and transforms representations for task heads (e.g., clicks, purchase, video watch). Upgrades to feature interaction (e.g., DCN-v2, MaskNet) and multi-task learning (e.g., MMoE, PLE) culminated in Meta‚Äôs DHEN, which combines multiple interaction modules and experts to push the limits of this ‚ÄúDeep Learning Recommender System‚Äù (DLRM) paradigm.
Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style ‚ÄòEmbedding &amp; Interaction &amp; Expert‚Äô model architectures.
In 2025, the tide seems to have finally turned after Meta‚Äôs HSTU delivered perhaps the biggest offline/online metric and serving efficiency gains in recent years ‚Äî other top companies such as Google (sidenote: Google DeepMind published TIGER a year before HSTU, but it was used for retrieval. Meta may have been the major influence behind using Generative Recommendation for both retrieval and ranking.) , Netflix, Kuaishou, ByteDance, Xiaohongshu, Tencent, Baidu, Alibaba, JD.com, and Meituan are starting to embrace a new ‚ÄúGenerative Recommendation‚Äù (GM) paradigm for retrieval and ranking, reframing the discriminative pAction prediction task as a generative task, akin to token predictions in language modeling.">
  <meta itemprop="datePublished" content="2025-07-29T00:00:00+00:00">
  <meta itemprop="wordCount" content="2061">
  <meta itemprop="keywords" content="Generative recommendation,Large language models">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Is Generative Recommendation the ChatGPT Moment of RecSys?">
  <meta name="twitter:description" content="Has the Tide Turned? From DLRM to GM For nearly a decade, recommender systems have remained largely the same (sidenote: It used to be (still is?) the case that if you&#39;re familiar with the cascade pipeline and the most popular L1 (e.g., two-tower models and embedding-based retrieval) and L2 (e.g., &#34;Embedding-MLP&#34; style `pAction` models, sequence modeling) architectures, you&#39;re golden in almost every ML system design interview. Perhaps a year from now, GenRec talents and experience will be what top companies seek instead.) . It‚Äôs hard to even imagine a system without a cascade pipeline in the iconic YouTube paper, which retrieves tens of thousands of candidates from a massive corpus, trims them down to hundreds of relevant items using a lightweight ranker (L1), selects the top dozen using a heavy ranker (L2), and makes adjustments based on policy and business logic (L3). Architecture-wise, the L2 ranker hasn‚Äôt drifted far from the seminal Deep &amp; Wide network, which embeds input features, passes them through interaction modules, and transforms representations for task heads (e.g., clicks, purchase, video watch). Upgrades to feature interaction (e.g., DCN-v2, MaskNet) and multi-task learning (e.g., MMoE, PLE) culminated in Meta‚Äôs DHEN, which combines multiple interaction modules and experts to push the limits of this ‚ÄúDeep Learning Recommender System‚Äù (DLRM) paradigm.
Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style ‚ÄòEmbedding &amp; Interaction &amp; Expert‚Äô model architectures.
In 2025, the tide seems to have finally turned after Meta‚Äôs HSTU delivered perhaps the biggest offline/online metric and serving efficiency gains in recent years ‚Äî other top companies such as Google (sidenote: Google DeepMind published TIGER a year before HSTU, but it was used for retrieval. Meta may have been the major influence behind using Generative Recommendation for both retrieval and ranking.) , Netflix, Kuaishou, ByteDance, Xiaohongshu, Tencent, Baidu, Alibaba, JD.com, and Meituan are starting to embrace a new ‚ÄúGenerative Recommendation‚Äù (GM) paradigm for retrieval and ranking, reframing the discriminative pAction prediction task as a generative task, akin to token predictions in language modeling.">

<link rel="canonical" href="http://localhost:1313/posts/generative_recommendation/" />

    <link rel="stylesheet" href="/css/index.css" />


      <script src="/js/main.js" defer></script>
  

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org/",
  "@id": "http://localhost:1313/posts/generative_recommendation/",
  "@type": "BlogPosting",
  "articleSection": [
    "Generative recommendation",
    "Large language models"
  ],
  "author": {
    "@type": "Person",
    "email": "mycaptainmy@gmail.com",
    "name": "Yuan Meng",
    "url": "http://localhost:1313/about/"
  },
  "copyrightNotice": "Yuan Meng",
  "datePublished": "2025-07-29",
  "description": " Has the Tide Turned? From DLRM to GM For nearly a decade, recommender systems have remained largely the same (sidenote: It used to be (still is?) the case that if you're familiar with the cascade pipeline and the most popular L1 (e.g., two-tower models and embedding-based retrieval) and L2 (e.g., \"Embedding-MLP\" style `pAction` models, sequence modeling) architectures, you're golden in almost every ML system design interview. Perhaps a year from now, GenRec talents and experience will be what top companies seek instead.) . It‚Äôs hard to even imagine a system without a cascade pipeline in the iconic YouTube paper, which retrieves tens of thousands of candidates from a massive corpus, trims them down to hundreds of relevant items using a lightweight ranker (L1), selects the top dozen using a heavy ranker (L2), and makes adjustments based on policy and business logic (L3). Architecture-wise, the L2 ranker hasn‚Äôt drifted far from the seminal Deep \u0026 Wide network, which embeds input features, passes them through interaction modules, and transforms representations for task heads (e.g., clicks, purchase, video watch). Upgrades to feature interaction (e.g., DCN-v2, MaskNet) and multi-task learning (e.g., MMoE, PLE) culminated in Meta‚Äôs DHEN, which combines multiple interaction modules and experts to push the limits of this ‚ÄúDeep Learning Recommender System‚Äù (DLRM) paradigm.\nSince 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style ‚ÄòEmbedding \u0026 Interaction \u0026 Expert‚Äô model architectures.\nIn 2025, the tide seems to have finally turned after Meta‚Äôs HSTU delivered perhaps the biggest offline/online metric and serving efficiency gains in recent years ‚Äî other top companies such as Google (sidenote: Google DeepMind published TIGER a year before HSTU, but it was used for retrieval. Meta may have been the major influence behind using Generative Recommendation for both retrieval and ranking.) , Netflix, Kuaishou, ByteDance, Xiaohongshu, Tencent, Baidu, Alibaba, JD.com, and Meituan are starting to embrace a new ‚ÄúGenerative Recommendation‚Äù (GM) paradigm for retrieval and ranking, reframing the discriminative pAction prediction task as a generative task, akin to token predictions in language modeling.",
  "headline": "Is Generative Recommendation the ChatGPT Moment of RecSys?",
  "isPartOf": {
    "@id": "http://localhost:1313/posts/",
    "@type": "Blog",
    "name": "Posts"
  },
  "mainEntityOfPage": "http://localhost:1313/posts/generative_recommendation/",
  "name": "Is Generative Recommendation the ChatGPT Moment of RecSys?",
  "timeRequired": "PT10M",
  "url": "http://localhost:1313/posts/generative_recommendation/",
  "wordCount": 2061
}
</script>


  </head>
  <body>
    <div class="container mx-auto flex max-w-prose flex-col space-y-10 p-4 md:p-6">
      <header class="flex flex-row items-center justify-between">
        <div>
  <a id="skip-nav" class="sr-only" href="#maincontent">Skip to main content</a>
  <a class="font-semibold" href="/">Yuan Meng</a>
</div>

  <nav>
    <ul class="flex flex-row items-center justify-end space-x-4">
    <li>
      <a href="/about/">About</a
      >
    </li>
    <li>
      <a aria-current="true" class="ancestor" href="/posts/">Posts</a
      >
    </li>
    <li>
      <a href="/notes/">Notes</a
      >
    </li>
    </ul>
  </nav>


      </header>
      <main class="prose prose-slate relative md:prose-lg prose-h1:text-[2em]" id="maincontent">
        <article class="main">
    <header>
      <h1 class="!mb-1">Is Generative Recommendation the ChatGPT Moment of RecSys?</h1><div class="flex flex-row items-center space-x-4">
          <time class="text-sm italic opacity-80" datetime="2025-07-29T00:00:00&#43;00:00">July 29, 2025</time>
        </div>
    </header>

    
    
      Reading time: 10 minutes
    

    
    
      <div class="toc-container">
        <span id="toc-toggle">
          <span id="toc-icon">‚ñ∂</span> 
          <span>Table of Contents</span>
        </span>
        <nav id="TableOfContents" class="toc-content">
          <nav id="TableOfContents">
  <ul>
    <li><a href="#has-the-tide-turned-from-dlrm-to-gm">Has the Tide Turned? From DLRM to GM</a></li>
    <li><a href="#compositionality-language-and-intelligence">Compositionality, Language, and Intelligence</a>
      <ul>
        <li><a href="#the-one-epoch-curse-of-dlrm-recommenders">The One-Epoch Curse of DLRM Recommenders</a></li>
        <li><a href="#what-makes-language-special-hocketts-design-features">What Makes Language Special: Hockett&rsquo;s Design Features</a></li>
      </ul>
    </li>
    <li><a href="#break-the-non-scalability-curse-in-recsys">Break the Non-Scalability Curse in RecSys</a>
      <ul>
        <li><a href="#conjure-up-compositionality-via-semantic-ids">Conjure Up Compositionality via Semantic IDs</a></li>
        <li><a href="#crank-up-task-complexity-via-generative-training">Crank Up Task Complexity via Generative Training</a></li>
      </ul>
    </li>
    <li><a href="#flavors-of-generative-recommenders">Flavors of Generative Recommenders</a>
      <ul>
        <li><a href="#generative-training-generative-inference-hstu-onerec">Generative Training, Generative Inference: HSTU, OneRec</a></li>
        <li><a href="#generative-pretraining-discriminative-inference-gpsd">Generative Pretraining, Discriminative Inference: GPSD</a></li>
      </ul>
    </li>
    <li><a href="#lessons-for-non-meta-and-non-kuaishou-companies">Lessons (for Non-Meta and Non-Kuaishou Companies)</a></li>
    <li><a href="#references">References</a>
      <ul>
        <li><a href="#scaling-laws-in-recommender-systems">Scaling Laws in Recommender Systems</a></li>
        <li><a href="#from-atomic-item-ids-to-semantic-ids">From Atomic Item IDs to Semantic IDs</a></li>
        <li><a href="#ditch-dlrm-for-end-to-end-generative-architectures">Ditch DLRM for End-to-End Generative Architectures</a></li>
        <li><a href="#weave-generative-architectures-into-dlrm">Weave Generative Architectures into DLRM</a></li>
      </ul>
    </li>
  </ul>
</nav>
        </nav>
      </div>

      <script>
        
        document.addEventListener('DOMContentLoaded', function () {
          var tocToggle = document.getElementById('toc-toggle');
          var tocContent = document.getElementById('TableOfContents');
          var tocIcon = document.getElementById('toc-icon');
          tocToggle.addEventListener('click', function () {
            if (tocContent.style.display === 'none' || tocContent.style.display === '') {
              tocContent.style.display = 'block';
              tocIcon.textContent = '‚ñº'; 
            } else {
              tocContent.style.display = 'none';
              tocIcon.textContent = '‚ñ∂'; 
            }
          });
        });
      </script>
    

    
    <div class="content">
      <h2 id="has-the-tide-turned-from-dlrm-to-gm" class="scroll-mt-8 group">
  Has the Tide Turned? From DLRM to GM
  
    <a href="#has-the-tide-turned-from-dlrm-to-gm"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p>For nearly a decade, recommender systems have remained largely <span class="sidenote">
  <input
    aria-label="Show sidenote"
    type="checkbox"
    id="sidenote-checkbox-01"
    class="sidenote-checkbox hidden"
  />
  <label
    tabindex="0"
    role="mark"
    aria-details="sidenote-01"
    for="sidenote-checkbox-01"
    class="sidenote-mark"
    >the same</label
  >
  <small id="sidenote-01" class="sidenote-content">
    <span class="sr-only"> (sidenote: </span>It used to be (still is?) the case that if you're familiar with the cascade pipeline and the most popular L1 (e.g., two-tower models and embedding-based retrieval) and L2 (e.g., "Embedding-MLP" style `pAction` models, sequence modeling) architectures, you're golden in almost every ML system design interview. Perhaps a year from now, GenRec talents and experience will be what top companies seek instead.<span class="sr-only">)</span>
  </small>
</span>
. It&rsquo;s hard to even imagine a system without a cascade pipeline in the iconic <a href="https://research.google.com/pubs/archive/45530.pdf">YouTube paper</a>, which retrieves tens of thousands of candidates from a massive corpus, trims them down to hundreds of relevant items using a lightweight ranker (L1), selects the top dozen using a heavy ranker (L2), and makes adjustments based on policy and business logic (L3). Architecture-wise, the L2 ranker hasn&rsquo;t drifted far from the seminal <a href="https://arxiv.org/abs/1606.07792">Deep &amp; Wide network</a>, which embeds input features, passes them through interaction modules, and transforms representations for task heads (e.g., clicks, purchase, video watch). Upgrades to feature interaction (e.g., <a href="https://arxiv.org/abs/2008.13535">DCN-v2</a>, <a href="https://arxiv.org/abs/2102.07619">MaskNet</a>) and multi-task learning (e.g., <a href="https://arxiv.org/abs/2311.09580">MMoE</a>, <a href="https://dl.acm.org/doi/abs/10.1145/3383313.3412236">PLE</a>) culminated in Meta&rsquo;s <a href="https://arxiv.org/abs/2203.11014">DHEN</a>, which combines multiple interaction modules and experts to push the limits of this &ldquo;Deep Learning Recommender System&rdquo; (DLRM) paradigm.</p>
<figure><img src="https://www.dropbox.com/scl/fi/96m8zb5yps9ffz9geheu7/Screenshot-2025-07-20-at-11.07.10-PM.png?rlkey=q4xtbxt3r50okrs2zo9vac2xq&amp;st=fzobjxgt&amp;raw=1"
    alt="Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style &lsquo;Embedding &amp; Interaction &amp; Expert&rsquo; model architectures." width="1800"><figcaption>
      <p>Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style &lsquo;Embedding &amp; Interaction &amp; Expert&rsquo; model architectures.</p>
    </figcaption>
</figure>

<p>In 2025, the tide seems to have finally turned after Meta&rsquo;s <a href="https://arxiv.org/abs/2402.17152">HSTU</a> delivered perhaps the biggest offline/online metric and serving efficiency gains in recent years &mdash; other top companies such as <span class="sidenote">
  <input
    aria-label="Show sidenote"
    type="checkbox"
    id="sidenote-checkbox-03"
    class="sidenote-checkbox hidden"
  />
  <label
    tabindex="0"
    role="mark"
    aria-details="sidenote-03"
    for="sidenote-checkbox-03"
    class="sidenote-mark"
    >Google</label
  >
  <small id="sidenote-03" class="sidenote-content">
    <span class="sr-only"> (sidenote: </span>Google DeepMind published TIGER a year before HSTU, but it was used for retrieval. Meta may have been the major influence behind using Generative Recommendation for both retrieval and ranking.<span class="sr-only">)</span>
  </small>
</span>
, Netflix, Kuaishou, ByteDance, Xiaohongshu, Tencent, Baidu, Alibaba, JD.com, and Meituan are starting to embrace a new &ldquo;Generative Recommendation&rdquo; (GM) paradigm for retrieval and ranking, reframing the discriminative <code>pAction</code> prediction task as a generative task, akin to token predictions in language modeling.</p>
<p>What makes Generative Recommendation so magical? Why is it able to unlock the scaling laws in recommender systems in ways that DLRM wasn&rsquo;t able to? Is GM a genuine paradigm shift or a short-lived fad? In this blogpost, let&rsquo;s take a look at GM models coming out from the aforementioned companies and see what the fuss is all about üïµÔ∏è.</p>
<h2 id="compositionality-language-and-intelligence" class="scroll-mt-8 group">
  Compositionality, Language, and Intelligence
  
    <a href="#compositionality-language-and-intelligence"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="the-one-epoch-curse-of-dlrm-recommenders" class="scroll-mt-8 group">
  The One-Epoch Curse of DLRM Recommenders
  
    <a href="#the-one-epoch-curse-of-dlrm-recommenders"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>If in the old days sciences had a <a href="https://en.wikipedia.org/wiki/Physics_envy">&ldquo;physics envy&rdquo;</a>, then today&rsquo;s machine learning definitely has an &ldquo;LLM envy&rdquo;. By duplicating a deceptively simple architecture &mdash; the Transformer (<a href="https://arxiv.org/abs/1706.03762">Vaswani et al., 2017</a>) &mdash; many times over and training models on massive amounts of data, large language models seem to have unlocked <span class="sidenote">
  <input
    aria-label="Show sidenote"
    type="checkbox"
    id="sidenote-checkbox-04"
    class="sidenote-checkbox hidden"
  />
  <label
    tabindex="0"
    role="mark"
    aria-details="sidenote-04"
    for="sidenote-checkbox-04"
    class="sidenote-mark"
    >human-like</label
  >
  <small id="sidenote-04" class="sidenote-content">
    <span class="sr-only"> (sidenote: </span>Or have they? The new ICML 2025 paper "What Has a Foundation Model Found?" shows foundation models struggle to learn underlying world knowledge and apply it to new tasks, such as discovering Newtonian mechanics from orbital trajectory training and applying it to new physics tasks.<span class="sr-only">)</span>
  </small>
</span>
intelligence. Other AI/ML domains are eager to replicate this success, especially recommender systems, which remain the lifeline of the tech industry.</p>
<p>Why not? After all, there are many similarities between recommender systems and the NLP domain, as noted by Meta researchers in <a href="https://arxiv.org/abs/2305.15333"><em>Breaking the Curse of Quality Saturation with User-Centric Ranking</em></a>:</p>
<blockquote>
<p>The key idea, with an analogy to NLP, is to think of items as tokens and users as documents, i.e., each user is modeled by a list of items that they engaged with, in chronological order according to the time of engagements.</p>
</blockquote>
<p>A user is thus a &ldquo;book&rdquo; written by a history of engaged items, and web-scale recommender systems provide endless training data. In language models, performance predictably improves with model size or training data (e.g., linear or power-law relationships), a phenomenon called &ldquo;scaling laws&rdquo; (<a href="https://arxiv.org/abs/2203.15556">Hoffmann et al., 2022</a>). In DLRM recommender systems, however, scaling laws have never emerged. In recommenders employing Transformer modules, increasing the number of Transformer layers doesn&rsquo;t improve performance &mdash; more often than not, one layer is all you need (e.g., Chen et al., <a href="https://dl.acm.org/doi/abs/10.1145/3326937.3341261">KDD 19</a>, Gu et al., CIKM <a href="https://dl.acm.org/doi/abs/10.1145/3340531.3412697">20</a>, <a href="https://dl.acm.org/doi/abs/10.1145/3459637.3481953">21</a>). Regardless of the architecture, training DLRM recommenders for more than one epoch typically results in rapid performance deterioration (the &ldquo;one-epoch phenomenon&rdquo;; Zhang et al., <a href="https://dl.acm.org/doi/abs/10.1145/3511808.3557479">CIKM 22</a>).</p>
<p>So how do we explain this stark contrast between language and recommendation models? Back in 2021, Netflix published a &ldquo;popular science&rdquo; paper that foresaw the challenges of DLRM-style recommenders. In this paper, <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/18140">Steck et al. (2021)</a> argued that deep learning recommender systems differ  from other deep learning applications such as image classification in that item IDs are &ldquo;atomic&rdquo; and directly available in the data &mdash; there are no low-to-high level feature representations to extract (e.g., pixels to objects in images). As such, deep learning recommenders only require a shallow network to learn user and item embeddings from user-item interactions, effectively learning &ldquo;dot product&rdquo; operations. They don&rsquo;t benefit from deeper architectures meant to capture low-level features.</p>
<p>This observation is profound, highlighting inherent flaws in DLRM:</p>
<ul>
<li><strong>Lack of task complexity</strong>: Why do we need the full power of deep learning to learn a task as simple as performing dot products? If shallow networks are enough, how could scaling laws emerge?</li>
</ul>
<blockquote>
<p>[&hellip;] a syntactically complex phrase is a function of the meanings of its constituent parts and the way they are combined. &mdash; <a href="https://oecs.mit.edu/pub/e222wyjy/release/1"><em>Compositionality</em></a>, Ryan M. Nefdt and Christopher Potts</p>
</blockquote>
<ul>
<li><strong>No compositionality, no intelligence</strong>: Imagine if each word as an arbitrary sound unrelated to others &mdash; e.g., saying &ldquo;blim&rdquo; for &ldquo;snake&rdquo; and &ldquo;plok&rdquo; for &ldquo;rattlesnake&rdquo; &mdash; learning any <span class="sidenote">
  <input
    aria-label="Show sidenote"
    type="checkbox"
    id="sidenote-checkbox-05"
    class="sidenote-checkbox hidden"
  />
  <label
    tabindex="0"
    role="mark"
    aria-details="sidenote-05"
    for="sidenote-checkbox-05"
    class="sidenote-mark"
    >language</label
  >
  <small id="sidenote-05" class="sidenote-content">
    <span class="sr-only"> (sidenote: </span>Or music, or planning‚Ä¶ For example, while sound frequencies are continuous and infinite, Western music relies on just 12 distinct frequencies and their multiples. Notes form chords, chords form progressions, and so forth. Without hierarchical relationships, composing music would be nearly impossible.<span class="sr-only">)</span>
  </small>
</span>
 would be impossible in our lifetime, as we&rsquo;d spend an eternity just acquiring the vocabulary. Recommender systems face precisely this challenge: a popular social media platform, for instance, may have billions of items, with new ones constantly being added, making the vocabulary enormous, ever-changing, and non-stationary. Moreover, item IDs are arbitrary and atomic, with no relationship between one another that learners can exploit to speed up learning. By contrast, without knowing German, we might guess that &ldquo;klapperschlange&rdquo; (rattlesnake) relates to &ldquo;schlange&rdquo; (snake). The compositionality of language, i.e., smaller units are combined into complex phrases and concepts, allows humans to express infinite ideas using a finite vocabulary acquired in finite time &mdash; a luxury that recommender systems don&rsquo;t have.</li>
</ul>
<h3 id="what-makes-language-special-hocketts-design-features" class="scroll-mt-8 group">
  What Makes Language Special: Hockett&rsquo;s Design Features
  
    <a href="#what-makes-language-special-hocketts-design-features"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>In the 1960s, American linguist Charles Hockett proposed 16 <a href="https://abacus.bates.edu/acad/depts/biobook/Hockett.htm">&ldquo;design features&rdquo;</a> to distinguish human language from animal communication. The <span style="background-color: #FFC31B">highlighted</span> ones below (with my paraphrasing) are what I found most interesting and relevant to recommender systems:</p>
<ol>
<li>Vocal-auditory channel: Communication must occur through some medium that transmits a message from the communicator to the receiver. (Note: Hockett&rsquo;s emphasis on vocal communication is now outdated, overlooking sign languages.)</li>
<li>Broadcast transmission and directional reception: Message is transmitted in all directions, yet the receiver can tell from where the message came. (Without knowing who&rsquo;s communicating with us from where, we can&rsquo;t communicate back.)</li>
<li>Rapid fading: Linguistic message does not persist over time, unlike permanent forms such as stone carvings or knot-tying.</li>
<li>Interchangeability: Anyone can say anything (e.g., I can claim &ldquo;I am your king&rdquo; even if I&rsquo;m not) &mdash; unlike only queen ants can produce certain chemicals. Communicators and receivers can switch roles.</li>
<li>Total feedback: The communicator can receive their own message, allowing them to control what message to send.</li>
<li>Specialization: Communication is for exchanging messages rather than practical purposes (e.g., dogs panting to cool down).</li>
<li><span style="background-color: #FFC31B">Semanticity</span>: Symbols carry stable meanings. (The same symbol shouldn&rsquo;t change meanings from one message to the next.)</li>
<li><span style="background-color: #FFC31B">Arbitrariness</span>: Which symbols map to which meanings is arbitrary.</li>
<li><span style="background-color: #FFC31B">Discreteness</span>: Smaller symbols can be combined into complex symbols in rule-governed ways (noun + &ldquo;s&rdquo; $\rightarrow$ plural).</li>
<li><span style="background-color: #FFC31B">Duality of patterning</span>: Atomic symbols have no meaning of their own, yet they can be combined into meaningful message.</li>
<li>Displacement: Language can discuss subjects not immediately present (e.g., dinner plans for tomorrow).</li>
<li>Prevarication: We can say things that are false or hypothetical.</li>
<li><span style="background-color: #FFC31B">Productivity</span>: We can say things that no one in history has said before, yet those new utterances can be readily understood.</li>
<li>Traditional transmission: Language is socially learned, not innate.</li>
<li><span style="background-color: #FFC31B">Learnability</span>: Language can be learned (with ease in childhood).</li>
<li>Reflexiveness: Language can describe itself (e.g., &ldquo;grammar,&rdquo; &ldquo;sentence,&rdquo; &ldquo;word,&rdquo; &ldquo;token,&rdquo; &ldquo;noun&rdquo;).</li>
</ol>
<p>For recommender systems to have &ldquo;intelligence,&rdquo; item IDs need not have inherent meanings from the get-go (&ldquo;arbitrariness&rdquo;), but should be decomposable into smaller units in a hierarchical, rule-governed manner (&ldquo;discreteness,&rdquo; &ldquo;duality of patterning&rdquo;), with stable mappings from tokens to meanings (&ldquo;semanticity&rdquo;). Hopefully as a result, the system will be able to learn the &ldquo;item language&rdquo; (&ldquo;learnability&rdquo;) and generalize knowledge to new items (&ldquo;productivity&rdquo;). Spoiler alert: This is the exact idea behind Semantic IDs (<a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/20dcab0f14046a5c6b02b61da9f13229-Abstract-Conference.html">Rajput et al., 2023</a>).</p>
<details>
  <summary style="color: #FFC31B; cursor: pointer;">Reflections on Linguistics and Recommender Systems</summary>
  <div style="color: #002676; padding-left: 10px;">
    Nine years ago when I first learned about Hockett's design features in a linguistics seminar as a first-year Cognitive Science PhD student, I had <span class="sidenote">
  <input
    aria-label="Show sidenote"
    type="checkbox"
    id="sidenote-checkbox-06"
    class="sidenote-checkbox hidden"
  />
  <label
    tabindex="0"
    role="mark"
    aria-details="sidenote-06"
    for="sidenote-checkbox-06"
    class="sidenote-mark"
    >zero interest</label
  >
  <small id="sidenote-06" class="sidenote-content">
    <span class="sr-only"> (sidenote: </span>Perhaps because it was too much for me to talk about how to talk about language in a language I didn't grow up speaking. Might that be a lack of reflexive-reflexiveness LOL?<span class="sr-only">)</span>
  </small>
</span>
 in linguistics. Three years ago when I started my career as a Machine Learning Engineer, I had zero interest in language models, dead set on becoming a recommender system expert ‚Äî despite closely following OpenAI since 2016 while at Berkeley. It's funny how recognizing the parallels between language and recommender systems finally helped me see the magic (structure $\rightarrow$ learnability) in the former and the beauty (can it be potentially intelligent?) in the latter.
  </div>
</details>
<!-- But how do we decompose an arbitrary item ID into meaningful smaller units? Right now, Semantic IDs is a go-to method and RQ-VAE is the most popular to learn Semantic IDs. 

# Semantic IDs -->
<h2 id="break-the-non-scalability-curse-in-recsys" class="scroll-mt-8 group">
  Break the Non-Scalability Curse in RecSys
  
    <a href="#break-the-non-scalability-curse-in-recsys"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="conjure-up-compositionality-via-semantic-ids" class="scroll-mt-8 group">
  Conjure Up Compositionality via Semantic IDs
  
    <a href="#conjure-up-compositionality-via-semantic-ids"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Introduce RQ-VAE. Discuss how it&rsquo;s used in TIGER. Talk about COBORA.</p>
<h3 id="crank-up-task-complexity-via-generative-training" class="scroll-mt-8 group">
  Crank Up Task Complexity via Generative Training
  
    <a href="#crank-up-task-complexity-via-generative-training"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<h2 id="flavors-of-generative-recommenders" class="scroll-mt-8 group">
  Flavors of Generative Recommenders
  
    <a href="#flavors-of-generative-recommenders"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="generative-training-generative-inference-hstu-onerec" class="scroll-mt-8 group">
  Generative Training, Generative Inference: HSTU, OneRec
  
    <a href="#generative-training-generative-inference-hstu-onerec"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<h3 id="generative-pretraining-discriminative-inference-gpsd" class="scroll-mt-8 group">
  Generative Pretraining, Discriminative Inference: GPSD
  
    <a href="#generative-pretraining-discriminative-inference-gpsd"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<h2 id="lessons-for-non-meta-and-non-kuaishou-companies" class="scroll-mt-8 group">
  Lessons (for Non-Meta and Non-Kuaishou Companies)
  
    <a href="#lessons-for-non-meta-and-non-kuaishou-companies"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h2 id="references" class="scroll-mt-8 group">
  References
  
    <a href="#references"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="scaling-laws-in-recommender-systems" class="scroll-mt-8 group">
  Scaling Laws in Recommender Systems
  
    <a href="#scaling-laws-in-recommender-systems"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<ol>
<li>&ldquo;One-epoch phenomenon&rdquo; üëâ <a href="https://arxiv.org/abs/2209.06053"><em>Towards Understanding the Overfitting Phenomenon of Deep Click-Through Rate Prediction Models</em></a> (2022) by Zhang et al., <em>CIKM</em>.</li>
<li>Quality saturation under the &ldquo;item-centric ranking&rdquo; framework üëâ <a href="https://arxiv.org/abs/2305.15333"><em>Breaking the Curse of Quality Saturation with User-Centric Ranking</em></a> (2023) by Zhao et al., <em>KDD</em>.</li>
<li>Netflix foresaw the lack of task complexity and item ID compositionality in DLRM üëâ <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/18140"><em>Deep Learning for Recommender Systems: A Netflix Case Study</em></a> (2021) by Steck et al., <em>AI Magazine</em>.</li>
<li>Power-law scaling hits diminishing returns in DLRM üëâ <a href="https://arxiv.org/abs/2208.08489"><em>Understanding Scaling Laws for Recommendation Models</em></a> (2022) by Ardalani et al., <em>arXiv</em>.</li>
<li>Generative training on pure IDs shows power-law scaling laws üëâ <a href="https://dl.acm.org/doi/abs/10.1145/3640457.3688129"><em>Scaling Law of Large Sequential Recommendation Models</em></a> (2025) by Zhang et al., <em>RecSys</em>.</li>
</ol>
<h3 id="from-atomic-item-ids-to-semantic-ids" class="scroll-mt-8 group">
  From Atomic Item IDs to Semantic IDs
  
    <a href="#from-atomic-item-ids-to-semantic-ids"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<ol start="6">
<li>RQ-VAE, the most popular technique for learning Semantic IDs üëâ initially invented to generate audios (<a href="https://arxiv.org/abs/2107.03312">Zeghidour et al., 2021</a>) and images (<a href="https://arxiv.org/abs/2203.01941">Lee et al., 2022</a>) with low costs and high fidelity</li>
<li>Google DeepMind&rsquo;s TIGER (<a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/20dcab0f14046a5c6b02b61da9f13229-Abstract-Conference.html">Rajput et al., 2023</a>) applied RQ-VAE to learning semantic IDs and using them for retrieval üëâ later, another Google paper (<a href="ttps://dl.acm.org/doi/abs/10.1145/3640457.3688190">Singh et al., 2024</a>) applied Semantic IDs to ranking as well</li>
<li>Baidu&rsquo;s COBRA (<a href="https://arxiv.org/abs/2503.02453">Yang et al., 2025</a>) tackles information loss from RQ-VAE quantization</li>
</ol>
<h3 id="ditch-dlrm-for-end-to-end-generative-architectures" class="scroll-mt-8 group">
  Ditch DLRM for End-to-End Generative Architectures
  
    <a href="#ditch-dlrm-for-end-to-end-generative-architectures"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<ol start="9">
<li>Meta&rsquo;s HSTU üëâ <a href="https://arxiv.org/abs/2402.17152"><em>Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations</em></a> (2024) by Zhai et al., <em>ICML</em>.</li>
<li>Kuaishou&rsquo;s OneRec üëâ <a href="https://arxiv.org/abs/2502.18965"><em>OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment</em></a> (2025) by Deng et al., <em>arXiv</em>.</li>
</ol>
<h3 id="weave-generative-architectures-into-dlrm" class="scroll-mt-8 group">
  Weave Generative Architectures into DLRM
  
    <a href="#weave-generative-architectures-into-dlrm"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<ol start="11">
<li>Alibaba&rsquo;s GPSD üëâ <a href="https://arxiv.org/abs/2506.03699"><em>Scaling Transformers for Discriminative Recommendation via Generative Pretraining</em></a> (2025) by Wang et al., <em>KDD</em>.</li>
<li>JD.com üëâ <a href="https://arxiv.org/abs/2507.11246"><em>Generative Click-through Rate Prediction with Applications to Search Advertising</em></a> (2025) by Kong et al., <em>arXiv</em>.</li>
<li>Netflix üëâ <a href="https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39"><em>Foundation Model for Personalized Recommendation</em></a> (2025) by  Hsiao et al., <em>Netflix Technology Blog</em>.</li>
<li>Xiaohongshu&rsquo;s RankGPT üëâ <a href="https://arxiv.org/abs/2505.04180"><em>Towards Large-Scale Generative Ranking</em></a> (2025) by Huang et al., <em>arXiv</em>.</li>
<li>Meituan&rsquo;s MTGR üëâ <a href="https://arxiv.org/abs/2505.18654"><em>MTGR: Industrial-Scale Generative Recommendation Framework in Meituan</em></a> (2025) by Han et al., <em>arXiv</em>.</li>
<li>Alibaba&rsquo;s LUM üëâ <a href="https://arxiv.org/abs/2502.08309"><em>Unlocking Scaling Law in Industrial Recommendation Systems with a Three-Step Paradigm Based Large User Model</em></a> (2025) by Yan et al., <em>arXiv</em>.</li>
<li>ByteDance&rsquo;s RankMixer üëâ <a href="https://arxiv.org/abs/2507.15551"><em>RankMixer: Scaling Up Ranking Models in Industrial Recommenders</em></a> (2025) by Zhu et al., <em>arXiv</em>.</li>
</ol>
    </div>
  </article>

  
    <aside class="not-prose flex flex-col space-y-8 border-t pt-6">
    <section class="flex flex-col space-y-4">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-shapes h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="M8.3 10a.7.7 0 0 1-.626-1.079L11.4 3a.7.7 0 0 1 1.198-.043L16.3 8.9a.7.7 0 0 1-.572 1.1Z"
  />
  <rect width="7" height="7" x="3" y="14" rx="1" />
  <circle cx="17.5" cy="17.5" r="3.5" />
</svg>

        <span>Categories</span>
      </h2>

      <ul class="ml-6 flex flex-row flex-wrap items-center space-x-2">
          <li>
            <a href="/categories/generative-recommendation/" class="taxonomy category">generative recommendation</a>
          </li>
          <li>
            <a href="/categories/large-language-models/" class="taxonomy category">large language models</a>
          </li>
      </ul>
    </section>
    <section class="flex flex-col space-y-4" aria-hidden="true">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-chart-network h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="m13.11 7.664 1.78 2.672M14.162 12.788l-3.324 1.424M20 4l-6.06 1.515M3 3v16a2 2 0 0 0 2 2h16"
  />
  <circle cx="12" cy="6" r="2" />
  <circle cx="16" cy="12" r="2" />
  <circle cx="9" cy="15" r="2" />
</svg>

        <span>Graph</span>
      </h2>

      <content-network-graph
  class="h-64 ml-6"
  data-endpoint="/graph/index.json"
  page="/posts/generative_recommendation/"
></content-network-graph>

    </section>
</aside>

      </main>
      <footer class="mt-20 border-t border-neutral-100 pt-2 text-xs">
        
<section class="items-top flex flex-row justify-between opacity-70">
  <div class="flex flex-col space-y-2">
      <p>Copyright &copy; 2025, Yuan Meng.</p>
      <div
        xmlns:cc="https://creativecommons.org/ns#"
        xmlns:dct="http://purl.org/dc/terms/"
        about="https://creativecommons.org"
      >
        Content is available under
        <a href="https://creativecommons.org/licenses/by-sa/4.0/" rel="license" class="inline-block" title="Creative Commons Attribution-ShareAlike 4.0 International"
          >CC BY-SA 4.0</a
        >
        unless otherwise noted.
      </div>
        <div
          class="mt-2 flex items-center space-x-2 fill-slate-400 hover:fill-slate-600 motion-safe:transition-colors"
        >
          <div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
  aria-hidden="true"
>
  <title>Creative Commons</title>
  <circle fill="transparent" cx="37.785" cy="28.501" r="28.836" />
  <path
    d="M37.441-3.5c8.951 0 16.572 3.125 22.857 9.372 3.008 3.009 5.295 6.448 6.857 10.314 1.561 3.867 2.344 7.971 2.344 12.314 0 4.381-.773 8.486-2.314 12.313-1.543 3.828-3.82 7.21-6.828 10.143-3.123 3.085-6.666 5.448-10.629 7.086-3.961 1.638-8.057 2.457-12.285 2.457s-8.276-.808-12.143-2.429c-3.866-1.618-7.333-3.961-10.4-7.027-3.067-3.066-5.4-6.524-7-10.372S5.5 32.767 5.5 28.5c0-4.229.809-8.295 2.428-12.2 1.619-3.905 3.972-7.4 7.057-10.486C21.08-.394 28.565-3.5 37.441-3.5zm.116 5.772c-7.314 0-13.467 2.553-18.458 7.657-2.515 2.553-4.448 5.419-5.8 8.6a25.204 25.204 0 0 0-2.029 9.972c0 3.429.675 6.734 2.029 9.913 1.353 3.183 3.285 6.021 5.8 8.516 2.514 2.496 5.351 4.399 8.515 5.715a25.652 25.652 0 0 0 9.943 1.971c3.428 0 6.75-.665 9.973-1.999 3.219-1.335 6.121-3.257 8.713-5.771 4.99-4.876 7.484-10.99 7.484-18.344 0-3.543-.648-6.895-1.943-10.057-1.293-3.162-3.18-5.98-5.654-8.458-5.146-5.143-11.335-7.715-18.573-7.715zm-.401 20.915-4.287 2.229c-.458-.951-1.019-1.619-1.685-2-.667-.38-1.286-.571-1.858-.571-2.856 0-4.286 1.885-4.286 5.657 0 1.714.362 3.084 1.085 4.113.724 1.029 1.791 1.544 3.201 1.544 1.867 0 3.181-.915 3.944-2.743l3.942 2c-.838 1.563-2 2.791-3.486 3.686-1.484.896-3.123 1.343-4.914 1.343-2.857 0-5.163-.875-6.915-2.629-1.752-1.752-2.628-4.19-2.628-7.313 0-3.048.886-5.466 2.657-7.257 1.771-1.79 4.009-2.686 6.715-2.686 3.963-.002 6.8 1.541 8.515 4.627zm18.457 0-4.229 2.229c-.457-.951-1.02-1.619-1.686-2-.668-.38-1.307-.571-1.914-.571-2.857 0-4.287 1.885-4.287 5.657 0 1.714.363 3.084 1.086 4.113.723 1.029 1.789 1.544 3.201 1.544 1.865 0 3.18-.915 3.941-2.743l4 2c-.875 1.563-2.057 2.791-3.541 3.686a9.233 9.233 0 0 1-4.857 1.343c-2.896 0-5.209-.875-6.941-2.629-1.736-1.752-2.602-4.19-2.602-7.313 0-3.048.885-5.466 2.658-7.257 1.77-1.79 4.008-2.686 6.713-2.686 3.962-.002 6.783 1.541 8.458 4.627z"
  />
</svg>
</div><div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
>
  <title>Credit must be given to the creator</title>
  <circle fill="transparent" cx="37.637" cy="28.806" r="28.276" />
  <path
    d="M37.443-3.5c8.988 0 16.57 3.085 22.742 9.257C66.393 11.967 69.5 19.548 69.5 28.5c0 8.991-3.049 16.476-9.145 22.456-6.476 6.363-14.113 9.544-22.912 9.544-8.649 0-16.153-3.144-22.514-9.43C8.644 44.784 5.5 37.262 5.5 28.5c0-8.761 3.144-16.342 9.429-22.742C21.101-.415 28.604-3.5 37.443-3.5zm.114 5.772c-7.276 0-13.428 2.553-18.457 7.657-5.22 5.334-7.829 11.525-7.829 18.572 0 7.086 2.59 13.22 7.77 18.398 5.181 5.182 11.352 7.771 18.514 7.771 7.123 0 13.334-2.607 18.629-7.828 5.029-4.838 7.543-10.952 7.543-18.343 0-7.276-2.553-13.465-7.656-18.571-5.104-5.104-11.276-7.656-18.514-7.656zm8.572 18.285v13.085h-3.656v15.542h-9.944V33.643h-3.656V20.557c0-.572.2-1.057.599-1.457.401-.399.887-.6 1.457-.6h13.144c.533 0 1.01.2 1.428.6.417.4.628.886.628 1.457zm-13.087-8.228c0-3.008 1.485-4.514 4.458-4.514s4.457 1.504 4.457 4.514c0 2.971-1.486 4.457-4.457 4.457s-4.458-1.486-4.458-4.457z"
  />
</svg>
</div><div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
>
  <title>Adaptations must be shared under the same terms</title>
  <circle fill="transparent" cx="36.944" cy="28.631" r="29.105" />
  <path
    d="M37.443-3.5c8.951 0 16.531 3.105 22.742 9.315C66.393 11.987 69.5 19.548 69.5 28.5c0 8.954-3.049 16.457-9.145 22.514-6.437 6.324-14.076 9.486-22.912 9.486-8.649 0-16.153-3.143-22.514-9.429C8.644 44.786 5.5 37.264 5.5 28.501c0-8.723 3.144-16.285 9.429-22.685C21.138-.395 28.643-3.5 37.443-3.5zm.114 5.772c-7.276 0-13.428 2.572-18.457 7.715-5.22 5.296-7.829 11.467-7.829 18.513 0 7.125 2.59 13.257 7.77 18.4 5.181 5.182 11.352 7.771 18.514 7.771 7.123 0 13.334-2.609 18.629-7.828 5.029-4.876 7.543-10.99 7.543-18.343 0-7.313-2.553-13.485-7.656-18.513-5.067-5.145-11.239-7.715-18.514-7.715zM23.271 23.985c.609-3.924 2.189-6.962 4.742-9.114 2.552-2.152 5.656-3.228 9.314-3.228 5.027 0 9.029 1.62 12 4.856 2.971 3.238 4.457 7.391 4.457 12.457 0 4.915-1.543 9-4.627 12.256-3.088 3.256-7.086 4.886-12.002 4.886-3.619 0-6.743-1.085-9.371-3.257-2.629-2.172-4.209-5.257-4.743-9.257H31.1c.19 3.886 2.533 5.829 7.029 5.829 2.246 0 4.057-.972 5.428-2.914 1.373-1.942 2.059-4.534 2.059-7.771 0-3.391-.629-5.971-1.885-7.743-1.258-1.771-3.066-2.657-5.43-2.657-4.268 0-6.667 1.885-7.2 5.656h2.343l-6.342 6.343-6.343-6.343 2.512.001z"
  />
</svg>
</div>
        </div>

  </div>
    <div>
      <a
        href="https://github.com/michenriksen/hugo-theme-til"
        title="Today I Learned &#8212; A Hugo theme by Michael Henriksen"
        data-theme-version="0.4.0"
        >theme: til</a
      >
    </div>
</section>

      </footer>
    </div>

    
    <button id="back-to-top" title="Go to top">‚òùÔ∏è</button>


    
    

    
    <script src="/js/back-to-top.js"></script>

     
    <script src="/js/cat-cursor.js" defer></script>
  </body>
</html>
