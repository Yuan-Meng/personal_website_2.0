<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Yuan Meng</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Yuan Meng</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>mycaptainmy@gmail.com (Yuan Meng)</managingEditor>
    <webMaster>mycaptainmy@gmail.com (Yuan Meng)</webMaster>
    <copyright>Yuan Meng</copyright>
    <lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ML Interview 2.0: Research Engineering and Scary Rounds</title>
      <link>http://localhost:1313/posts/mle_interviews_2.0/</link>
      <pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/mle_interviews_2.0/</guid>
      <description>&lt;h2 id=&#34;recap-standard-mle-interviews&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Recap: &amp;ldquo;Standard&amp;rdquo; MLE Interviews&#xA;  &#xA;    &lt;a href=&#34;#recap-standard-mle-interviews&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;In 2025, most FAANG or similar companies still have the same rounds of MLE interviews, most typically coding, ML system design, ML fundamentals, and behavior. Some companies have made slight changes. For instance, Meta added an OA round even for senior candidates as well as an AI-assisted coding round. Companies like Google, LinkedIn, and many startups (e.g., xAI, Perplexity) now conduct in-person onsite interviews. I wrote about how to prepare for &amp;ldquo;standard&amp;rdquo; ML interviews in my 2024 &lt;a href=&#34;http://localhost:1313/posts/mle_interviews/&#34; class=&#34;backlink&#34;&gt;blogpost&lt;/a&gt;&#xA;  &#xA;   &amp;mdash; below is a recap.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Preparing for ML Infra System Design Interviews</title>
      <link>http://localhost:1313/posts/ml_infra_interviews/</link>
      <pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/ml_infra_interviews/</guid>
      <description>&lt;h2 id=&#34;dilemma-model-builders--infra-builders&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Dilemma: Model Builders != Infra Builders&#xA;  &#xA;    &lt;a href=&#34;#dilemma-model-builders--infra-builders&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;A handful of companies like Netflix, Snap, Reddit, Notion, and DoorDash have an ML infra system design round for MLE candidates &amp;mdash; in addition to standard ML system design. Maybe you&amp;rsquo;ll never have to interview with them. However, apart from the frontier AI &lt;span class=&#34;sidenote&#34;&gt;&#xA;  &lt;input&#xA;    aria-label=&#34;Show sidenote&#34;&#xA;    type=&#34;checkbox&#34;&#xA;    id=&#34;sidenote-checkbox-01&#34;&#xA;    class=&#34;sidenote-checkbox hidden&#34;&#xA;  /&gt;&#xA;  &lt;label&#xA;    tabindex=&#34;0&#34;&#xA;    role=&#34;mark&#34;&#xA;    aria-details=&#34;sidenote-01&#34;&#xA;    for=&#34;sidenote-checkbox-01&#34;&#xA;    class=&#34;sidenote-mark&#34;&#xA;    &gt;labs&lt;/label&#xA;  &gt;&#xA;  &lt;small id=&#34;sidenote-01&#34; class=&#34;sidenote-content&#34;&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt; (sidenote: &lt;/span&gt;If you do get an offer from a frontier AI lab but outside of the research org, you don&#39;t necessarily get paid more than an MLE at Netflix or Snap at the same level (e.g., OpenAI L4 Research Engineer vs. Snap/Netflix L5 MLE).&lt;span class=&#34;sr-only&#34;&gt;)&lt;/span&gt;&#xA;  &lt;/small&gt;&#xA;&lt;/span&gt;&#xA; (e.g., OpenAI, Anthropic, xAI, DeepMind, Meta TBD, Thinking Machines Lab, Reflection), the first two pay more than other companies are able to match at the same level. So I feel that many talented MLEs are incentivized to pass their interviews at some point in their careers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Is Generative Recommendation the ChatGPT Moment of RecSys?</title>
      <link>http://localhost:1313/posts/generative_recommendation/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/generative_recommendation/</guid>
      <description>&lt;h2 id=&#34;has-the-tide-turned-from-dlrm-to-gr&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Has the Tide Turned? From DLRM to GR&#xA;  &#xA;    &lt;a href=&#34;#has-the-tide-turned-from-dlrm-to-gr&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;For nearly a decade, recommender systems have remained largely &lt;span class=&#34;sidenote&#34;&gt;&#xA;  &lt;input&#xA;    aria-label=&#34;Show sidenote&#34;&#xA;    type=&#34;checkbox&#34;&#xA;    id=&#34;sidenote-checkbox-01&#34;&#xA;    class=&#34;sidenote-checkbox hidden&#34;&#xA;  /&gt;&#xA;  &lt;label&#xA;    tabindex=&#34;0&#34;&#xA;    role=&#34;mark&#34;&#xA;    aria-details=&#34;sidenote-01&#34;&#xA;    for=&#34;sidenote-checkbox-01&#34;&#xA;    class=&#34;sidenote-mark&#34;&#xA;    &gt;the same&lt;/label&#xA;  &gt;&#xA;  &lt;small id=&#34;sidenote-01&#34; class=&#34;sidenote-content&#34;&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt; (sidenote: &lt;/span&gt;It used to be (still is?) the case that if you&#39;re familiar with the cascade pipeline and the most popular L1 (e.g., two-tower models and embedding-based retrieval) and L2 (e.g., &#34;Embedding-MLP&#34; style `pAction` models, sequence modeling) architectures, you&#39;re golden in almost every ML system design interview. Perhaps a year from now, GenRec talents and experience will be what top companies seek instead.&lt;span class=&#34;sr-only&#34;&gt;)&lt;/span&gt;&#xA;  &lt;/small&gt;&#xA;&lt;/span&gt;&#xA;. It&amp;rsquo;s hard to even imagine a system without a cascade pipeline in the iconic &lt;a href=&#34;https://research.google.com/pubs/archive/45530.pdf&#34;&gt;YouTube paper&lt;/a&gt;, which retrieves tens of thousands of candidates from a massive corpus, trims them down to hundreds of relevant items using a lightweight ranker (L1), selects the top dozen using a heavy ranker (L2), and makes adjustments based on policy and business logic (L3). Architecture-wise, the L2 ranker hasn&amp;rsquo;t drifted far from the seminal &lt;a href=&#34;https://arxiv.org/abs/1606.07792&#34;&gt;Deep &amp;amp; Wide network&lt;/a&gt;, which embeds input features, passes them through interaction modules, and transforms representations for task heads (e.g., clicks, purchase, video watch). Upgrades to feature interaction (e.g., &lt;a href=&#34;https://arxiv.org/abs/2008.13535&#34;&gt;DCN-v2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2102.07619&#34;&gt;MaskNet&lt;/a&gt;) and multi-task learning (e.g., &lt;a href=&#34;https://arxiv.org/abs/2311.09580&#34;&gt;MMoE&lt;/a&gt;, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3383313.3412236&#34;&gt;PLE&lt;/a&gt;) culminated in Meta&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2203.11014&#34;&gt;DHEN&lt;/a&gt;, which combines multiple interaction modules and experts to push the limits of this &amp;ldquo;Deep Learning Recommender System&amp;rdquo; (DLRM) paradigm.&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://www.dropbox.com/scl/fi/96m8zb5yps9ffz9geheu7/Screenshot-2025-07-20-at-11.07.10-PM.png?rlkey=q4xtbxt3r50okrs2zo9vac2xq&amp;amp;st=fzobjxgt&amp;amp;raw=1&#34;&#xA;    alt=&#34;Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style &amp;lsquo;Embedding &amp;amp; Interaction &amp;amp; Expert&amp;rsquo; model architectures.&#34; width=&#34;1800&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;p&gt;Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style &amp;lsquo;Embedding &amp;amp; Interaction &amp;amp; Expert&amp;rsquo; model architectures.&lt;/p&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;In 2025, the tide seems to have finally turned after Meta&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2402.17152&#34;&gt;HSTU&lt;/a&gt; delivered perhaps the biggest offline/online metric and serving efficiency gains in recent years &amp;mdash; other top companies such as &lt;span class=&#34;sidenote&#34;&gt;&#xA;  &lt;input&#xA;    aria-label=&#34;Show sidenote&#34;&#xA;    type=&#34;checkbox&#34;&#xA;    id=&#34;sidenote-checkbox-03&#34;&#xA;    class=&#34;sidenote-checkbox hidden&#34;&#xA;  /&gt;&#xA;  &lt;label&#xA;    tabindex=&#34;0&#34;&#xA;    role=&#34;mark&#34;&#xA;    aria-details=&#34;sidenote-03&#34;&#xA;    for=&#34;sidenote-checkbox-03&#34;&#xA;    class=&#34;sidenote-mark&#34;&#xA;    &gt;Google&lt;/label&#xA;  &gt;&#xA;  &lt;small id=&#34;sidenote-03&#34; class=&#34;sidenote-content&#34;&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt; (sidenote: &lt;/span&gt;Google DeepMind published TIGER a year before HSTU, but it was used for retrieval only. Meta might have been the major influence behind using Generative Recommendation for both retrieval and ranking.&lt;span class=&#34;sr-only&#34;&gt;)&lt;/span&gt;&#xA;  &lt;/small&gt;&#xA;&lt;/span&gt;&#xA;, Kuaishou, Meituan, Alibaba, Netflix, Xiaohongshu, ByteDance, Tencent, Baidu, and JD.com are starting to embrace a new &amp;ldquo;Generative Recommendation&amp;rdquo; (GR) paradigm for retrieval and ranking, reframing the discriminative &lt;code&gt;pAction&lt;/code&gt; prediction task as a generative task, akin to token predictions in language modeling.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hardware-Aware Attention for Long Sequence Modeling</title>
      <link>http://localhost:1313/posts/hardware_aware_transformers/</link>
      <pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/hardware_aware_transformers/</guid>
      <description>&lt;h2 id=&#34;attention-is-all-you-need-----if-you-can-afford-the-on2-complexity&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Attention Is All You Need &amp;mdash; if You Can Afford the $O(N^2)$ Complexity&#xA;  &#xA;    &lt;a href=&#34;#attention-is-all-you-need-----if-you-can-afford-the-on2-complexity&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Attention is key to the success of large language models (LLMs). By attending to all (unmasked) tokens in the input sequence at once, attention-based Transformers overcome RNNs&amp;rsquo; difficulty in modeling long-range dependencies, avoiding vanishing and exploding gradients. However, with the power to &amp;ldquo;attend to all&amp;rdquo; comes hefty costs.&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://www.dropbox.com/scl/fi/m8vdwmpqwt40c896ty24v/Screenshot-2025-03-15-at-11.37.40-PM.png?rlkey=t6852oqzse600dc48gjg7rfal&amp;amp;st=r3h14cla&amp;amp;raw=1&#34;&#xA;    alt=&#34;Writing materialized $\mathbf{S}$, $\mathbf{A}$, and $\mathbf{O}$ to the GPU&amp;rsquo;s high-bandwidth memory (HBM) has an $O(N^2)$ IO complexity.&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;p&gt;Writing materialized $\mathbf{S}$, $\mathbf{A}$, and $\mathbf{O}$ to the GPU&amp;rsquo;s high-bandwidth memory (HBM) has an $O(N^2)$ IO complexity.&lt;/p&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;</description>
    </item>
    <item>
      <title>How I&#39;d Start My Engineering Career All Over Again</title>
      <link>http://localhost:1313/posts/career_reflection/</link>
      <pubDate>Wed, 25 Dec 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/career_reflection/</guid>
      <description>&lt;h2 id=&#34;foreword-to-the-ambitious-and-confused-early-career-engineers&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Foreword: To the Ambitious (and Confused) Early Career Engineers&#xA;  &#xA;    &lt;a href=&#34;#foreword-to-the-ambitious-and-confused-early-career-engineers&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;I had far too chill parents growing up. The first day they dropped me off at school, I had no idea why I was there. I sat quietly, confused, for about a year before realizing there was such a thing as education. Similarly, when I started my first job at DoorDash in Summer 2022 (or shall I say &amp;ldquo;Q3&amp;rdquo;), I had far too chill a manager during my first 6 months and only gradually figured out what a career in engineering is about.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Down the Rabbit Hole: Sequential User Modeling</title>
      <link>http://localhost:1313/posts/seq_user_modeling/</link>
      <pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/seq_user_modeling/</guid>
      <description>&lt;h2 id=&#34;catch-the-train-of-actions&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Catch the Train of Actions&#xA;  &#xA;    &lt;a href=&#34;#catch-the-train-of-actions&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Shown below is my Amazon browsing history last week. Any recommendations on what I might buy next?&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://www.dropbox.com/scl/fi/t3fs6pgs8rxlysnpoqeq9/Screenshot-2024-11-11-at-4.25.33-PM.png?rlkey=kv37yorsm6qdlro4y4ot496s8&amp;amp;st=p200ep0j&amp;amp;raw=1&#34;&#xA;    alt=&#34;Yuan&amp;rsquo;s Amazon browsing history last week; distinct sessions are color-coded.&#34; width=&#34;1800&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;p&gt;Yuan&amp;rsquo;s Amazon browsing history last week; distinct sessions are color-coded.&lt;/p&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;</description>
    </item>
    <item>
      <title>(Opinionated) Guide to ML Engineer Job Hunting</title>
      <link>http://localhost:1313/posts/mle_interviews/</link>
      <pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/mle_interviews/</guid>
      <description>&lt;h2 id=&#34;the-marriage-analogy&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  The Marriage Analogy&#xA;  &#xA;    &lt;a href=&#34;#the-marriage-analogy&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Over 2 years ago, I wrote a &lt;a href=&#34;https://www.yuan-meng.com/posts/newgrads/&#34;&gt;blog post&lt;/a&gt; on how to find jobs as a new grad data scientist (as a twist of fate, I never worked as a product data scientist but instead became an ML engineer at DoorDash). Back in 2021, I cared a ton about interview skills, answer &amp;ldquo;frameworks&amp;rdquo;, and whatnot, which may still come handy at New Grad or Early Career levels. For experienced hires, however, I think of interviews as some sort of marriage proposal &amp;mdash; &lt;em&gt;it&amp;rsquo;s something you can rehearse but can never force&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Is Human Vision More like CNN or Vision Transformer?</title>
      <link>http://localhost:1313/posts/human_vision/</link>
      <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/human_vision/</guid>
      <description>&lt;h2 id=&#34;engineer-the-mind&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Engineer the Mind&#xA;  &#xA;    &lt;a href=&#34;#engineer-the-mind&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;In Winter 2015, after coming back from grad school interviews in the States, I told my dad over hotpot that I was going to study &lt;a href=&#34;https://en.wikipedia.org/wiki/Cognitive_science&#34;&gt;cognitive science&lt;/a&gt; at Berkeley.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Negative Sampling for Learning Two-Tower Networks</title>
      <link>http://localhost:1313/posts/negative_sampling/</link>
      <pubDate>Mon, 02 Sep 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/negative_sampling/</guid>
      <description>&lt;h2 id=&#34;web-scale-recommender-systems&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Web-Scale Recommender Systems&#xA;  &#xA;    &lt;a href=&#34;#web-scale-recommender-systems&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;two-stage-architecture&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Two-Stage Architecture&#xA;  &#xA;    &lt;a href=&#34;#two-stage-architecture&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The iconic YouTube paper (&lt;a href=&#34;https://research.google/pubs/deep-neural-networks-for-youtube-recommendations/&#34;&gt;Covington et al., 2016&lt;/a&gt;) introduced a two-stage architecture that since became the industry standard for large-scale recommender systems:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Retrieval&lt;/strong&gt; (a.k.a. &amp;ldquo;candidate generation&amp;rdquo;): Quickly select top k (in the hundreds or thousands) loosely relevant items from a large corpus of billions&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Ranking&lt;/strong&gt; (a.k.a. &amp;ldquo;reranking&amp;rdquo;): Order final candidates (dozens) by predicted reward probability (e.g., an ads click, a listing booking, a video watch, &lt;em&gt;etc.&lt;/em&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>The Annotated Multi-Task Ranker: An MMoE Code Example</title>
      <link>http://localhost:1313/posts/mtml/</link>
      <pubDate>Fri, 05 Jul 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/mtml/</guid>
      <description>&lt;p&gt;Natural Language Processing (NLP) has an abundance of intuitively explained tutorials with code, such as Andrej Kaparthy&amp;rsquo;s &lt;a href=&#34;https://karpathy.ai/zero-to-hero.html&#34;&gt;Neural Networks: Zero to Hero&lt;/a&gt;, the viral &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;The Illustrated Transformer&lt;/a&gt; and its successor &lt;a href=&#34;https://nlp.seas.harvard.edu/annotated-transformer/&#34;&gt;The Annotated Transformer&lt;/a&gt;, Umar Jamil&amp;rsquo;s YouTube &lt;a href=&#34;https://www.youtube.com/@umarjamilai&#34;&gt;series&lt;/a&gt; dissecting SOTA models and the companion &lt;a href=&#34;https://github.com/hkproj&#34;&gt;repo&lt;/a&gt;, among others.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Introduction to Embedding-Based Retrieval</title>
      <link>http://localhost:1313/posts/ebr/</link>
      <pubDate>Sat, 22 Jun 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/ebr/</guid>
      <description>&lt;h2 id=&#34;so-what-is-an-embedding&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  So, What is an Embedding?&#xA;  &#xA;    &lt;a href=&#34;#so-what-is-an-embedding&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embedding&#34;&gt;Embedding&lt;/a&gt; is a classic idea in mathematical topology and machine learning (click â–¶ for definitions). You can think of embeddings as a special type of vectors.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Attention as Soft Dictionary Lookup</title>
      <link>http://localhost:1313/posts/attention_as_dict/</link>
      <pubDate>Sat, 09 Mar 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/attention_as_dict/</guid>
      <description>&lt;h2 id=&#34;the-dictionary-metaphor-&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  The Dictionary Metaphor ðŸ”‘ðŸ“š&#xA;  &#xA;    &lt;a href=&#34;#the-dictionary-metaphor-&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>An Evolution of Learning to Rank</title>
      <link>http://localhost:1313/posts/ltr/</link>
      <pubDate>Sat, 17 Feb 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/ltr/</guid>
      <description>&lt;h2 id=&#34;first-thing-first&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  First Thing First&#xA;  &#xA;    &lt;a href=&#34;#first-thing-first&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Enigmas of the universe &lt;br/&gt; Cannot be known without a search &lt;br/&gt; &amp;mdash; Epica, &lt;a href=&#34;https://open.spotify.com/track/34Oz0bzAq7E1aUnKksPfJJ?si=9eabd1446a6a4ccc&#34;&gt;&lt;em&gt;Omega&lt;/em&gt;&lt;/a&gt; (2021)&lt;/p&gt;&#xA;&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>Autocompletion for Search Enginees</title>
      <link>http://localhost:1313/posts/autocomplete/</link>
      <pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/autocomplete/</guid>
      <description>&lt;p&gt;Autocompletion dates back half a century ago (&lt;a href=&#34;https://www.doc.ic.ac.uk/~shm/MI/mi3.html&#34;&gt;Longuet-Higgins &amp;amp; Ortony, 1968&lt;/a&gt;), initially designed to save keystrokes as people type and help those with physical disabilities type faster. The incomplete user input is the &lt;strong&gt;&amp;ldquo;query prefix&amp;rdquo;&lt;/strong&gt; and suggested ways of extending the prefix into a full query are &lt;strong&gt;&amp;ldquo;query completions&amp;rdquo;&lt;/strong&gt;. This feature is essential to modern text editors and search engines.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Teaching A Peceptron to See</title>
      <link>http://localhost:1313/posts/perceptron/</link>
      <pubDate>Sun, 02 Jul 2023 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/perceptron/</guid>
      <description>&lt;p&gt;Ten years after the ImageNet Challenge thawed the last AI winter, ChaptGPT and generative AI have become part of our everyday life and colloquial language, like (almost) no one has imagined just 2 years back. As increasingly more folks aspire to foray into the field of ML/AI, I can&amp;rsquo;t help but think about a lesson from my guitar teacher:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Code ML Algorithms From Scratch</title>
      <link>http://localhost:1313/posts/md_coding/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/md_coding/</guid>
      <description>&lt;p&gt;Coding interviews can mean different things for &amp;ldquo;traditional&amp;rdquo; software engineers (back-end, front-end, full-stack, etc.) and engineers with a machine learning focus. Apart from LeetCode-style questions, ML engineers (as well as applied scientists, research engineers, and, occasionally, machine learning data scientists) may be asked to implement a classic ML algorithm from scratch during an interview.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Quirky) Roadmap for New Grad Data Scientists</title>
      <link>http://localhost:1313/posts/newgrads/</link>
      <pubDate>Sun, 13 Mar 2022 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/newgrads/</guid>
      <description>&lt;h2 id=&#34;new-grad-timeline&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  New Grad Timeline&#xA;  &#xA;    &lt;a href=&#34;#new-grad-timeline&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Since last November, I regularly get questions about how to get data science jobs. I&amp;rsquo;ve hesitated to give advice because no advice applies to everyone. As more people ask, however, I want to write a post for those in my shoes:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;(PhD) students applying to machine learning or product data scientist roles at tech companies through university recruiting.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>Causal Inference in Data Science</title>
      <link>http://localhost:1313/posts/causality/causality/</link>
      <pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/causality/causality/</guid>
      <description>&lt;p&gt;What do you see? ðŸ‘€&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://www.dropbox.com/s/gfiw187r502jb1l/1t1_redgreen_2sec.gif?raw=1&#34; width=&#34;350&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;You&amp;rsquo;re probably thinking, &lt;em&gt;&amp;ldquo;The red block stopped, right after which the green block started to move coincidentally.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Nice try&amp;hellip; I know what you&amp;rsquo;re thinking, &lt;em&gt;&amp;ldquo;The red block &lt;u&gt;made&lt;/u&gt; the green one move&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Choosing Metrics</title>
      <link>http://localhost:1313/posts/metrics/</link>
      <pubDate>Sat, 06 Nov 2021 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/metrics/</guid>
      <description>&lt;p&gt;&amp;lsquo;Tis the college and job application season of the year. If only schools and companies have crystal balls to see into each candidate&amp;rsquo;s future achievements, they need not interview people; since they do not have such things, SAT scores, GPA, internships, and other quantifiable metrics are used to aid decisions. Simplified metrics are by no means ideal â€” As Goodhart put it (often &lt;a href=&#34;https://en.wikipedia.org/wiki/Goodhart%27s_law&#34;&gt;paraphrased&lt;/a&gt;), &amp;ldquo;When a measure becomes a metric, it ceases to be a good measure.&amp;rdquo; However, the opposite is worse: Making critical decisions based on heuristics, biases, and personal opinions.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
