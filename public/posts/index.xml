<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Yuan Meng</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Yuan Meng</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>mycaptainmy@gmail.com (Yuan Meng)</managingEditor>
    <webMaster>mycaptainmy@gmail.com (Yuan Meng)</webMaster>
    <copyright>Yuan Meng</copyright>
    <lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Down the Rabbit Hole: Sequential User Modeling</title>
      <link>http://localhost:1313/posts/seq_user_modeling/</link>
      <pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/seq_user_modeling/</guid>
      <description>&lt;h2 id=&#34;catch-the-train-of-actions&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Catch the Train of Actions&#xA;  &#xA;    &lt;a href=&#34;#catch-the-train-of-actions&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Shown below is my Amazon browsing history last week. Any recommendations on what I might buy next?&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://www.dropbox.com/scl/fi/t3fs6pgs8rxlysnpoqeq9/Screenshot-2024-11-11-at-4.25.33-PM.png?rlkey=kv37yorsm6qdlro4y4ot496s8&amp;amp;st=p200ep0j&amp;amp;raw=1&#34;&#xA;    alt=&#34;Yuan&amp;rsquo;s Amazon browsing history last week; distinct sessions are color-coded.&#34; width=&#34;1800&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;p&gt;Yuan&amp;rsquo;s Amazon browsing history last week; distinct sessions are color-coded.&lt;/p&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;</description>
    </item>
    <item>
      <title>(Opinionated) Guide to ML Engineer Job Hunting</title>
      <link>http://localhost:1313/posts/mle_interviews/</link>
      <pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/mle_interviews/</guid>
      <description>&lt;h2 id=&#34;the-marriage-analogy&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  The Marriage Analogy&#xA;  &#xA;    &lt;a href=&#34;#the-marriage-analogy&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Over 2 years ago, I wrote a &lt;a href=&#34;https://www.yuan-meng.com/posts/newgrads/&#34;&gt;blog post&lt;/a&gt; on how to find jobs as a new grad data scientist (as a twist of fate, I never worked as a product data scientist but instead became an ML engineer at DoorDash). Back in 2021, I cared a ton about interview skills, answer &amp;ldquo;frameworks&amp;rdquo;, and whatnot, which may still come handy at New Grad or Early Career levels. For experienced hires, however, I think of interviews as some sort of marriage proposal &amp;mdash; &lt;em&gt;it&amp;rsquo;s something you can rehearse but can never force&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Is Human Vision More like CNN or Vision Transformer?</title>
      <link>http://localhost:1313/posts/human_vision/</link>
      <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/human_vision/</guid>
      <description>&lt;h2 id=&#34;engineer-the-mind&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Engineer the Mind&#xA;  &#xA;    &lt;a href=&#34;#engineer-the-mind&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;In Winter 2015, after coming back from grad school interviews in the States, I told my dad over hotpot that I was going to study &lt;a href=&#34;https://en.wikipedia.org/wiki/Cognitive_science&#34;&gt;cognitive science&lt;/a&gt; at Berkeley.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Negative Sampling for Learning Two-Tower Networks</title>
      <link>http://localhost:1313/posts/negative_sampling/</link>
      <pubDate>Mon, 02 Sep 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/negative_sampling/</guid>
      <description>&lt;h2 id=&#34;web-scale-recommender-systems&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Web-Scale Recommender Systems&#xA;  &#xA;    &lt;a href=&#34;#web-scale-recommender-systems&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;two-stage-architecture&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Two-Stage Architecture&#xA;  &#xA;    &lt;a href=&#34;#two-stage-architecture&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The iconic YouTube paper (&lt;a href=&#34;https://research.google/pubs/deep-neural-networks-for-youtube-recommendations/&#34;&gt;Covington et al., 2016&lt;/a&gt;) introduced a two-stage architecture that since became the industry standard for large-scale recommender systems:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Retrieval&lt;/strong&gt; (a.k.a. &amp;ldquo;candidate generation&amp;rdquo;): Quickly select top k (in the hundreds or thousands) loosely relevant items from a large corpus of billions&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Ranking&lt;/strong&gt; (a.k.a. &amp;ldquo;reranking&amp;rdquo;): Order final candidates (dozens) by predicted reward probability (e.g., an ads click, a listing booking, a video watch, &lt;em&gt;etc.&lt;/em&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>The Annotated Multi-Task Ranker: An MMoE Code Example</title>
      <link>http://localhost:1313/posts/mtml/</link>
      <pubDate>Fri, 05 Jul 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/mtml/</guid>
      <description>&lt;p&gt;Natural Language Processing (NLP) has an abundance of intuitively explained tutorials with code, such as Andrej Kaparthy&amp;rsquo;s &lt;a href=&#34;https://karpathy.ai/zero-to-hero.html&#34;&gt;Neural Networks: Zero to Hero&lt;/a&gt;, the viral &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;The Illustrated Transformer&lt;/a&gt; and its successor &lt;a href=&#34;https://nlp.seas.harvard.edu/annotated-transformer/&#34;&gt;The Annotated Transformer&lt;/a&gt;, Umar Jamil&amp;rsquo;s YouTube &lt;a href=&#34;https://www.youtube.com/@umarjamilai&#34;&gt;series&lt;/a&gt; dissecting SOTA models and the companion &lt;a href=&#34;https://github.com/hkproj&#34;&gt;repo&lt;/a&gt;, among others.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Introduction to Embedding-Based Retrieval</title>
      <link>http://localhost:1313/posts/ebr/</link>
      <pubDate>Sat, 22 Jun 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/ebr/</guid>
      <description>&lt;h2 id=&#34;so-what-is-an-embedding&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  So, What is an Embedding?&#xA;  &#xA;    &lt;a href=&#34;#so-what-is-an-embedding&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embedding&#34;&gt;Embedding&lt;/a&gt; is a classic idea in mathematical topology and machine learning (click ▶ for definitions). You can think of embeddings as a special type of vectors.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Attention as Soft Dictionary Lookup</title>
      <link>http://localhost:1313/posts/attention_as_dict/</link>
      <pubDate>Sat, 09 Mar 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/attention_as_dict/</guid>
      <description>&lt;h2 id=&#34;the-dictionary-metaphor-&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  The Dictionary Metaphor 🔑📚&#xA;  &#xA;    &lt;a href=&#34;#the-dictionary-metaphor-&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>An Evolution of Learning to Rank</title>
      <link>http://localhost:1313/posts/ltr/</link>
      <pubDate>Sat, 17 Feb 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/ltr/</guid>
      <description>&lt;h2 id=&#34;first-thing-first&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  First Thing First&#xA;  &#xA;    &lt;a href=&#34;#first-thing-first&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Enigmas of the universe &lt;br/&gt; Cannot be known without a search &lt;br/&gt; &amp;mdash; Epica, &lt;a href=&#34;https://open.spotify.com/track/34Oz0bzAq7E1aUnKksPfJJ?si=9eabd1446a6a4ccc&#34;&gt;&lt;em&gt;Omega&lt;/em&gt;&lt;/a&gt; (2021)&lt;/p&gt;&#xA;&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>Autocompletion for Search Enginees</title>
      <link>http://localhost:1313/posts/autocomplete/</link>
      <pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/autocomplete/</guid>
      <description>&lt;p&gt;Autocompletion dates back half a century ago (&lt;a href=&#34;https://www.doc.ic.ac.uk/~shm/MI/mi3.html&#34;&gt;Longuet-Higgins &amp;amp; Ortony, 1968&lt;/a&gt;), initially designed to save keystrokes as people type and help those with physical disabilities type faster. The incomplete user input is the &lt;strong&gt;&amp;ldquo;query prefix&amp;rdquo;&lt;/strong&gt; and suggested ways of extending the prefix into a full query are &lt;strong&gt;&amp;ldquo;query completions&amp;rdquo;&lt;/strong&gt;. This feature is essential to modern text editors and search engines.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Teaching A Peceptron to See</title>
      <link>http://localhost:1313/posts/perceptron/</link>
      <pubDate>Sun, 02 Jul 2023 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/perceptron/</guid>
      <description>&lt;p&gt;Ten years after the ImageNet Challenge thawed the last AI winter, ChaptGPT and generative AI have become part of our everyday life and colloquial language, like (almost) no one has imagined just 2 years back. As increasingly more folks aspire to foray into the field of ML/AI, I can&amp;rsquo;t help but think about a lesson from my guitar teacher:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Code ML Algorithms From Scratch</title>
      <link>http://localhost:1313/posts/md_coding/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/md_coding/</guid>
      <description>&lt;p&gt;Coding interviews can mean different things for &amp;ldquo;traditional&amp;rdquo; software engineers (back-end, front-end, full-stack, etc.) and engineers with a machine learning focus. Apart from LeetCode-style questions, ML engineers (as well as applied scientists, research engineers, and, occasionally, machine learning data scientists) may be asked to implement a classic ML algorithm from scratch during an interview.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Quirky) Roadmap for New Grad Data Scientists</title>
      <link>http://localhost:1313/posts/newgrads/</link>
      <pubDate>Sun, 13 Mar 2022 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/newgrads/</guid>
      <description>&lt;h2 id=&#34;new-grad-timeline&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  New Grad Timeline&#xA;  &#xA;    &lt;a href=&#34;#new-grad-timeline&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Since last November, I regularly get questions about how to get data science jobs. I&amp;rsquo;ve hesitated to give advice because no advice applies to everyone. As more people ask, however, I want to write a post for those in my shoes:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;(PhD) students applying to machine learning or product data scientist roles at tech companies through university recruiting.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>Causal Inference in Data Science</title>
      <link>http://localhost:1313/posts/causality/causality/</link>
      <pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/causality/causality/</guid>
      <description>&lt;p&gt;What do you see? 👀&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://www.dropbox.com/s/gfiw187r502jb1l/1t1_redgreen_2sec.gif?raw=1&#34; width=&#34;350&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;You&amp;rsquo;re probably thinking, &lt;em&gt;&amp;ldquo;The red block stopped, right after which the green block started to move coincidentally.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Nice try&amp;hellip; I know what you&amp;rsquo;re thinking, &lt;em&gt;&amp;ldquo;The red block &lt;u&gt;made&lt;/u&gt; the green one move&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Choosing Metrics</title>
      <link>http://localhost:1313/posts/metrics/</link>
      <pubDate>Sat, 06 Nov 2021 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/metrics/</guid>
      <description>&lt;p&gt;&amp;lsquo;Tis the college and job application season of the year. If only schools and companies have crystal balls to see into each candidate&amp;rsquo;s future achievements, they need not interview people; since they do not have such things, SAT scores, GPA, internships, and other quantifiable metrics are used to aid decisions. Simplified metrics are by no means ideal — As Goodhart put it (often &lt;a href=&#34;https://en.wikipedia.org/wiki/Goodhart%27s_law&#34;&gt;paraphrased&lt;/a&gt;), &amp;ldquo;When a measure becomes a metric, it ceases to be a good measure.&amp;rdquo; However, the opposite is worse: Making critical decisions based on heuristics, biases, and personal opinions.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
