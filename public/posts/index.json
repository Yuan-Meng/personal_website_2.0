[{"categories":["Career","Machine learning","Interview"],"date":"2025-12-12T00:00:00Z","id":"/posts/mle_interviews_2.0/","keywords":[],"summary":"\u003ch2 id=\"recap-standard-mle-interviews\" class=\"scroll-mt-8 group\"\u003e\n  Recap: \u0026ldquo;Standard\u0026rdquo; MLE Interviews\n  \n    \u003ca href=\"#recap-standard-mle-interviews\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cp\u003eThis is a sequel to my ML interview \u003ca href=\"/posts/mle_interviews/\" class=\"backlink\"\u003eblogpost\u003c/a\u003e\n  \n   in 2024. Back then, I argued that ML engineers are unicorns: the variety of ML interview rounds far exceeds that of other job families. A typical full loop had 6 rounds:\u003c/p\u003e","tags":[],"title":"Tackling Challenging ML + Research Engineering Interviews"},{"categories":["Career","Ml infra","Interview"],"date":"2025-12-01T00:00:00Z","id":"/posts/ml_infra_interviews/","keywords":[],"summary":"\u003ch2 id=\"dilemma-model-builders--infra-builders\" class=\"scroll-mt-8 group\"\u003e\n  Dilemma: Model Builders != Infra Builders\n  \n    \u003ca href=\"#dilemma-model-builders--infra-builders\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cp\u003eA handful of companies like Netflix, Snap, Reddit, Notion, and DoorDash have an ML infra system design round for MLE candidates \u0026mdash; in addition to standard ML system design. Maybe you\u0026rsquo;ll never have to interview with them. However, apart from the frontier AI \u003cspan class=\"sidenote\"\u003e\n  \u003cinput\n    aria-label=\"Show sidenote\"\n    type=\"checkbox\"\n    id=\"sidenote-checkbox-01\"\n    class=\"sidenote-checkbox hidden\"\n  /\u003e\n  \u003clabel\n    tabindex=\"0\"\n    role=\"mark\"\n    aria-details=\"sidenote-01\"\n    for=\"sidenote-checkbox-01\"\n    class=\"sidenote-mark\"\n    \u003elabs\u003c/label\n  \u003e\n  \u003csmall id=\"sidenote-01\" class=\"sidenote-content\"\u003e\n    \u003cspan class=\"sr-only\"\u003e (sidenote: \u003c/span\u003eIf you do get an offer from a frontier AI lab but outside of the research org, you don't necessarily get paid more than an MLE at Netflix or Snap at the same level (e.g., OpenAI L4 Research Engineer vs. Snap/Netflix L5 MLE).\u003cspan class=\"sr-only\"\u003e)\u003c/span\u003e\n  \u003c/small\u003e\n\u003c/span\u003e\n (e.g., OpenAI, Anthropic, xAI, DeepMind, Meta TBD, Thinking Machines Lab, Reflection), the first two pay more than other companies are able to match at the same level. So I feel that many talented MLEs are incentivized to pass their interviews at some point in their careers.\u003c/p\u003e","tags":[],"title":"Preparing for ML Infra System Design Interviews"},{"categories":["Generative recommendation","Large language models"],"date":"2025-08-03T00:00:00Z","id":"/posts/generative_recommendation/","keywords":[],"summary":"\u003ch2 id=\"has-the-tide-turned-from-dlrm-to-gr\" class=\"scroll-mt-8 group\"\u003e\n  Has the Tide Turned? From DLRM to GR\n  \n    \u003ca href=\"#has-the-tide-turned-from-dlrm-to-gr\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cp\u003eFor nearly a decade, recommender systems have remained largely \u003cspan class=\"sidenote\"\u003e\n  \u003cinput\n    aria-label=\"Show sidenote\"\n    type=\"checkbox\"\n    id=\"sidenote-checkbox-01\"\n    class=\"sidenote-checkbox hidden\"\n  /\u003e\n  \u003clabel\n    tabindex=\"0\"\n    role=\"mark\"\n    aria-details=\"sidenote-01\"\n    for=\"sidenote-checkbox-01\"\n    class=\"sidenote-mark\"\n    \u003ethe same\u003c/label\n  \u003e\n  \u003csmall id=\"sidenote-01\" class=\"sidenote-content\"\u003e\n    \u003cspan class=\"sr-only\"\u003e (sidenote: \u003c/span\u003eIt used to be (still is?) the case that if you're familiar with the cascade pipeline and the most popular L1 (e.g., two-tower models and embedding-based retrieval) and L2 (e.g., \"Embedding-MLP\" style `pAction` models, sequence modeling) architectures, you're golden in almost every ML system design interview. Perhaps a year from now, GenRec talents and experience will be what top companies seek instead.\u003cspan class=\"sr-only\"\u003e)\u003c/span\u003e\n  \u003c/small\u003e\n\u003c/span\u003e\n. It\u0026rsquo;s hard to even imagine a system without a cascade pipeline in the iconic \u003ca href=\"https://research.google.com/pubs/archive/45530.pdf\"\u003eYouTube paper\u003c/a\u003e, which retrieves tens of thousands of candidates from a massive corpus, trims them down to hundreds of relevant items using a lightweight ranker (L1), selects the top dozen using a heavy ranker (L2), and makes adjustments based on policy and business logic (L3). Architecture-wise, the L2 ranker hasn\u0026rsquo;t drifted far from the seminal \u003ca href=\"https://arxiv.org/abs/1606.07792\"\u003eDeep \u0026amp; Wide network\u003c/a\u003e, which embeds input features, passes them through interaction modules, and transforms representations for task heads (e.g., clicks, purchase, video watch). Upgrades to feature interaction (e.g., \u003ca href=\"https://arxiv.org/abs/2008.13535\"\u003eDCN-v2\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2102.07619\"\u003eMaskNet\u003c/a\u003e) and multi-task learning (e.g., \u003ca href=\"https://arxiv.org/abs/2311.09580\"\u003eMMoE\u003c/a\u003e, \u003ca href=\"https://dl.acm.org/doi/abs/10.1145/3383313.3412236\"\u003ePLE\u003c/a\u003e) culminated in Meta\u0026rsquo;s \u003ca href=\"https://arxiv.org/abs/2203.11014\"\u003eDHEN\u003c/a\u003e, which combines multiple interaction modules and experts to push the limits of this \u0026ldquo;Deep Learning Recommender System\u0026rdquo; (DLRM) paradigm.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src=\"https://www.dropbox.com/scl/fi/96m8zb5yps9ffz9geheu7/Screenshot-2025-07-20-at-11.07.10-PM.png?rlkey=q4xtbxt3r50okrs2zo9vac2xq\u0026amp;st=fzobjxgt\u0026amp;raw=1\"\n    alt=\"Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style \u0026lsquo;Embedding \u0026amp; Interaction \u0026amp; Expert\u0026rsquo; model architectures.\" width=\"1800\"\u003e\u003cfigcaption\u003e\n      \u003cp\u003eSince 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style \u0026lsquo;Embedding \u0026amp; Interaction \u0026amp; Expert\u0026rsquo; model architectures.\u003c/p\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn 2025, the tide seems to have finally turned after Meta\u0026rsquo;s \u003ca href=\"https://arxiv.org/abs/2402.17152\"\u003eHSTU\u003c/a\u003e delivered perhaps the biggest offline/online metric and serving efficiency gains in recent years \u0026mdash; other top companies such as \u003cspan class=\"sidenote\"\u003e\n  \u003cinput\n    aria-label=\"Show sidenote\"\n    type=\"checkbox\"\n    id=\"sidenote-checkbox-03\"\n    class=\"sidenote-checkbox hidden\"\n  /\u003e\n  \u003clabel\n    tabindex=\"0\"\n    role=\"mark\"\n    aria-details=\"sidenote-03\"\n    for=\"sidenote-checkbox-03\"\n    class=\"sidenote-mark\"\n    \u003eGoogle\u003c/label\n  \u003e\n  \u003csmall id=\"sidenote-03\" class=\"sidenote-content\"\u003e\n    \u003cspan class=\"sr-only\"\u003e (sidenote: \u003c/span\u003eGoogle DeepMind published TIGER a year before HSTU, but it was used for retrieval only. Meta might have been the major influence behind using Generative Recommendation for both retrieval and ranking.\u003cspan class=\"sr-only\"\u003e)\u003c/span\u003e\n  \u003c/small\u003e\n\u003c/span\u003e\n, Kuaishou, Meituan, Alibaba, Netflix, Xiaohongshu, ByteDance, Tencent, Baidu, and JD.com are starting to embrace a new \u0026ldquo;Generative Recommendation\u0026rdquo; (GR) paradigm for retrieval and ranking, reframing the discriminative \u003ccode\u003epAction\u003c/code\u003e prediction task as a generative task, akin to token predictions in language modeling.\u003c/p\u003e","tags":[],"title":"Is Generative Recommendation the ChatGPT Moment of RecSys?"},{"categories":["Gpu","Transformers","Ml systems"],"date":"2025-03-19T00:00:00Z","id":"/posts/hardware_aware_transformers/","keywords":[],"summary":"\u003ch2 id=\"attention-is-all-you-need-----if-you-can-afford-the-on2-complexity\" class=\"scroll-mt-8 group\"\u003e\n  Attention Is All You Need \u0026mdash; if You Can Afford the $O(N^2)$ Complexity\n  \n    \u003ca href=\"#attention-is-all-you-need-----if-you-can-afford-the-on2-complexity\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cp\u003eAttention is key to the success of large language models (LLMs). By attending to all (unmasked) tokens in the input sequence at once, attention-based Transformers overcome RNNs\u0026rsquo; difficulty in modeling long-range dependencies, avoiding vanishing and exploding gradients. However, with the power to \u0026ldquo;attend to all\u0026rdquo; comes hefty costs.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src=\"https://www.dropbox.com/scl/fi/m8vdwmpqwt40c896ty24v/Screenshot-2025-03-15-at-11.37.40-PM.png?rlkey=t6852oqzse600dc48gjg7rfal\u0026amp;st=r3h14cla\u0026amp;raw=1\"\n    alt=\"Writing materialized $\\mathbf{S}$, $\\mathbf{A}$, and $\\mathbf{O}$ to the GPU\u0026rsquo;s high-bandwidth memory (HBM) has an $O(N^2)$ IO complexity.\" width=\"600\"\u003e\u003cfigcaption\u003e\n      \u003cp\u003eWriting materialized $\\mathbf{S}$, $\\mathbf{A}$, and $\\mathbf{O}$ to the GPU\u0026rsquo;s high-bandwidth memory (HBM) has an $O(N^2)$ IO complexity.\u003c/p\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e","tags":[],"title":"Hardware-Aware Attention for Long Sequence Modeling"},{"categories":["Career advice","Personal reflections"],"date":"2024-12-25T00:00:00Z","id":"/posts/career_reflection/","keywords":[],"summary":"\u003ch2 id=\"foreword-to-the-ambitious-and-confused-early-career-engineers\" class=\"scroll-mt-8 group\"\u003e\n  Foreword: To the Ambitious (and Confused) Early Career Engineers\n  \n    \u003ca href=\"#foreword-to-the-ambitious-and-confused-early-career-engineers\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cp\u003eI had far too chill parents growing up. The first day they dropped me off at school, I had no idea why I was there. I sat quietly, confused, for about a year before realizing there was such a thing as education. Similarly, when I started my first job at DoorDash in Summer 2022 (or shall I say \u0026ldquo;Q3\u0026rdquo;), I had far too chill a manager during my first 6 months and only gradually figured out what a career in engineering is about.\u003c/p\u003e","tags":[],"title":"How I'd Start My Engineering Career All Over Again"},{"categories":["Recommender systems","Information retrieval"],"date":"2024-11-17T00:00:00Z","id":"/posts/seq_user_modeling/","keywords":[],"summary":"\u003ch2 id=\"catch-the-train-of-actions\" class=\"scroll-mt-8 group\"\u003e\n  Catch the Train of Actions\n  \n    \u003ca href=\"#catch-the-train-of-actions\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cp\u003eShown below is my Amazon browsing history last week. Any recommendations on what I might buy next?\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src=\"https://www.dropbox.com/scl/fi/t3fs6pgs8rxlysnpoqeq9/Screenshot-2024-11-11-at-4.25.33-PM.png?rlkey=kv37yorsm6qdlro4y4ot496s8\u0026amp;st=p200ep0j\u0026amp;raw=1\"\n    alt=\"Yuan\u0026rsquo;s Amazon browsing history last week; distinct sessions are color-coded.\" width=\"1800\"\u003e\u003cfigcaption\u003e\n      \u003cp\u003eYuan\u0026rsquo;s Amazon browsing history last week; distinct sessions are color-coded.\u003c/p\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e","tags":[],"title":"Down the Rabbit Hole: Sequential User Modeling"},{"categories":["Career","Machine learning","Interview"],"date":"2024-10-15T00:00:00Z","id":"/posts/mle_interviews/","keywords":[],"summary":"\u003ch2 id=\"the-marriage-analogy\" class=\"scroll-mt-8 group\"\u003e\n  The Marriage Analogy\n  \n    \u003ca href=\"#the-marriage-analogy\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cp\u003eOver 2 years ago, I wrote a \u003ca href=\"https://www.yuan-meng.com/posts/newgrads/\"\u003eblog post\u003c/a\u003e on how to find jobs as a new grad data scientist (as a twist of fate, I never worked as a product data scientist but instead became an ML engineer at DoorDash). Back in 2021, I cared a ton about interview skills, answer \u0026ldquo;frameworks\u0026rdquo;, and whatnot, which may still come handy at New Grad or Early Career levels. For experienced hires, however, I think of interviews as some sort of marriage proposal \u0026mdash; \u003cem\u003eit\u0026rsquo;s something you can rehearse but can never force\u003c/em\u003e.\u003c/p\u003e","tags":[],"title":"(Opinionated) Guide to ML Engineer Job Hunting"},{"categories":["AI","Cognitive science"],"date":"2024-10-13T00:00:00Z","id":"/posts/human_vision/","keywords":[],"summary":"\u003ch2 id=\"engineer-the-mind\" class=\"scroll-mt-8 group\"\u003e\n  Engineer the Mind\n  \n    \u003ca href=\"#engineer-the-mind\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cp\u003eIn Winter 2015, after coming back from grad school interviews in the States, I told my dad over hotpot that I was going to study \u003ca href=\"https://en.wikipedia.org/wiki/Cognitive_science\"\u003ecognitive science\u003c/a\u003e at Berkeley.\u003c/p\u003e","tags":[],"title":"Is Human Vision More like CNN or Vision Transformer?"},{"categories":["Negative sampling","Recommender system"],"date":"2024-09-02T00:00:00Z","id":"/posts/negative_sampling/","keywords":[],"summary":"\u003ch2 id=\"web-scale-recommender-systems\" class=\"scroll-mt-8 group\"\u003e\n  Web-Scale Recommender Systems\n  \n    \u003ca href=\"#web-scale-recommender-systems\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003ch3 id=\"two-stage-architecture\" class=\"scroll-mt-8 group\"\u003e\n  Two-Stage Architecture\n  \n    \u003ca href=\"#two-stage-architecture\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h3\u003e\n\u003cp\u003eThe iconic YouTube paper (\u003ca href=\"https://research.google/pubs/deep-neural-networks-for-youtube-recommendations/\"\u003eCovington et al., 2016\u003c/a\u003e) introduced a two-stage architecture that since became the industry standard for large-scale recommender systems:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRetrieval\u003c/strong\u003e (a.k.a. \u0026ldquo;candidate generation\u0026rdquo;): Quickly select top k (in the hundreds or thousands) loosely relevant items from a large corpus of billions\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRanking\u003c/strong\u003e (a.k.a. \u0026ldquo;reranking\u0026rdquo;): Order final candidates (dozens) by predicted reward probability (e.g., an ads click, a listing booking, a video watch, \u003cem\u003eetc.\u003c/em\u003e)\u003c/li\u003e\n\u003c/ul\u003e","tags":[],"title":"Negative Sampling for Learning Two-Tower Networks"},{"categories":["Information retrieval","Multi-task learning"],"date":"2024-07-05T00:00:00Z","id":"/posts/mtml/","keywords":[],"summary":"\u003cp\u003eNatural Language Processing (NLP) has an abundance of intuitively explained tutorials with code, such as Andrej Kaparthy\u0026rsquo;s \u003ca href=\"https://karpathy.ai/zero-to-hero.html\"\u003eNeural Networks: Zero to Hero\u003c/a\u003e, the viral \u003ca href=\"https://jalammar.github.io/illustrated-transformer/\"\u003eThe Illustrated Transformer\u003c/a\u003e and its successor \u003ca href=\"https://nlp.seas.harvard.edu/annotated-transformer/\"\u003eThe Annotated Transformer\u003c/a\u003e, Umar Jamil\u0026rsquo;s YouTube \u003ca href=\"https://www.youtube.com/@umarjamilai\"\u003eseries\u003c/a\u003e dissecting SOTA models and the companion \u003ca href=\"https://github.com/hkproj\"\u003erepo\u003c/a\u003e, among others.\u003c/p\u003e","tags":[],"title":"The Annotated Multi-Task Ranker: An MMoE Code Example"},{"categories":["Embedding","Information retrieval","Vector-based search"],"date":"2024-06-22T00:00:00Z","id":"/posts/ebr/","keywords":[],"summary":"\u003ch2 id=\"so-what-is-an-embedding\" class=\"scroll-mt-8 group\"\u003e\n  So, What is an Embedding?\n  \n    \u003ca href=\"#so-what-is-an-embedding\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Embedding\"\u003eEmbedding\u003c/a\u003e is a classic idea in mathematical topology and machine learning (click â–¶ for definitions). You can think of embeddings as a special type of vectors.\u003c/p\u003e","tags":[],"title":"An Introduction to Embedding-Based Retrieval"},{"categories":["Machine learning","Natural language processing"],"date":"2024-03-09T00:00:00Z","id":"/posts/attention_as_dict/","keywords":[],"summary":"\u003ch2 id=\"the-dictionary-metaphor-\" class=\"scroll-mt-8 group\"\u003e\n  The Dictionary Metaphor ðŸ”‘ðŸ“š\n  \n    \u003ca href=\"#the-dictionary-metaphor-\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e","tags":[],"title":"Attention as Soft Dictionary Lookup"},{"categories":["Search","Information retrieval","Learning to rank"],"date":"2024-02-17T00:00:00Z","id":"/posts/ltr/","keywords":[],"summary":"\u003ch2 id=\"first-thing-first\" class=\"scroll-mt-8 group\"\u003e\n  First Thing First\n  \n    \u003ca href=\"#first-thing-first\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eEnigmas of the universe \u003cbr/\u003e Cannot be known without a search \u003cbr/\u003e \u0026mdash; Epica, \u003ca href=\"https://open.spotify.com/track/34Oz0bzAq7E1aUnKksPfJJ?si=9eabd1446a6a4ccc\"\u003e\u003cem\u003eOmega\u003c/em\u003e\u003c/a\u003e (2021)\u003c/p\u003e\n\u003c/blockquote\u003e","tags":[],"title":"An Evolution of Learning to Rank"},{"categories":["Search","Information retrieval","Autocomplete"],"date":"2024-01-06T00:00:00Z","id":"/posts/autocomplete/","keywords":[],"summary":"\u003cp\u003eAutocompletion dates back half a century ago (\u003ca href=\"https://www.doc.ic.ac.uk/~shm/MI/mi3.html\"\u003eLonguet-Higgins \u0026amp; Ortony, 1968\u003c/a\u003e), initially designed to save keystrokes as people type and help those with physical disabilities type faster. The incomplete user input is the \u003cstrong\u003e\u0026ldquo;query prefix\u0026rdquo;\u003c/strong\u003e and suggested ways of extending the prefix into a full query are \u003cstrong\u003e\u0026ldquo;query completions\u0026rdquo;\u003c/strong\u003e. This feature is essential to modern text editors and search engines.\u003c/p\u003e","tags":[],"title":"Autocompletion for Search Enginees"},{"categories":["Machine learning"],"date":"2023-07-02T00:00:00Z","id":"/posts/perceptron/","keywords":[],"summary":"\u003cp\u003eTen years after the ImageNet Challenge thawed the last AI winter, ChaptGPT and generative AI have become part of our everyday life and colloquial language, like (almost) no one has imagined just 2 years back. As increasingly more folks aspire to foray into the field of ML/AI, I can\u0026rsquo;t help but think about a lesson from my guitar teacher:\u003c/p\u003e","tags":[],"title":"Teaching A Peceptron to See"},{"categories":["Machine learning","Algorithms"],"date":"2022-06-22T00:00:00Z","id":"/posts/md_coding/","keywords":[],"summary":"\u003cp\u003eCoding interviews can mean different things for \u0026ldquo;traditional\u0026rdquo; software engineers (back-end, front-end, full-stack, etc.) and engineers with a machine learning focus. Apart from LeetCode-style questions, ML engineers (as well as applied scientists, research engineers, and, occasionally, machine learning data scientists) may be asked to implement a classic ML algorithm from scratch during an interview.\u003c/p\u003e","tags":[],"title":"Code ML Algorithms From Scratch"},{"categories":["Career","Data science","Machine learning","Interview"],"date":"2022-03-13T00:00:00Z","id":"/posts/newgrads/","keywords":[],"summary":"\u003ch2 id=\"new-grad-timeline\" class=\"scroll-mt-8 group\"\u003e\n  New Grad Timeline\n  \n    \u003ca href=\"#new-grad-timeline\"\n        class=\"no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block\"\n        aria-hidden=\"true\" title=\"Link to this heading\" tabindex=\"-1\"\u003e\n        \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"16\"\n  height=\"16\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n  stroke-width=\"2\"\n  class=\"lucide lucide-link w-4 h-4 block\"\n  viewBox=\"0 0 24 24\"\n\u003e\n  \u003cpath d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\" /\u003e\n  \u003cpath d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\" /\u003e\n\u003c/svg\u003e\n\n    \u003c/a\u003e\n  \n\u003c/h2\u003e\n\u003cp\u003eSince last November, I regularly get questions about how to get data science jobs. I\u0026rsquo;ve hesitated to give advice because no advice applies to everyone. As more people ask, however, I want to write a post for those in my shoes:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e(PhD) students applying to machine learning or product data scientist roles at tech companies through university recruiting.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e","tags":[],"title":"(Quirky) Roadmap for New Grad Data Scientists"},{"categories":["Causal inference","Quasi-experiment"],"date":"2021-11-12T00:00:00Z","id":"/posts/causality/causality/","keywords":[],"summary":"\u003cp\u003eWhat do you see? ðŸ‘€\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src=\"https://www.dropbox.com/s/gfiw187r502jb1l/1t1_redgreen_2sec.gif?raw=1\" width=\"350\"\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eYou\u0026rsquo;re probably thinking, \u003cem\u003e\u0026ldquo;The red block stopped, right after which the green block started to move coincidentally.\u0026rdquo;\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eNice try\u0026hellip; I know what you\u0026rsquo;re thinking, \u003cem\u003e\u0026ldquo;The red block \u003cu\u003emade\u003c/u\u003e the green one move\u0026rdquo;\u003c/em\u003e.\u003c/p\u003e","tags":[],"title":"Causal Inference in Data Science"},{"categories":["Metrics","Product"],"date":"2021-11-06T00:00:00Z","id":"/posts/metrics/","keywords":[],"summary":"\u003cp\u003e\u0026lsquo;Tis the college and job application season of the year. If only schools and companies have crystal balls to see into each candidate\u0026rsquo;s future achievements, they need not interview people; since they do not have such things, SAT scores, GPA, internships, and other quantifiable metrics are used to aid decisions. Simplified metrics are by no means ideal â€” As Goodhart put it (often \u003ca href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\"\u003eparaphrased\u003c/a\u003e), \u0026ldquo;When a measure becomes a metric, it ceases to be a good measure.\u0026rdquo; However, the opposite is worse: Making critical decisions based on heuristics, biases, and personal opinions.\u003c/p\u003e","tags":[],"title":"Choosing Metrics"}]
