<!doctype html>
<html
  lang="en-us"
  dir="ltr"
>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<link rel="stylesheet" href="http://localhost:1313/css/styles.min.29149e7eece4eab92c5f2dc32ab7ccaad6427a19dd21db0153b88b4ccb8f3645.css">
<meta charset="utf-8" />
<meta name="language" content="en" />
<meta name="viewport" content="width=device-width" />
<title>
    Hardware-Aware Attention for Long Sequence Modeling | Yuan Meng
</title>
  <meta name="description" content=" Attention Is All You Need — if You Can Afford the $O(N^2)$ Complexity Attention is key to the success of large language models (LLMs). By attending to all (unmasked) tokens in the input sequence at once, attention-based Transformers overcome RNNs’ difficulty in modeling long-range dependencies, avoiding vanishing and exploding gradients. However, with the power to “attend to all” comes hefty costs.
In vanilla attention, writing $\mathbf{S}$, $\mathbf{A}$, and $\mathbf{O}$ to memory has an $O(N^2)$ IO complexity." />
<meta property="og:url" content="http://localhost:1313/posts/hardware_aware_transformers/">
  <meta property="og:site_name" content="Yuan Meng">
  <meta property="og:title" content="Hardware-Aware Attention for Long Sequence Modeling">
  <meta property="og:description" content="Attention Is All You Need — if You Can Afford the $O(N^2)$ Complexity Attention is key to the success of large language models (LLMs). By attending to all (unmasked) tokens in the input sequence at once, attention-based Transformers overcome RNNs’ difficulty in modeling long-range dependencies, avoiding vanishing and exploding gradients. However, with the power to “attend to all” comes hefty costs.
In vanilla attention, writing $\mathbf{S}$, $\mathbf{A}$, and $\mathbf{O}$ to memory has an $O(N^2)$ IO complexity.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-03-18T00:00:00+00:00">


  <meta itemprop="name" content="Hardware-Aware Attention for Long Sequence Modeling">
  <meta itemprop="description" content="Attention Is All You Need — if You Can Afford the $O(N^2)$ Complexity Attention is key to the success of large language models (LLMs). By attending to all (unmasked) tokens in the input sequence at once, attention-based Transformers overcome RNNs’ difficulty in modeling long-range dependencies, avoiding vanishing and exploding gradients. However, with the power to “attend to all” comes hefty costs.
In vanilla attention, writing $\mathbf{S}$, $\mathbf{A}$, and $\mathbf{O}$ to memory has an $O(N^2)$ IO complexity.">
  <meta itemprop="datePublished" content="2025-03-18T00:00:00+00:00">
  <meta itemprop="wordCount" content="3426">
  <meta itemprop="keywords" content="Gpu,Transformers,Ml systems">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Hardware-Aware Attention for Long Sequence Modeling">
  <meta name="twitter:description" content="Attention Is All You Need — if You Can Afford the $O(N^2)$ Complexity Attention is key to the success of large language models (LLMs). By attending to all (unmasked) tokens in the input sequence at once, attention-based Transformers overcome RNNs’ difficulty in modeling long-range dependencies, avoiding vanishing and exploding gradients. However, with the power to “attend to all” comes hefty costs.
In vanilla attention, writing $\mathbf{S}$, $\mathbf{A}$, and $\mathbf{O}$ to memory has an $O(N^2)$ IO complexity.">

<link rel="canonical" href="http://localhost:1313/posts/hardware_aware_transformers/" />

    <link rel="stylesheet" href="/css/index.css" />


      <script src="/js/main.js" defer></script>
  

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org/",
  "@id": "http://localhost:1313/posts/hardware_aware_transformers/",
  "@type": "BlogPosting",
  "articleSection": [
    "Gpu",
    "Transformers",
    "Ml systems"
  ],
  "author": {
    "@type": "Person",
    "email": "mycaptainmy@gmail.com",
    "name": "Yuan Meng",
    "url": "http://localhost:1313/about/"
  },
  "copyrightNotice": "Yuan Meng",
  "datePublished": "2025-03-18",
  "description": " Attention Is All You Need — if You Can Afford the $O(N^2)$ Complexity Attention is key to the success of large language models (LLMs). By attending to all (unmasked) tokens in the input sequence at once, attention-based Transformers overcome RNNs’ difficulty in modeling long-range dependencies, avoiding vanishing and exploding gradients. However, with the power to “attend to all” comes hefty costs.\nIn vanilla attention, writing $\\mathbf{S}$, $\\mathbf{A}$, and $\\mathbf{O}$ to memory has an $O(N^2)$ IO complexity.",
  "headline": "Hardware-Aware Attention for Long Sequence Modeling",
  "isPartOf": {
    "@id": "http://localhost:1313/posts/",
    "@type": "Blog",
    "name": "Posts"
  },
  "mainEntityOfPage": "http://localhost:1313/posts/hardware_aware_transformers/",
  "name": "Hardware-Aware Attention for Long Sequence Modeling",
  "timeRequired": "PT17M",
  "url": "http://localhost:1313/posts/hardware_aware_transformers/",
  "wordCount": 3426
}
</script>


  </head>
  <body>
    <div class="container mx-auto flex max-w-prose flex-col space-y-10 p-4 md:p-6">
      <header class="flex flex-row items-center justify-between">
        <div>
  <a id="skip-nav" class="sr-only" href="#maincontent">Skip to main content</a>
  <a class="font-semibold" href="/">Yuan Meng</a>
</div>

  <nav>
    <ul class="flex flex-row items-center justify-end space-x-4">
    <li>
      <a href="/about/">About</a
      >
    </li>
    <li>
      <a aria-current="true" class="ancestor" href="/posts/">Posts</a
      >
    </li>
    <li>
      <a href="/notes/">Notes</a
      >
    </li>
    </ul>
  </nav>


      </header>
      <main class="prose prose-slate relative md:prose-lg prose-h1:text-[2em]" id="maincontent">
        <article class="main">
    <header>
      <h1 class="!mb-1">Hardware-Aware Attention for Long Sequence Modeling</h1><div class="flex flex-row items-center space-x-4">
          <time class="text-sm italic opacity-80" datetime="2025-03-18T00:00:00&#43;00:00">March 18, 2025</time>
        </div>
    </header>

    
    
      Reading time: 17 minutes
    

    
    
      <div class="toc-container">
        <span id="toc-toggle">
          <span id="toc-icon">▶</span> 
          <span>Table of Contents</span>
        </span>
        <nav id="TableOfContents" class="toc-content">
          <nav id="TableOfContents">
  <ul>
    <li><a href="#attention-is-all-you-need-----if-you-can-afford-the-on2-complexity">Attention Is All You Need &mdash; if You Can Afford the $O(N^2)$ Complexity</a></li>
    <li><a href="#know-your-gpus">Know Your GPUs</a>
      <ul>
        <li><a href="#game-cards-born-for-deep-learning">Game Cards Born for Deep Learning</a></li>
        <li><a href="#gpu-memory-hierarchy">GPU Memory Hierarchy</a></li>
        <li><a href="#are-you-bound-by-memory-compute-or-overhead">Are You Bound by Memory, Compute, or Overhead?</a></li>
      </ul>
    </li>
    <li><a href="#flashattention-minimize-memory-readwrites">FlashAttention: Minimize Memory Read/Writes</a>
      <ul>
        <li><a href="#flashattention-1-tiling--recomputation">FlashAttention-1: Tiling + Recomputation</a></li>
        <li><a href="#flashattention-2-fewer-non-matmul-flops--better-parallelism">FlashAttention-2: Fewer Non-<code>matmul</code> FLOPs + Better Parallelism</a></li>
        <li><a href="#flashattention-3-asynchrony--low-precision">FlashAttention-3: Asynchrony + Low-Precision</a></li>
      </ul>
    </li>
    <li><a href="#reduce-compute-anyways-attention-approximation">Reduce Compute Anyways: Attention Approximation</a>
      <ul>
        <li><a href="#sparse-approximation">Sparse Approximation</a></li>
        <li><a href="#low-rank-approximation">Low-Rank Approximation</a></li>
        <li><a href="#try-em-all-deepseekmoe">Try &lsquo;Em All: DeepSeekMoE</a></li>
      </ul>
    </li>
    <li><a href="#applications-in-user-sequence-modeling">Applications in User Sequence Modeling</a></li>
    <li><a href="#references">References</a>
      <ul>
        <li><a href="#understand-deep-learning-systems">Understand Deep Learning Systems</a></li>
        <li><a href="#flashattention-io-aware-exact-attention">FlashAttention: IO-Aware, Exact Attention</a></li>
        <li><a href="#fast--accurate-attention-approximations">Fast &amp; Accurate Attention Approximations</a></li>
      </ul>
    </li>
  </ul>
</nav>
        </nav>
      </div>

      <script>
        
        document.addEventListener('DOMContentLoaded', function () {
          var tocToggle = document.getElementById('toc-toggle');
          var tocContent = document.getElementById('TableOfContents');
          var tocIcon = document.getElementById('toc-icon');
          tocToggle.addEventListener('click', function () {
            if (tocContent.style.display === 'none' || tocContent.style.display === '') {
              tocContent.style.display = 'block';
              tocIcon.textContent = '▼'; 
            } else {
              tocContent.style.display = 'none';
              tocIcon.textContent = '▶'; 
            }
          });
        });
      </script>
    

    
    <div class="content">
      <h2 id="attention-is-all-you-need-----if-you-can-afford-the-on2-complexity" class="scroll-mt-8 group">
  Attention Is All You Need &mdash; if You Can Afford the $O(N^2)$ Complexity
  
    <a href="#attention-is-all-you-need-----if-you-can-afford-the-on2-complexity"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p>Attention is key to the success of large language models (LLMs). By attending to all (unmasked) tokens in the input sequence at once, attention-based Transformers overcome RNNs&rsquo; difficulty in modeling long-range dependencies, avoiding vanishing and exploding gradients. However, with the power to &ldquo;attend to all&rdquo; comes hefty costs.</p>
<figure><img src="https://www.dropbox.com/scl/fi/m8vdwmpqwt40c896ty24v/Screenshot-2025-03-15-at-11.37.40-PM.png?rlkey=t6852oqzse600dc48gjg7rfal&amp;st=r3h14cla&amp;raw=1"
    alt="In vanilla attention, writing $\mathbf{S}$, $\mathbf{A}$, and $\mathbf{O}$ to memory has an $O(N^2)$ IO complexity." width="600"><figcaption>
      <p>In vanilla attention, writing $\mathbf{S}$, $\mathbf{A}$, and $\mathbf{O}$ to memory has an $O(N^2)$ IO complexity.</p>
    </figcaption>
</figure>

<p>Matrix multiplications and scaling take <span class="sidenote">
  <input
    aria-label="Show sidenote"
    type="checkbox"
    id="sidenote-checkbox-02"
    class="sidenote-checkbox hidden"
  />
  <label
    tabindex="0"
    role="mark"
    aria-details="sidenote-02"
    for="sidenote-checkbox-02"
    class="sidenote-mark"
    >$O(N^2d)$</label
  >
  <small id="sidenote-02" class="sidenote-content">
    <span class="sr-only"> (sidenote: </span>Here's the trick for estimating matrix multiplication complexity: the outer dimensions tell us how many inner products are performed, which is $N \times N$ in self-attention. The inner dimension tells us the complexity of each inner product, which is $2d - 1$ = $d$ (element-wise products) + $(d-1)$ (additions) in this case. Row-wise softmax takes $O(N^2)$ time, which is dominated by the $O(N^2d)$ matrix multiplication time.<span class="sr-only">)</span>
  </small>
</span>
 time, where $N$ is the sequence length and $d$ is the embedding dimension. To make matters worse, the fast static random-access memory (<a href="https://en.wikipedia.org/wiki/Static_random-access_memory">SRAM</a>) near the GPU compute has no room to store the resulting $N \times N$ matrices &mdash; ferrying them to the slow high-bandwidth memory (<a href="https://en.wikipedia.org/wiki/High_Bandwidth_Memory">HBM</a>) takes $O(N^2)$ time. As such, vanilla attention doesn&rsquo;t scale well with $N$ 💀.</p>
<p>Many flavors of &ldquo;efficient attention&rdquo; (see Lilian Weng&rsquo;s <a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#efficient-attention">blog</a> for a nice summary) aim to reduce the $O(N^2d)$ compute cost, typically via sparse (e.g., <a href="https://arxiv.org/abs/1904.10509"><em>Sparse Transformers</em></a>, <a href="https://arxiv.org/abs/2001.04451"><em>Reformer</em></a>, <a href="https://arxiv.org/abs/2003.05997"><em>Routing Transformer</em></a>) or low-rank (e.g., <a href="https://arxiv.org/abs/2006.04768"><em>Linformer</em></a>, <a href="https://arxiv.org/abs/2006.16236"><em>Linear Transformer</em></a>, <a href="https://openreview.net/forum?id=Ua6zuk0WRH"><em>Performer</em></a>, <a href="https://arxiv.org/abs/2406.02542"><em>Loki</em></a>) approximations. Strangely, they don&rsquo;t always reduce the wall time.</p>
<p>Before diving into any details, the lesson is this: <span style="background-color: #abe0bb">to optimize any system, first identify its bottleneck; otherwise, you&rsquo;re wasting your time</span>. Horace He&rsquo;s <a href="https://horace.io/brrr_intro.html">insight</a> inspired <a href="https://arxiv.org/abs/2205.14135">FlashAttention</a> by Tri Dao&rsquo;s team: with a large enough $N$, attention is actually <em>not</em> bottlenecked by compute but by data movements between SRAM and HBM. So fitting intermediate results on SRAM if possible can give us a true speedup.</p>
<p>I&rsquo;ve been increasingly fascinated by this type of software-hardware co-design for attention (or deep learning in general) and will review some classic works in this post. First, let&rsquo;s take a look at the &ldquo;metal&rdquo;, GPUs.</p>
<h2 id="know-your-gpus" class="scroll-mt-8 group">
  Know Your GPUs
  
    <a href="#know-your-gpus"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<blockquote>
<p>GPUs are throughput processors that rely on concurrency and asynchrony to hide memory and execution latencies. &mdash; <a href="https://arxiv.org/abs/2407.08608">Shah et al., (2023)</a></p>
</blockquote>
<h3 id="game-cards-born-for-deep-learning" class="scroll-mt-8 group">
  Game Cards Born for Deep Learning
  
    <a href="#game-cards-born-for-deep-learning"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Inside your Nintendo Switch are graphics processing units (GPUs) that render life-like scenes in real time. The <a href="https://en.wikipedia.org/wiki/Game_engine">game engine</a> determines how characters act and what objects look like based on rules and physics. Then, GPUs break down the scene to draw into small units (e.g., triangles and vertices), rendering pixels, lighting, textures, and other graph elements in each unit simultaneously. Without such parallelism, the game world unfolds slowly, making your game too stuck to play.</p>
<figure><img src="https://www.dropbox.com/scl/fi/oirp77bh00gkux8gtic5v/Screenshot-2025-03-15-at-2.58.36-PM.png?rlkey=qstiht7lzgzbvpm4wgqzhkt18&amp;st=g3s1ufu5&amp;raw=1"
    alt="Game graphics are rendered seamlessly by graphics processing units (GPUs) thanks to their massive parallel processing power." width="500"><figcaption>
      <p>Game graphics are rendered seamlessly by graphics processing units (GPUs) thanks to their massive parallel processing power.</p>
    </figcaption>
</figure>

<p>Turns out, matrix multiplications &mdash; the heart of deep learning &mdash; can similarly be broken down into small units and processed in parallel. For instance, $\mathbf{A} \in \mathbb{R}^{n \times k} \times \mathbf{B} \in \mathbb{R}^{k \times m}$ can decomposed into $n \times m$ inner products between two $k$-vectors (the $i$-th row vector in $\mathbf{A}$ and the $j$-th column vector in $\mathbf{B}$) to be executed at once. Each inner product can be further decomposed into $k$ parallel element-wise multiplications, followed by $(k-1)$ additions to sum up $k$ products.</p>
<figure><img src="https://www.dropbox.com/scl/fi/vvqvrqfw0eyotbzl2j1y7/Screenshot-2025-03-15-at-3.49.46-PM.png?rlkey=b6vgw73tnjb8rs937j7istoaz&amp;st=269p6gyk&amp;raw=1"
    alt="Multiplication between two matrices can be executed as parallel inner products. Each inner product can be executed as parallel element-wise multiplications, followed by an addition." width="600"><figcaption>
      <p>Multiplication between two matrices can be executed as parallel inner products. Each inner product can be executed as parallel element-wise multiplications, followed by an addition.</p>
    </figcaption>
</figure>

<p>In <a href="https://blogs.nvidia.com/blog/why-gpus-are-great-for-ai/">2009</a>, Geoff Hinton was the first to preach the power of GPUs in the machine learning community. <a href="https://www.newyorker.com/magazine/2023/12/04/how-jensen-huangs-nvidia-is-powering-the-ai-revolution">Three years later</a>, Hinton and his students trained AlexNet, a CNN variant that sparked the deep learning revolution, on two GeForce GPUs bought from Amazon.</p>
<h3 id="gpu-memory-hierarchy" class="scroll-mt-8 group">
  GPU Memory Hierarchy
  
    <a href="#gpu-memory-hierarchy"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>NVIDIA GPUs have different types of compute units: Tensor Cores are extremely fast for general matrix multiplications (GEMMs) and CUDA Cores are designed for more flexible, general-purpose computing.</p>
<figure><img src="https://www.dropbox.com/scl/fi/5g3edyxzwq1q83gj6cj46/Screenshot-2025-03-15-at-4.26.31-PM.png?rlkey=8h6wet9gim1ofjksu8ibbitk8&amp;st=5k6f3fq2&amp;raw=1"
    alt="The GPU memory hierarchy (source: Tri Dao&rsquo;s slides)." width="800"><figcaption>
      <p>The GPU memory hierarchy (source: Tri Dao&rsquo;s <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/slides/cs224n-2024-lecture18-deployment-and-efficiency.pdf">slides</a>).</p>
    </figcaption>
</figure>

<p>Near the compute units is static random-access memory (SRAM), which is fast but limited in capacity &mdash; it&rsquo;s like the cache in many computer systems. The off-chip high-bandwidth memory (HBM) has greater capacity but lower data transfer speeds. Horace He has a wonderful <a href="https://horace.io/brrr_intro.html">analogy</a> that HBM is like a warehouse where raw materials and finished products are stored, while SRAM is like the storage right outside the factory where new products are being produced.</p>
<p>As the GPU carries out an operation (called a &ldquo;kernel&rdquo;), it read inputs from HBM into SRAM and writes outputs from SRAM back to HBM. There are 3 types of costs associated with any GPU systems:</p>
<ul>
<li><strong>Bandwidth costs</strong>: The time spent ferrying data between HBM and SRAM. Also known as &ldquo;memory bandwidth&rdquo; or IO cost.</li>
<li><strong>Compute costs</strong>: The time actually spent on computing (GEMMs or otherwise), often measured by FLOPs (floating points per second). This is what people think/hope they pay NVIDIA for.</li>
<li><strong>Overhead costs</strong>: All else &mdash; e.g., deciding to which factory to send what materials for what products, or spinning up an idle factory.</li>
</ul>
<p>Understanding the bottleneck of your system is key to efficiency improvements. For instance, if you can&rsquo;t ferry materials fast enough into the factories, then buying more expensive factory machines won&rsquo;t help increase your output. The million-dollar question is: how do you know if you&rsquo;re bound by memory, compute, or overhead?</p>
<h3 id="are-you-bound-by-memory-compute-or-overhead" class="scroll-mt-8 group">
  Are You Bound by Memory, Compute, or Overhead?
  
    <a href="#are-you-bound-by-memory-compute-or-overhead"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<figure><img src="https://www.dropbox.com/scl/fi/63qnev0qkwnmynp99kj31/Screenshot-2025-03-15-at-8.09.27-PM.png?rlkey=92ofkzpw8qxrhb6xwapsedkds&amp;st=fqr7ekik&amp;raw=1"
    alt="Diagnose the bottleneck of your system by gradually intensifying compute and measuring wall time (source: Horace He&rsquo;s blog)." width="800"><figcaption>
      <p>Diagnose the bottleneck of your system by gradually intensifying compute and measuring wall time (source: Horace He&rsquo;s <a href="https://horace.io/brrr_intro.html">blog</a>).</p>
    </figcaption>
</figure>

<p>Horace He proposed an elegant method for distinguishing memory-bound vs. compute-bound workloads &mdash; all else being equal, if you increase the <span class="sidenote">
  <input
    aria-label="Show sidenote"
    type="checkbox"
    id="sidenote-checkbox-07"
    class="sidenote-checkbox hidden"
  />
  <label
    tabindex="0"
    role="mark"
    aria-details="sidenote-07"
    for="sidenote-checkbox-07"
    class="sidenote-mark"
    >compute intensity</label
  >
  <small id="sidenote-07" class="sidenote-content">
    <span class="sr-only"> (sidenote: </span>It's the ratio of the number of calculations to the amount of data moved, typically measured in FLOPs per byte.<span class="sr-only">)</span>
  </small>
</span>
 (e.g., repeat a toy operation $n$ times for testing, or increasing the inner dimension of matrices in actual use cases) but the runtime doesn&rsquo;t increase, you&rsquo;re likely memory-bound: some of your compute sits idle, ready to process any incoming data ASAP. However, if runtime starts to increase with compute intensity, you&rsquo;re compute-bound since all your compute is likely occupied.</p>
<p>To identify overhead costs, you can use the PyTorch profiler to check for large gaps between CPU kernels (e.g., sending &ldquo;instructions&rdquo;) and GPU kernels (e.g., ferrying data between HBM and SRAM, computing).</p>
<h2 id="flashattention-minimize-memory-readwrites" class="scroll-mt-8 group">
  FlashAttention: Minimize Memory Read/Writes
  
    <a href="#flashattention-minimize-memory-readwrites"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p>If there&rsquo;s no room in the factory to store intermediate outputs and moving them to a distant warehouse takes a long time, we can assemble one small part at a time and combine them into the final product. So long as each small part stays in the factory, we save time on transportation. This is the key intuition behind FlashAttention.</p>
<p>First, let&rsquo;s review how vanilla attention is computed (see my <a href="/posts/attention_as_dict/" class="backlink">post</a>
  
   for an intuitive explanation). To begin, we look up token embeddings and add them with positional encodings. Then we project the $N \times d$ input matrix into 3 matrices, $\mathbf{Q}$ (queries), $\mathbf{K}$ (keys), and $\mathbf{V}$ (values).</p>
<figure><img src="https://www.dropbox.com/scl/fi/e8frszo46hrpklzlhh363/Screenshot-2025-03-16-at-12.32.41-PM.png?rlkey=6r8hdpupn6m5va0u8wbtknnay&amp;st=lvcue89u&amp;raw=1"
    alt="Computing attention requires us to materialize large $N \times N$ matrices in HBM, which is the bottleneck for long sequence modeling (source: Tri Dao&rsquo;s talk)." width="800"><figcaption>
      <p>Computing attention requires us to materialize large $N \times N$ matrices in HBM, which is the bottleneck for long sequence modeling (source: Tri Dao&rsquo;s <a href="https://horace.io/brrr_intro.html">talk</a>).</p>
    </figcaption>
</figure>

<p>We carry out a series of matrix operations in order to eventually obtain the &ldquo;contextual embedding&rdquo; of each input token:</p>
<ol>
<li>Compute raw attention scores, <strong>$\mathbf{S} = \mathbf{Q}\mathbf{K}^{\top} \in \mathbb{R}^{N \times N}$</strong>;</li>
<li>Apply row-wise softmax on $\mathbf{S}$ so that each row sums to 1. In practice, we can keep 2 separate components:
<ul>
<li>Exponentiate each element in $\mathbf{S}$, $\mathbf{A} = \exp(\mathbf{S}) \in \mathbb{R}^{N \times N}$;
<ul>
<li>For better numerical stability, we could also subtract the row maximum $\bm{m}$ from each row element before exponentiating it, $\mathbf{A} = \exp(\mathbf{S} - \bm{m}) \in \mathbb{R}^{N \times N}$</li>
</ul>
</li>
<li>Track the sum of each row $i$, $\bm{l} = \sum_i \exp(\mathbf{S})_i$ or $\bm{l} = \sum_i \exp(\mathbf{S} - \bm{m})_i$, which will be the softmax denominators;</li>
</ul>
</li>
<li>Compute the output matrix, $\mathbf{O}=\frac{\mathbf{A}}{\bm{l}}\mathbf{V} \in \mathbb{R}^{N \times d}$, which represents the &ldquo;contextual embedding&rdquo; of each input token.</li>
</ol>
<p>Fitting $\mathbf{S}$ and $\mathbf{A}$ on SRAM is not possible for long sequences, so they are moved to HBM with an $O(N^2)$ IO complexity. <em>If we can split the inputs, may the intermediate results will fit?</em> People thought about it but hesitated. While inputs are naturally split along the $\mathbf{Q}$ dimension, it seems wrong to further split them along $\mathbf{K}$ and $\mathbf{V}$ since computing the softmax requires summing over each row in the <em>full</em> $\mathbf{A}$ matrix.</p>
<h3 id="flashattention-1-tiling--recomputation" class="scroll-mt-8 group">
  FlashAttention-1: Tiling + Recomputation
  
    <a href="#flashattention-1-tiling--recomputation"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>With simple rescaling, we can obtain correct softmax results even when splitting inputs along $\mathbf{K}$ and $\mathbf{V}$. &ldquo;Softmax + rescaling&rdquo; is the key innovation behind <em>tiling</em> in FlashAttention (<a href="https://arxiv.org/abs/2205.14135">Dao et al., 2022</a>).</p>
<p>Suppose we split $\mathbf{K}$ and $\mathbf{V}$ into two blocks. We can compute $\mathbf{S}^{(1)}$ and $\mathbf{S}^{(2)}$ without issue since matrix multiplications in one block don&rsquo;t interfere with those in another. Similarly, we can obtain $\mathbf{A}^{(1)}$ and $\mathbf{A}^{(2)}$ with issue since element-wise exponentiation is independent.</p>
<figure><img src="https://www.dropbox.com/scl/fi/eprfs3xgnvr8s6n8p8elm/Screenshot-2025-03-16-at-1.15.52-PM.png?rlkey=6oqxe4wjlta3uxecejei4bqla&amp;st=hgq348vi&amp;raw=1"
    alt="By splitting $\mathbf{K}$ and $\mathbf{V}$ into blocks and computing outputs block by block, we keep computations within the fast SRAM and never materialize large matrices, thereby breaking through the IO bottleneck (source: Tri Dao&rsquo;s talk)." width="1000"><figcaption>
      <p>By splitting $\mathbf{K}$ and $\mathbf{V}$ into blocks and computing outputs block by block, we keep computations within the fast SRAM and never materialize large matrices, thereby breaking through the IO bottleneck (source: Tri Dao&rsquo;s <a href="https://horace.io/brrr_intro.html">talk</a>).</p>
    </figcaption>
</figure>

<p>However, if we scale $\mathbf{A}^{(1)}$ by $\bm{l}^{(1)} = \sum_i \exp(\mathbf{S}^{(1)})_i$ to obtain the output matrix $\mathbf{O}^{(1)}$, we get wrong results. This is because $\bm{l}^{(1)}$ sums each row within block 1, but we need to sum the entire row in the original $\mathbf{A}$. The good news is that once we process block 2, the final block here, we regain knowledge of the full row sums, $\bm{l}^{(2)} = \bm{l}^{(1)} + \sum_i \exp(\mathbf{S}^{(2)})_i$. This allows us to rescale $\mathbf{O}^{(1)}$ by $\frac{\bm{l}^{(1)}}{\bm{l}^{(2)}}$ to get the right answers. Note that the computation of each block happens <em>sequentially</em>, but since each fits within SRAM, it still provides a significant speedup compared to transferring large matrices to HBM.</p>
<p>Each input token&rsquo;s $\bm{q} \in \mathbf{Q}$, $\bm{k} \in \mathbf{K}$, $\bm{v} \in \mathbf{V}$ projections are learnable &mdash; by updating them through backpropagation, we train the model to make better predictions. To calculate gradients w.r.t. $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ in the backward pass, we normally store intermediate matrices such as $\mathbf{S}$ and $\mathbf{A}$ to avoid recomputation. However, by keeping <span class="sidenote">
  <input
    aria-label="Show sidenote"
    type="checkbox"
    id="sidenote-checkbox-11"
    class="sidenote-checkbox hidden"
  />
  <label
    tabindex="0"
    role="mark"
    aria-details="sidenote-11"
    for="sidenote-checkbox-11"
    class="sidenote-mark"
    >only</label
  >
  <small id="sidenote-11" class="sidenote-content">
    <span class="sr-only"> (sidenote: </span>If the input sequence is masked, we also need to store the pseudo-random number generator states so we can generate correct `MASK` tokens on the fly.<span class="sr-only">)</span>
  </small>
</span>
 $\mathbf{O}$ (output) and $\bm{l}$ (softmax denominators), $\mathbf{S}$ and $\mathbf{A}$ can be recomputed in SRAM.</p>
<figure><img src="https://www.dropbox.com/scl/fi/ap9fxmtuw3xt2w27c2uvn/Screenshot-2025-03-16-at-2.34.46-PM.png?rlkey=t1f82pgzksr0hrhga5jfumnz0&amp;st=wkfpees1&amp;raw=1"
    alt="By recomputing $\mathbf{S}$ and $\mathbf{A}$ in SRAM, we trade off extra work for memory saving (source: Tri Dao&rsquo;s talk)." width="500"><figcaption>
      <p>By recomputing $\mathbf{S}$ and $\mathbf{A}$ in SRAM, we trade off extra work for memory saving (source: Tri Dao&rsquo;s <a href="https://horace.io/brrr_intro.html">talk</a>).</p>
    </figcaption>
</figure>

<p>By doing a bit more computation, we can decrease the memory cost from $O(N^2)$ to $O(N)$, increasing the overall training throughput.</p>
<h3 id="flashattention-2-fewer-non-matmul-flops--better-parallelism" class="scroll-mt-8 group">
  FlashAttention-2: Fewer Non-<code>matmul</code> FLOPs + Better Parallelism
  
    <a href="#flashattention-2-fewer-non-matmul-flops--better-parallelism"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>As FlashAttention reduces the IO bottleneck, compute becomes a bigger concern &mdash; matrix multiplications are  fast because they are executed on Tensor Cores, which are specialized for <code>matmul</code>. Non-<code>matmul</code> operations such as element-wise exponentiation or scaling are slow because they are executed on non-specialized CUDA Cores. Since the industry-wide adoption of FlashAttention, researchers at companies like OpenAI, NVIDIA, and Meta have been exploring better ways to parallelize attention block computations. FlashAttention 2.0 (<a href="https://arxiv.org/abs/2307.08691">Dao, 2023</a>) was developed to (1) reduce non-<code>matmul</code> FLOPs, (2) further parallelize over the sequence length dimension, and (3) better partition work between <span class="sidenote">
  <input
    aria-label="Show sidenote"
    type="checkbox"
    id="sidenote-checkbox-13"
    class="sidenote-checkbox hidden"
  />
  <label
    tabindex="0"
    role="mark"
    aria-details="sidenote-13"
    for="sidenote-checkbox-13"
    class="sidenote-mark"
    >warps</label
  >
  <small id="sidenote-13" class="sidenote-content">
    <span class="sr-only"> (sidenote: </span>Execution units on the GPU are called "threads". A group of 32 threads are a "warp". 4 contiguous warps form a warpgroup. A thread block typically contains one or two warpgroups. Hopper H100 GPUs have thread block clusters.<span class="sr-only">)</span>
  </small>
</span>
, achieving a 2–4x speedup and a 10–20x memory reduction compared with FlashAttention 1.0.</p>
<p>To reduce non-<code>matmul</code> FLOPs, FlashAttention 2.0 avoids rescaling each previous block&rsquo;s output. Instead, it carries an updated $\bm{l}$ and rescales only the final output $\mathbf{O}^{(last)}$ for the right results. As mentioned, for more stable softmax, FlashAttention 1.0 not just stores $\bm{l}^{(j)}$ (row sums of exponentials of the $j$-th block) but also $\bm{m}^{(j)}$ (row maximums of the the $j$-th block) &mdash; row maximums are subtracted from row elements before the softmax. FlashAttention 2.0 only stores the log sum of exponentials, $L^{(j)} = \bm{m}^{(j)} + \log\bm{l}^{(j)}$.</p>
<figure><img src="https://www.dropbox.com/scl/fi/z1203la1pkz7ckha0crn9/Screenshot-2025-03-16-at-5.07.07-PM.png?rlkey=66a3mw07p9225f3hae6nevo6j&amp;st=t5jcnbux&amp;raw=1"
    alt="The sequence dimension is divided into row (forward) and column (backward) blocks, with one thread block dedicated to each block (source: Dao, 2023)." width="600"><figcaption>
      <p>The sequence dimension is divided into row (forward) and column (backward) blocks, with one thread block dedicated to each block (source: <a href="https://arxiv.org/abs/2307.08691">Dao, 2023</a>).</p>
    </figcaption>
</figure>

<p>FlashAttention 1.0 parallelize along the batch and the head dimensions, but not the sequence dimension &mdash; each thread block is responsible for an entire head of an entire sequence. FlashAttention 2.0 further divides the sequence dimension into row/column blocks and assigns one thread block to each row/column block, allowing different &ldquo;chunks&rdquo; of a long sequence to be processed in parallel.</p>
<figure><img src="https://www.dropbox.com/scl/fi/lpp95ckjok0mlbh9i7wtt/Screenshot-2025-03-16-at-5.41.17-PM.png?rlkey=aq4pqccri0zs1cf5si1iv7mx4&amp;st=qwfg21h5&amp;raw=1"
    alt="Splitting by $\mathbf{Q}$ reduces reads/writes to shared memory (source: Dao, 2023)." width="600"><figcaption>
      <p>Splitting by $\mathbf{Q}$ reduces reads/writes to shared memory (source: <a href="https://arxiv.org/abs/2307.08691">Dao, 2023</a>).</p>
    </figcaption>
</figure>

<p>FlashAttention 1.0 was motivated by the fact that data transfer is slow between SRAM and HBM. Even inside the compute, communication speed differs within vs. between warps, with the former being much faster. FlashAttention 1.0 splits $\mathbf{K}$ and $\mathbf{V}$ into 4 warps &mdash; each warp computes a slice of $\mathbf{S}$ and writes it to the shared memory. To reduce shared memory reads/writes and achieve a speedup, FlashAttention 2.0 splits $\mathbf{Q}$ into 4 warps, allowing each query&rsquo;s output to be computed independently without communication between warps.</p>
<h3 id="flashattention-3-asynchrony--low-precision" class="scroll-mt-8 group">
  FlashAttention-3: Asynchrony + Low-Precision
  
    <a href="#flashattention-3-asynchrony--low-precision"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>As mentioned in the beginning, FlashAttention is fascinating and effective because it emerges from careful <em>software-hardware co-design</em>. Both FlashAttention 1.0 and FlashAttention 2.0 are optimized for the <a href="https://en.wikipedia.org/wiki/Ampere_(microarchitecture)">Ampere A100 GPU</a>. The newer <a href="https://en.wikipedia.org/wiki/Hopper_(microarchitecture)">Hopper H100 GPU</a> has its own characteristics, notably asynchronous Tensor Cores and low-precision number formats (FP8), for which FlashAttention 3.0 is optimized. Tri Dao&rsquo;s lab wrote a wonderful <a href="https://tridao.me/blog/2024/flash3/">blogpost</a> on FlashAttention 3.0.</p>
<p>To take advantage of the asynchrony of Hopper Tensor Cores, we can manually enforce &ldquo;pingpong scheduling&rdquo;: whenever one warpgroup is performing the slow softmax, we can schedule a fast GEMM on another warpgroup. The goal is to &ldquo;hide&rdquo; the cost of softmax with GEMMs. Pingpong scheduling can also be done within a warpgroup.</p>
<figure><img src="https://www.dropbox.com/scl/fi/csm5056r9jg7vdjaxz12o/Screenshot-2025-03-16-at-10.44.12-PM.png?rlkey=jnwo9ewhmkpybxav4mpkfhcpu&amp;st=i1v498vd&amp;raw=1"
    alt="Whenever one warpgroup is performing softmax, schedule GEMM on the other warpgroup to make use of this time (source: Shah et al., 2024)." width="1000"><figcaption>
      <p>Whenever one warpgroup is performing softmax, schedule GEMM on the other warpgroup to make use of this time (source: <a href="https://arxiv.org/abs/2407.08608">Shah et al., 2024</a>).</p>
    </figcaption>
</figure>

<p>Quantizing to FP8 can cause errors because large models&rsquo; activation often has outliers. A clever trick, &ldquo;incoherent processing&rdquo;, multiplies $\mathbf{Q}$ and $\mathbf{K}$ each by a random orthogonal matrix $\mathbf{M}$ before quantization. This doesn&rsquo;t affect attention outputs, since $(\mathbf{Q}\mathbf{M})(\mathbf{K}\mathbf{M})^{\top} = \mathbf{Q}\mathbf{K}^{\top}$ and $\mathbf{M}\mathbf{M}^{\top} = \bm{I}$. However, the random $\mathbf{M}$ &ldquo;spreads out&rdquo; potential outliers in $\mathbf{Q}\mathbf{M}$ and $\mathbf{K}\mathbf{M}$, thereby reducing quantization errors.</p>
<h2 id="reduce-compute-anyways-attention-approximation" class="scroll-mt-8 group">
  Reduce Compute Anyways: Attention Approximation
  
    <a href="#reduce-compute-anyways-attention-approximation"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p>The thing about optimization is, once a process becomes very efficient, another process inevitably becomes the new bottleneck. When matrix dimensions are large, computing attention scores is expensive. We can make it less expensive based on two observations:</p>
<ul>
<li><strong>Sparsity</strong>: If only a few keys are relevant to the query, we can just use the compatible $(q_i, k_j)$ pairs (&ldquo;support&rdquo;, or $\mathbf{S}$) to compute attention. This is the idea behind <em>sparse approximation</em>.</li>
<li><strong>Low-rank</strong>: We may be able to find low-rank matrices $\tilde{\mathbf{Q}} \in \mathbb{R}^{N \times m}$ and $\tilde{\mathbf{K}} \in \mathbb{R}^{N \times m}$, where $m &lt; d$, such that $\tilde{\mathbf{Q}}\tilde{\mathbf{K}}^{\top}$ approximates $\mathbf{Q}\mathbf{K}^{\top}$. The procedure of finding them is <em>low-rank</em> approximation.</li>
</ul>
<p>Models like Scatterbrain (<a href="https://arxiv.org/abs/2110.15343">Chen et al, 2021</a>) combines the two flexibly:</p>
<p>$$(\tilde{\mathbf{Q}}\tilde{\mathbf{K}}^{\top} + \mathbf{S})\mathbf{V} = \tilde{\mathbf{Q}}(\tilde{\mathbf{K}}^{\top}\mathbf{V}) + \mathbf{S}\mathbf{V}.$$</p>
<figure><img src="https://www.dropbox.com/scl/fi/qgfmdw1ewydt1g1j4iy4m/Screenshot-2025-03-18-at-9.07.30-PM.png?rlkey=niv90i41jjonlcgtv0yse67oa&amp;st=5w7h7d8s&amp;raw=1"
    alt="Scatterbrain, done by Tri Dao and colleagues before the FlashAttention days, combines low-rank and sparsity approximations (source: Chen et al., 2021)." width="1000"><figcaption>
      <p>Scatterbrain, done by Tri Dao and colleagues before the FlashAttention days, combines low-rank and sparsity approximations (source: <a href="https://arxiv.org/abs/2110.15343">Chen et al., 2021</a>).</p>
    </figcaption>
</figure>

<h3 id="sparse-approximation" class="scroll-mt-8 group">
  Sparse Approximation
  
    <a href="#sparse-approximation"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>The heart of sparse approximation is to quickly find compatible $(q_i, k_j)$ pairs. Popular methods are often based on locality-sensitive hashing (e.g., <a href="https://arxiv.org/abs/2001.04451">Reformer</a>) or clustering (e.g., <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00353/97776/Efficient-Content-Based-Sparse-Attention-with">Routing Transformer</a>).</p>
<p>Locality-sensitive hashing (LSH) is a type of hashing techniques that more likely to hash similar inputs into the same buckets than those that are dissimilar. Reformer (<a href="https://arxiv.org/abs/2001.04451">Kitaev et al., 2020</a>), for instance, uses an angular locality sensitive hash, where vectors (or points) are randomly rotated by the same degree and projected onto signed axes $k$ times. The $k$ signs form the hash signature of each original vector. Vectors closer in the higher dimensional embedding space are more likely to share hash signatures after projection. For each query, we can select keys that share the same or sufficiently similar hash signatures.</p>
<figure><img src="https://www.dropbox.com/scl/fi/gduuc0pltmtqusc5juh89/Screenshot-2025-03-18-at-9.19.16-PM.png?rlkey=3u3slph1r8347cp5hyj0xp48w&amp;st=pbs46qx6&amp;raw=1"
    alt="Reformer uses locality-sensitive hashing (LSH) to select query-key pairs in the same buckets for attention computation (source: Kitaev et al., 2020)." width="1000"><figcaption>
      <p>Reformer uses locality-sensitive hashing (LSH) to select query-key pairs in the same buckets for attention computation (source: <a href="https://arxiv.org/abs/2001.04451">Kitaev et al., 2020</a>).</p>
    </figcaption>
</figure>

<p>Routing Transformers (<a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00353/1923932/tacl_a_00353.pdf">Roy et al., 2021</a>) take a different approach to selecting top $k$ keys: tokens are assigned to clusters by $k$-means clustering ($k$ can be tuned), and each query token attends only to other tokens whose keys belong to the same cluster as its query.</p>
<!-- block sparse -->
<h3 id="low-rank-approximation" class="scroll-mt-8 group">
  Low-Rank Approximation
  
    <a href="#low-rank-approximation"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Sparse approximation is best for, well, sparse matrices. Low-rank approximation doesn&rsquo;t assume sparsity. Performer (<a href="https://openreview.net/forum?id=Ua6zuk0WRH">Choromanski et al., 2021</a>) doesn&rsquo;t even assume low-rankness &mdash; it uses the kernel trick to factorize the full-rank attention matrix into 2 smaller matrix multiplications, reducing complexity from $O(N^2d)$ to $O(Nmd)$:</p>
<p>$$\text{softmax}(\mathbf{Q} \mathbf{K}^{\top})\mathbf{V} \approx \phi(\mathbf{Q})(\phi(\mathbf{K})^{\top}\mathbf{V}).$$</p>
<figure><img src="https://www.dropbox.com/scl/fi/dztunjex6sl10keq0o24x/Screenshot-2025-03-18-at-9.50.08-PM.png?rlkey=oe5h7d63a5p61a3n1lq13z11v&amp;st=606j1a6r&amp;raw=1"
    alt="Performer avoids materializing the full-rank attention matrix by factorizing it into 2 smaller matrix multiplications, reducing time complexity to linear (source: Choromanski et al., 2021)." width="1000"><figcaption>
      <p>Performer avoids materializing the full-rank attention matrix by factorizing it into 2 smaller matrix multiplications, reducing time complexity to linear (source: <a href="https://openreview.net/forum?id=Ua6zuk0WRH">Choromanski et al., 2021</a>).</p>
    </figcaption>
</figure>

<p>$\phi(\cdot)$ is a random feature map that projects each original $d$-vector into an $m$-vector. Rows in the feature map are orthogonal to each other to make each projection more evenly spread, which helps convergence.</p>
<!-- <figure><img src="https://www.dropbox.com/scl/fi/xsf8pqwf9mnrx5ezgig9s/Screenshot-2025-03-18-at-9.53.04-PM.png?rlkey=n7up98waz7jc5nlxwoys0f11g&amp;st=9fg2t8db&amp;raw=1"
    alt="Loki (source: Singhania et al., 2024)." width="1000"><figcaption>
      <p>Loki (source: <a href="https://arxiv.org/abs/2406.02542">Singhania et al., 2024</a>).</p>
    </figcaption>
</figure>
 -->
<h3 id="try-em-all-deepseekmoe" class="scroll-mt-8 group">
  Try &lsquo;Em All: DeepSeekMoE
  
    <a href="#try-em-all-deepseekmoe"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Different attention approximation methods each have their strengths. What&rsquo;s cool about DeepSeek is that it adopts a mixture-of-experts (MoE) design to take advantage of several types of sparse attention.</p>
<figure><img src="https://www.dropbox.com/scl/fi/8nk5j63vr7mk88vzkknjq/Screenshot-2025-03-18-at-9.48.19-PM.png?rlkey=cjf036slvq7wavd5ftiz371cv&amp;st=prcgof8l&amp;raw=1"
    alt="DeepSeekMoE combines 3 types of sparse attention (source: Yuan et al., 2025)." width="1000"><figcaption>
      <p>DeepSeekMoE combines 3 types of sparse attention (source: <a href="https://arxiv.org/abs/2502.11089">Yuan et al., 2025</a>).</p>
    </figcaption>
</figure>

<p>DeepSeekMoE (<a href="https://arxiv.org/abs/2502.11089">Yuan et al., 2025</a>) partitions keys into blocks, selects a representative key per block, and performs top-$k$ selection at the block level. Block sparse attention and block selection are natural for natural language, where nearby tokens tend to have similar relevance to the query. In addition, DeepSeek MoE uses sliding window attention to capture local patterns, without letting them dominate learning. Moreover, these sparsity approximation techniques can be trained end-to-end with language modeling, unlike hash- or clustering-based sparsity methods, which are not inherently learnable.</p>
<h2 id="applications-in-user-sequence-modeling" class="scroll-mt-8 group">
  Applications in User Sequence Modeling
  
    <a href="#applications-in-user-sequence-modeling"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p>It&rsquo;s strange how I keep having this conversation with friends and (former) colleagues about whether we want to stick with search/recommendation/ads (&ldquo;搜广推&rdquo;) ranking or jump on the LLM bandwagon. I was (am?) one of those stubborn ranking kids with little interest in NLP/LLMs, thinking that only ranking models are &ldquo;alive&rdquo; &mdash; adapting to user engagement and essentially shaping what they see.</p>
<p>Last year, I wrote a <a href="/posts/seq_user_modeling/" class="backlink">blogpost</a>
  
   on user sequence modeling since it&rsquo;s the hot topic in search/recommendation/ads, driving major model gains this time around. It&rsquo;s funny that as I read more efficient transformer papers, I realized that many &ldquo;new&rdquo; ideas in user sequence modeling are rooted in classic or SOTA concepts from the NLP/LLM community.</p>
<p>For instance, Alibaba&rsquo;s <a href="https://arxiv.org/pdf/2108.04468">ETA</a> is a well-cited paper on ultra-long sequence modeling, but it&rsquo;s essentially an extension of Reformer &mdash; ETA took it a step further by hashing embeddings into binary vectors so that $O(1)$ Hamming distance can be used to select the top $k$ keys (candidate items) for a query (the target item). Alibaba&rsquo;s latest <a href="https://arxiv.org/html/2503.02542v1">LREA</a> borrows from DeepSeek&rsquo;s <a href="https://arxiv.org/abs/2502.07864">multi-head latent attention</a> borrows from DeepSeek&rsquo;s multi-head latent attention, but instead of compressing the feature dimension, it compresses the sequence dimension.</p>
<figure><img src="https://www.dropbox.com/scl/fi/z2yvkczynk7emflh14cp0/Screenshot-2025-03-18-at-10.00.20-PM.png?rlkey=9581qgz3g578ijojkmzqzq8z8&amp;st=ahzjp0wy&amp;raw=1"
    alt="LREA compresses/decompresses user sequences at the sequence dimension using multi-head latent attention by DeepSeek (source: Song et al., 2025)." width="1000"><figcaption>
      <p>LREA compresses/decompresses user sequences at the sequence dimension using multi-head latent attention by DeepSeek (source: <a href="https://arxiv.org/html/2503.02542v1">Song et al., 2025</a>).</p>
    </figcaption>
</figure>

<p>Over time, I realized there&rsquo;s no true distinction between an NLP MLE and a search/recommendation/ads MLE—great ML engineers are always learning new ideas and applying them wherever they fit.</p>
<h2 id="references" class="scroll-mt-8 group">
  References
  
    <a href="#references"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="understand-deep-learning-systems" class="scroll-mt-8 group">
  Understand Deep Learning Systems
  
    <a href="#understand-deep-learning-systems"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<ol>
<li>Deep learning efficiency = compute + memory + overhead 👉 <a href="https://horace.io/brrr_intro.html"><em>Making Deep Learning Go Brrrr From First Principles</em></a> by Horace He.</li>
<li>Software-hardware co-design 👉 <em>Hardware-aware Algorithms for Sequence Modeling</em> by Tri Dao, <a href="https://www.youtube.com/live/foG0ebzuw34?si=6FSChDzXjBUqAQX8&amp;t=242">talk</a> + <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/slides/cs224n-2024-lecture18-deployment-and-efficiency.pdf">slides</a> at Stanford MLSys.</li>
<li><a href="https://openai.com/index/triton/">Triton</a>, the most popular GPU programming language 👉 <a href="https://triton-lang.org/main/getting-started/tutorials/index.html"><em>Tutorials</em></a> by OpenAI + Triton rewrite (<a href="https://github.com/unslothai/unsloth">repo</a>) of popular LLMs by Unsloth AI.</li>
</ol>
<h3 id="flashattention-io-aware-exact-attention" class="scroll-mt-8 group">
  FlashAttention: IO-Aware, Exact Attention
  
    <a href="#flashattention-io-aware-exact-attention"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<ol start="4">
<li>FlashAttention 1.0 👉 <a href="https://arxiv.org/abs/2205.14135"><em>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</em></a> (2022) by Dao et al., <em>NeurIPS</em>.</li>
<li>FlashAttention 2.0 👉 <a href="https://arxiv.org/abs/2307.08691"><em>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</em></a> (2023) by Dao, <em>ICLR</em>.</li>
<li>FlashAttention 3.0 👉 <a href="https://arxiv.org/abs/2407.08608"><em>FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-Precision</em></a> (2024) by Shah et al., <em>arXiv</em>.</li>
</ol>
<h3 id="fast--accurate-attention-approximations" class="scroll-mt-8 group">
  Fast &amp; Accurate Attention Approximations
  
    <a href="#fast--accurate-attention-approximations"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<ol start="7">
<li>Sparse approximation 👉 <a href="https://arxiv.org/abs/1904.10509"><em>Sparse Transformers</em></a> (2019), <a href="https://arxiv.org/abs/2001.04451"><em>Reformer</em></a> (ICLR 2020), <a href="https://arxiv.org/abs/2003.05997"><em>Routing Transformer</em></a> (ACL 2020), <a href="https://arxiv.org/abs/2406.02542"><em>Loki</em></a> (NeurIPS 2024), <a href="https://arxiv.org/html/2503.02542v1"><em>LREA</em></a> (2025)</li>
<li>Low-rank approximation 👉 <a href="https://arxiv.org/abs/2006.04768"><em>Linformer</em></a> (2020), <a href="https://arxiv.org/abs/2006.16236"><em>Linear Transformer</em></a> (ICML 2020), <a href="https://openreview.net/forum?id=Ua6zuk0WRH"><em>Performer</em></a> (ICLR 2021)</li>
<li>Low-rank + sparse 👉 <a href="https://arxiv.org/abs/2110.15343"><em>Scatterbrain: Unifying Sparse and Low-rank Attention Approximation</em></a> (2021) by Chen et al., NeurIPS.</li>
<li>DeepSeek combines blockwise compression/selection + sliding window attention 👉 <a href="https://arxiv.org/abs/2502.11089"><em>Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</em></a> (2025) by Yuan et al., <em>arXiv</em>.</li>
</ol>
<!-- ## GPU Terminology
- **HBM**: XX
- **SRAM**: XX
- **Compute intensity**: XX
- **FLOPs**: XX
- **Kernel**: an operation
- **Kernel fusion**: xx
- **Tensor core**: xx
- **Wrap**: NVIDIA; AMD has a different name
- **CUDA**: a platform
- **Triton**: a programming language
 -->
<!-- ## Mamba: Attention is Not What You Need
11. Mamba 1.0 👉 [*Mamba: Linear-Time Sequence Modeling with Selective State Spaces*](https://arxiv.org/abs/2312.00752) (2023) by Gu and Dao, *COLM*.
12. Mamba 2.0 👉 [*Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality*](https://arxiv.org/abs/2405.21060) (2024) by Dao and Gu, *ICML*. -->
    </div>
  </article>

  
    <aside class="not-prose flex flex-col space-y-8 border-t pt-6">
    <section class="flex flex-col space-y-4">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-shapes h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="M8.3 10a.7.7 0 0 1-.626-1.079L11.4 3a.7.7 0 0 1 1.198-.043L16.3 8.9a.7.7 0 0 1-.572 1.1Z"
  />
  <rect width="7" height="7" x="3" y="14" rx="1" />
  <circle cx="17.5" cy="17.5" r="3.5" />
</svg>

        <span>Categories</span>
      </h2>

      <ul class="ml-6 flex flex-row flex-wrap items-center space-x-2">
          <li>
            <a href="/categories/gpu/" class="taxonomy category">gpu</a>
          </li>
          <li>
            <a href="/categories/transformers/" class="taxonomy category">transformers</a>
          </li>
          <li>
            <a href="/categories/ml-systems/" class="taxonomy category">ml systems</a>
          </li>
      </ul>
    </section>
    <section class="flex flex-col space-y-4" aria-hidden="true">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-chart-network h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="m13.11 7.664 1.78 2.672M14.162 12.788l-3.324 1.424M20 4l-6.06 1.515M3 3v16a2 2 0 0 0 2 2h16"
  />
  <circle cx="12" cy="6" r="2" />
  <circle cx="16" cy="12" r="2" />
  <circle cx="9" cy="15" r="2" />
</svg>

        <span>Graph</span>
      </h2>

      <content-network-graph
  class="h-64 ml-6"
  data-endpoint="/graph/index.json"
  page="/posts/hardware_aware_transformers/"
></content-network-graph>

    </section>
    <section class="flex flex-col space-y-4">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-newspaper h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="M4 22h16a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v16a2 2 0 0 1-2 2Zm0 0a2 2 0 0 1-2-2v-9c0-1.1.9-2 2-2h2M18 14h-8M15 18h-5"
  />
  <path d="M10 6h8v4h-8V6Z" />
</svg>

        <span>Posts</span>
      </h2>
        <section class="flex flex-col space-y-1">
          <h3 class="flex flex-row items-center space-x-2 text-sm font-semibold">
            <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-arrow-up-from-dot h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path d="m5 9 7-7 7 7M12 16V2" />
  <circle cx="12" cy="21" r="1" />
</svg>

            <span>Outgoing</span>
          </h3>

          <ol class="not-prose ml-6">
    <li>
      <article class="flex flex-row items-center">
        <header class="grow">
          <h3>
            <a
              href="/posts/attention_as_dict/"
              class="truncate text-sm underline decoration-slate-300 decoration-2 underline-offset-4 hover:decoration-inherit"
              title="Attention as Soft Dictionary Lookup"
              >Attention as Soft Dictionary Lookup</a
            >
          </h3>
        </header>
          <ul class="flex flex-row items-center justify-end space-x-2">
              <li>
                <a
                  href="/categories/machine-learning/"
                  class="taxonomy"
                  title="Posts and notes on Machine learning"
                  >Machine learning</a
                >
              </li>
              <li>
                <a
                  href="/categories/natural-language-processing/"
                  class="taxonomy"
                  title="Posts and notes on Natural language processing"
                  >Natural language processing</a
                >
              </li>
          </ul>
      </article>
    </li>
    <li>
      <article class="flex flex-row items-center">
        <header class="grow">
          <h3>
            <a
              href="/posts/seq_user_modeling/"
              class="truncate text-sm underline decoration-slate-300 decoration-2 underline-offset-4 hover:decoration-inherit"
              title="Down the Rabbit Hole: Sequential User Modeling"
              >Down the Rabbit Hole: Sequential User Modeling</a
            >
          </h3>
        </header>
          <ul class="flex flex-row items-center justify-end space-x-2">
              <li>
                <a
                  href="/categories/recommender-systems/"
                  class="taxonomy"
                  title="Posts and notes on Recommender systems"
                  >Recommender systems</a
                >
              </li>
              <li>
                <a
                  href="/categories/information-retrieval/"
                  class="taxonomy"
                  title="Posts and notes on Information retrieval"
                  >Information retrieval</a
                >
              </li>
          </ul>
      </article>
    </li>
</ol>

        </section>
    </section>
</aside>

      </main>
      <footer class="mt-20 border-t border-neutral-100 pt-2 text-xs">
        
<section class="items-top flex flex-row justify-between opacity-70">
  <div class="flex flex-col space-y-2">
      <p>Copyright &copy; 2025, Yuan Meng.</p>
      <div
        xmlns:cc="https://creativecommons.org/ns#"
        xmlns:dct="http://purl.org/dc/terms/"
        about="https://creativecommons.org"
      >
        Content is available under
        <a href="https://creativecommons.org/licenses/by-sa/4.0/" rel="license" class="inline-block" title="Creative Commons Attribution-ShareAlike 4.0 International"
          >CC BY-SA 4.0</a
        >
        unless otherwise noted.
      </div>
        <div
          class="mt-2 flex items-center space-x-2 fill-slate-400 hover:fill-slate-600 motion-safe:transition-colors"
        >
          <div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
  aria-hidden="true"
>
  <title>Creative Commons</title>
  <circle fill="transparent" cx="37.785" cy="28.501" r="28.836" />
  <path
    d="M37.441-3.5c8.951 0 16.572 3.125 22.857 9.372 3.008 3.009 5.295 6.448 6.857 10.314 1.561 3.867 2.344 7.971 2.344 12.314 0 4.381-.773 8.486-2.314 12.313-1.543 3.828-3.82 7.21-6.828 10.143-3.123 3.085-6.666 5.448-10.629 7.086-3.961 1.638-8.057 2.457-12.285 2.457s-8.276-.808-12.143-2.429c-3.866-1.618-7.333-3.961-10.4-7.027-3.067-3.066-5.4-6.524-7-10.372S5.5 32.767 5.5 28.5c0-4.229.809-8.295 2.428-12.2 1.619-3.905 3.972-7.4 7.057-10.486C21.08-.394 28.565-3.5 37.441-3.5zm.116 5.772c-7.314 0-13.467 2.553-18.458 7.657-2.515 2.553-4.448 5.419-5.8 8.6a25.204 25.204 0 0 0-2.029 9.972c0 3.429.675 6.734 2.029 9.913 1.353 3.183 3.285 6.021 5.8 8.516 2.514 2.496 5.351 4.399 8.515 5.715a25.652 25.652 0 0 0 9.943 1.971c3.428 0 6.75-.665 9.973-1.999 3.219-1.335 6.121-3.257 8.713-5.771 4.99-4.876 7.484-10.99 7.484-18.344 0-3.543-.648-6.895-1.943-10.057-1.293-3.162-3.18-5.98-5.654-8.458-5.146-5.143-11.335-7.715-18.573-7.715zm-.401 20.915-4.287 2.229c-.458-.951-1.019-1.619-1.685-2-.667-.38-1.286-.571-1.858-.571-2.856 0-4.286 1.885-4.286 5.657 0 1.714.362 3.084 1.085 4.113.724 1.029 1.791 1.544 3.201 1.544 1.867 0 3.181-.915 3.944-2.743l3.942 2c-.838 1.563-2 2.791-3.486 3.686-1.484.896-3.123 1.343-4.914 1.343-2.857 0-5.163-.875-6.915-2.629-1.752-1.752-2.628-4.19-2.628-7.313 0-3.048.886-5.466 2.657-7.257 1.771-1.79 4.009-2.686 6.715-2.686 3.963-.002 6.8 1.541 8.515 4.627zm18.457 0-4.229 2.229c-.457-.951-1.02-1.619-1.686-2-.668-.38-1.307-.571-1.914-.571-2.857 0-4.287 1.885-4.287 5.657 0 1.714.363 3.084 1.086 4.113.723 1.029 1.789 1.544 3.201 1.544 1.865 0 3.18-.915 3.941-2.743l4 2c-.875 1.563-2.057 2.791-3.541 3.686a9.233 9.233 0 0 1-4.857 1.343c-2.896 0-5.209-.875-6.941-2.629-1.736-1.752-2.602-4.19-2.602-7.313 0-3.048.885-5.466 2.658-7.257 1.77-1.79 4.008-2.686 6.713-2.686 3.962-.002 6.783 1.541 8.458 4.627z"
  />
</svg>
</div><div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
>
  <title>Credit must be given to the creator</title>
  <circle fill="transparent" cx="37.637" cy="28.806" r="28.276" />
  <path
    d="M37.443-3.5c8.988 0 16.57 3.085 22.742 9.257C66.393 11.967 69.5 19.548 69.5 28.5c0 8.991-3.049 16.476-9.145 22.456-6.476 6.363-14.113 9.544-22.912 9.544-8.649 0-16.153-3.144-22.514-9.43C8.644 44.784 5.5 37.262 5.5 28.5c0-8.761 3.144-16.342 9.429-22.742C21.101-.415 28.604-3.5 37.443-3.5zm.114 5.772c-7.276 0-13.428 2.553-18.457 7.657-5.22 5.334-7.829 11.525-7.829 18.572 0 7.086 2.59 13.22 7.77 18.398 5.181 5.182 11.352 7.771 18.514 7.771 7.123 0 13.334-2.607 18.629-7.828 5.029-4.838 7.543-10.952 7.543-18.343 0-7.276-2.553-13.465-7.656-18.571-5.104-5.104-11.276-7.656-18.514-7.656zm8.572 18.285v13.085h-3.656v15.542h-9.944V33.643h-3.656V20.557c0-.572.2-1.057.599-1.457.401-.399.887-.6 1.457-.6h13.144c.533 0 1.01.2 1.428.6.417.4.628.886.628 1.457zm-13.087-8.228c0-3.008 1.485-4.514 4.458-4.514s4.457 1.504 4.457 4.514c0 2.971-1.486 4.457-4.457 4.457s-4.458-1.486-4.458-4.457z"
  />
</svg>
</div><div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
>
  <title>Adaptations must be shared under the same terms</title>
  <circle fill="transparent" cx="36.944" cy="28.631" r="29.105" />
  <path
    d="M37.443-3.5c8.951 0 16.531 3.105 22.742 9.315C66.393 11.987 69.5 19.548 69.5 28.5c0 8.954-3.049 16.457-9.145 22.514-6.437 6.324-14.076 9.486-22.912 9.486-8.649 0-16.153-3.143-22.514-9.429C8.644 44.786 5.5 37.264 5.5 28.501c0-8.723 3.144-16.285 9.429-22.685C21.138-.395 28.643-3.5 37.443-3.5zm.114 5.772c-7.276 0-13.428 2.572-18.457 7.715-5.22 5.296-7.829 11.467-7.829 18.513 0 7.125 2.59 13.257 7.77 18.4 5.181 5.182 11.352 7.771 18.514 7.771 7.123 0 13.334-2.609 18.629-7.828 5.029-4.876 7.543-10.99 7.543-18.343 0-7.313-2.553-13.485-7.656-18.513-5.067-5.145-11.239-7.715-18.514-7.715zM23.271 23.985c.609-3.924 2.189-6.962 4.742-9.114 2.552-2.152 5.656-3.228 9.314-3.228 5.027 0 9.029 1.62 12 4.856 2.971 3.238 4.457 7.391 4.457 12.457 0 4.915-1.543 9-4.627 12.256-3.088 3.256-7.086 4.886-12.002 4.886-3.619 0-6.743-1.085-9.371-3.257-2.629-2.172-4.209-5.257-4.743-9.257H31.1c.19 3.886 2.533 5.829 7.029 5.829 2.246 0 4.057-.972 5.428-2.914 1.373-1.942 2.059-4.534 2.059-7.771 0-3.391-.629-5.971-1.885-7.743-1.258-1.771-3.066-2.657-5.43-2.657-4.268 0-6.667 1.885-7.2 5.656h2.343l-6.342 6.343-6.343-6.343 2.512.001z"
  />
</svg>
</div>
        </div>

  </div>
    <div>
      <a
        href="https://github.com/michenriksen/hugo-theme-til"
        title="Today I Learned &#8212; A Hugo theme by Michael Henriksen"
        data-theme-version="0.4.0"
        >theme: til</a
      >
    </div>
</section>

      </footer>
    </div>

    
    <button id="back-to-top" title="Go to top">☝️</button>


    
    

    
    <script src="/js/back-to-top.js"></script>

     
    <script src="/js/cat-cursor.js" defer></script>
  </body>
</html>
