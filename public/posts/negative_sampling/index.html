<!doctype html>
<html
  lang="en-us"
  dir="ltr"
>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<link rel="stylesheet" href="http://localhost:1313/css/styles.min.30bb45eb22a875545611138569e9d0de6a7f6d840a287bda29b5f6ea87c1d2f1.css">
<meta charset="utf-8" />
<meta name="language" content="en" />
<meta name="viewport" content="width=device-width" />
<title>
    Negative Sampling for Learning Two-Tower Networks | Yuan Meng
</title>
  <meta name="description" content=" Web-Scale Recommender Systems Two-Stage Architecture The iconic YouTube paper (Covington et al., 2016) introduced a two-stage architecture that since became the industry standard for large-scale recommender systems:
Retrieval (a.k.a. “candidate generation”): Quickly select top k (in the hundreds or thousands) loosely relevant items from a large corpus of billions Ranking (a.k.a. “reranking”): Order final candidates (dozens) by predicted reward probability (e.g., an ads click, a listing booking, a video watch, etc.) " />
<meta property="og:url" content="http://localhost:1313/posts/negative_sampling/">
  <meta property="og:site_name" content="Yuan Meng">
  <meta property="og:title" content="Negative Sampling for Learning Two-Tower Networks">
  <meta property="og:description" content="Web-Scale Recommender Systems Two-Stage Architecture The iconic YouTube paper (Covington et al., 2016) introduced a two-stage architecture that since became the industry standard for large-scale recommender systems:
Retrieval (a.k.a. “candidate generation”): Quickly select top k (in the hundreds or thousands) loosely relevant items from a large corpus of billions Ranking (a.k.a. “reranking”): Order final candidates (dozens) by predicted reward probability (e.g., an ads click, a listing booking, a video watch, etc.)">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-09-02T00:00:00+00:00">


  <meta itemprop="name" content="Negative Sampling for Learning Two-Tower Networks">
  <meta itemprop="description" content="Web-Scale Recommender Systems Two-Stage Architecture The iconic YouTube paper (Covington et al., 2016) introduced a two-stage architecture that since became the industry standard for large-scale recommender systems:
Retrieval (a.k.a. “candidate generation”): Quickly select top k (in the hundreds or thousands) loosely relevant items from a large corpus of billions Ranking (a.k.a. “reranking”): Order final candidates (dozens) by predicted reward probability (e.g., an ads click, a listing booking, a video watch, etc.)">
  <meta itemprop="datePublished" content="2024-09-02T00:00:00+00:00">
  <meta itemprop="wordCount" content="2571">
  <meta itemprop="keywords" content="Negative sampling,Recommender system">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Negative Sampling for Learning Two-Tower Networks">
  <meta name="twitter:description" content="Web-Scale Recommender Systems Two-Stage Architecture The iconic YouTube paper (Covington et al., 2016) introduced a two-stage architecture that since became the industry standard for large-scale recommender systems:
Retrieval (a.k.a. “candidate generation”): Quickly select top k (in the hundreds or thousands) loosely relevant items from a large corpus of billions Ranking (a.k.a. “reranking”): Order final candidates (dozens) by predicted reward probability (e.g., an ads click, a listing booking, a video watch, etc.)">

<link rel="canonical" href="http://localhost:1313/posts/negative_sampling/" />

    <link rel="stylesheet" href="/css/index.css" />


      <script src="/js/main.js" defer></script>
  

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org/",
  "@id": "http://localhost:1313/posts/negative_sampling/",
  "@type": "BlogPosting",
  "articleSection": [
    "Negative sampling",
    "Recommender system"
  ],
  "author": {
    "@type": "Person",
    "email": "mycaptainmy@gmail.com",
    "name": "Yuan Meng",
    "url": "http://localhost:1313/about/"
  },
  "copyrightNotice": "Yuan Meng",
  "datePublished": "2024-09-02",
  "description": " Web-Scale Recommender Systems Two-Stage Architecture The iconic YouTube paper (Covington et al., 2016) introduced a two-stage architecture that since became the industry standard for large-scale recommender systems:\nRetrieval (a.k.a. “candidate generation”): Quickly select top k (in the hundreds or thousands) loosely relevant items from a large corpus of billions Ranking (a.k.a. “reranking”): Order final candidates (dozens) by predicted reward probability (e.g., an ads click, a listing booking, a video watch, etc.) ",
  "headline": "Negative Sampling for Learning Two-Tower Networks",
  "isPartOf": {
    "@id": "http://localhost:1313/posts/",
    "@type": "Blog",
    "name": "Posts"
  },
  "mainEntityOfPage": "http://localhost:1313/posts/negative_sampling/",
  "name": "Negative Sampling for Learning Two-Tower Networks",
  "timeRequired": "PT13M",
  "url": "http://localhost:1313/posts/negative_sampling/",
  "wordCount": 2571
}
</script>


  </head>
  <body>
    <div class="container mx-auto flex max-w-prose flex-col space-y-10 p-4 md:p-6">
      <header class="flex flex-row items-center justify-between">
        <div>
  <a id="skip-nav" class="sr-only" href="#maincontent">Skip to main content</a>
  <a class="font-semibold" href="/">Yuan Meng</a>
</div>

  <nav>
    <ul class="flex flex-row items-center justify-end space-x-4">
    <li>
      <a href="/about/">About</a
      >
    </li>
    <li>
      <a aria-current="true" class="ancestor" href="/posts/">Posts</a
      >
    </li>
    <li>
      <a href="/notes/">Notes</a
      >
    </li>
    </ul>
  </nav>


      </header>
      <main class="prose prose-slate relative md:prose-lg prose-h1:text-[2em]" id="maincontent">
        <article class="main">
    <header>
      <h1 class="!mb-1">Negative Sampling for Learning Two-Tower Networks</h1><div class="flex flex-row items-center space-x-4">
          <time class="text-sm italic opacity-80" datetime="2024-09-02T00:00:00&#43;00:00">September 2, 2024</time>
        </div>
    </header>

    
    
      Reading time: 13 minutes
    

    
    
      <div class="toc-container">
        <span id="toc-toggle">
          <span id="toc-icon">▶</span> 
          <span>Table of Contents</span>
        </span>
        <nav id="TableOfContents" class="toc-content">
          <nav id="TableOfContents">
  <ul>
    <li><a href="#web-scale-recommender-systems">Web-Scale Recommender Systems</a>
      <ul>
        <li><a href="#two-stage-architecture">Two-Stage Architecture</a></li>
        <li><a href="#two-tower-network-for-retrieval">Two-Tower Network for Retrieval</a></li>
      </ul>
    </li>
    <li><a href="#negative-sampling-approaches">Negative Sampling Approaches</a>
      <ul>
        <li><a href="#random-negative-sampling-rns">Random Negative Sampling (RNS)</a></li>
        <li><a href="#batch-negative-sampling-bns">Batch Negative Sampling (BNS)</a></li>
        <li><a href="#mixed-negative-sampling-mns">Mixed Negative Sampling (MNS)</a></li>
        <li><a href="#online-hard-negative-mining-hnm">Online Hard Negative Mining (HNM)</a></li>
      </ul>
    </li>
    <li><a href="#case-studies">Case Studies</a>
      <ul>
        <li><a href="#social-media">Social Media</a>
          <ul>
            <li><a href="#facebook-people-search-kdd-2020httpsdlacmorgdoiabs10114533944863403305">Facebook People Search (<a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403305">KDD 2020</a>)</a></li>
            <li><a href="#linkedin-job-search-kdd-2024httpsarxivorgabs240213435">LinkedIn Job Search (<a href="https://arxiv.org/abs/2402.13435">KDD 2024</a>)</a></li>
          </ul>
        </li>
        <li><a href="#e-commerce">E-Commerce</a>
          <ul>
            <li><a href="#jdcom-product-search-sigir-2020httpsdlacmorgdoiabs10114533972713401446">JD.com Product Search (<a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401446">SIGIR 2020</a>)</a></li>
            <li><a href="#amazon-product-search-kdd-2019httpswwwamazonsciencepublicationssemantic-product-search">Amazon Product Search (<a href="https://www.amazon.science/publications/semantic-product-search">KDD 2019</a>)</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#learn-more">Learn More</a>
      <ul>
        <li><a href="#papers">Papers</a></li>
        <li><a href="#blogposts--repos">Blogposts + Repos</a></li>
      </ul>
    </li>
  </ul>
</nav>
        </nav>
      </div>

      <script>
        
        document.addEventListener('DOMContentLoaded', function () {
          var tocToggle = document.getElementById('toc-toggle');
          var tocContent = document.getElementById('TableOfContents');
          var tocIcon = document.getElementById('toc-icon');
          tocToggle.addEventListener('click', function () {
            if (tocContent.style.display === 'none' || tocContent.style.display === '') {
              tocContent.style.display = 'block';
              tocIcon.textContent = '▼'; 
            } else {
              tocContent.style.display = 'none';
              tocIcon.textContent = '▶'; 
            }
          });
        });
      </script>
    

    
    <div class="content">
      <h2 id="web-scale-recommender-systems" class="scroll-mt-8 group">
  Web-Scale Recommender Systems
  
    <a href="#web-scale-recommender-systems"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="two-stage-architecture" class="scroll-mt-8 group">
  Two-Stage Architecture
  
    <a href="#two-stage-architecture"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>The iconic YouTube paper (<a href="https://research.google/pubs/deep-neural-networks-for-youtube-recommendations/">Covington et al., 2016</a>) introduced a two-stage architecture that since became the industry standard for large-scale recommender systems:</p>
<ul>
<li><strong>Retrieval</strong> (a.k.a. &ldquo;candidate generation&rdquo;): Quickly select top k (in the hundreds or thousands) loosely relevant items from a large corpus of billions</li>
<li><strong>Ranking</strong> (a.k.a. &ldquo;reranking&rdquo;): Order final candidates (dozens) by predicted reward probability (e.g., an ads click, a listing booking, a video watch, <em>etc.</em>)</li>
</ul>
<figure><img src="https://www.dropbox.com/scl/fi/xfzcn69xt5umpiqgiblvj/Screenshot-2024-09-01-at-3.27.32-PM.png?rlkey=co9ingrwzjdawfkj4nfg7j4eg&amp;st=ym59f9sr&amp;raw=1" width="500">
</figure>

<h3 id="two-tower-network-for-retrieval" class="scroll-mt-8 group">
  Two-Tower Network for Retrieval
  
    <a href="#two-tower-network-for-retrieval"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>Reranking models are highly customized for each product at each company, tailored to bespoke ranking objectives for the business. For retrieval, the two-tower network is the go-to choice at many companies to efficiently maximize Recall@k.</p>
<figure><img src="https://www.dropbox.com/scl/fi/xg3m8ro5h06brz50q4rcd/Screenshot-2024-09-02-at-1.23.11-PM.png?rlkey=sn15v2btnk0aameivbjtork2j&amp;st=zhbxc8hu&amp;raw=1" width="500">
</figure>

<p>As the name suggests, the two-tower network consists of two deep neural networks, each shaped like a &ldquo;tower&rdquo;, with every successive hidden layer halving in the number of hidden units. The left tower encodes a query $x \in \mathcal{X}$ (e.g., usually contains a user in some context, and sometimes also a search query or a seed item) whereas the right tower encodes an item $y \in \mathcal{Y}$. The query and the item towers both take sparse IDs and dense features as inputs and learn a function mapping from the input feature to a dense embedding, $u : \mathcal{X} \times \mathbb{R}^d \rightarrow \mathbb{R}^k$ and $v : \mathcal{Y} \times \mathbb{R}^d \rightarrow \mathbb{R}^k$, respectively. The output of the network is the dot product of the query and the item embeddings,</p>
<p>$$s(x, y) = \langle u(x, \theta), v(y, \theta) \rangle.$$</p>
<p>How do we know if $s(x_i, y_i)$ is any good for the pair of query $x_i$ and item $y_i$ or not?</p>
<p>Intuitively, if item $y_i$ is positive for query $x_i$, $s(x_i, y_i)$ should be greater than if $y_i$ is negative for $x_i$. After training, the model should learn to embed positive $(x_i, y_i)$ pairs closer in the embedding space and negative pairs further apart.</p>
<p>Formally, we can frame retrieval as extreme multi-class classification: Given the query $x_i$, predict the probability of selecting each item $y_j$ from $M$ items $\{y_j\}_{j=1}^M$ &mdash;</p>
<p>$$p(y_i | x_i; \theta) = \frac{e^{s (x_i, y_i)}}{\sum_{j \in [M]} e^{s (x_i, y_j)}},$$</p>
<p>where $\theta$ is the model parameters. Each pair of query $x_i$ and item $y_i$ is associated with a reward $r_i \in \mathbb{R}$ denoting binary (1: positive; 0: negative) or varying degrees of user engagement. A common setup in training is that for each $x_i$, only $y_i$ is positive, whereas every $y_{i \neq j}$ is negative. A good model should predict $p(y_i | x_i; \theta) &gt; p(y_j | x_i; \theta)$ for all $j \neq i$. How &ldquo;wrong&rdquo; $p(y_i | x_i; \theta)$ is can be given by the softmax loss:</p>
<p>$$\mathcal{l(\theta)} = -r_i \cdot \log(p(y_i | x_i; \theta)) = -r_i \cdot \log(\frac{e^{s (x_i, y_i)}}{\sum_{j \in [M]} e^{s (x_i, y_j)}}),$$</p>
<p>For those from an NLP background, the above framing is exactly the same as <em><a href="https://paperswithcode.com/task/language-modelling">language modeling</a></em>, i.e., given a sequence of tokens $x_{0:t - 1}$, predict the probability of the next token $x_t$ from the vocabulary. In fact, the two-tower network itself is inspired by the dual encoder architecture first popularized in NLP (e.g., <a href="ttps://aclanthology.org/W16-1617.pdf">Neculoiu, 2016</a>).</p>
<p>In language modeling, the corpus size $M$ is in the order of tens of thousands (e.g., 30,522 in <a href="https://arxiv.org/abs/1810.04805">original BERT</a>, 40,478 in <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">original GPT</a>) &mdash; already, exhaustive softmax computation is <a href="https://arxiv.org/abs/1412.2007">way too expensive</a>; in a large-scale recommender system with billions of items (Netflix is an exception with &lt; 10k titles), it just cannot be done.</p>
<p>A neat solution is <em>negative sampling</em> (or <a href="https://www.tensorflow.org/extras/candidate_sampling.pdf">&ldquo;candidate sampling&rdquo;</a>), where we use a subset of negative items from $M$ to compute the loss for each positive $(x_i, y_i)$ pair, instead of using all negatives in the full training set. There is a vast body of literature on negative sampling since it&rsquo;s extremely common across NLP, CV, graphs, and information retrieval. You can find 100+ papers in this amazing <a href="https://github.com/RUCAIBox/Negative-Samplaing-Paper">repo</a>. This post focuses on several widely adapted approaches in web-scale recommender systems.</p>
<h2 id="negative-sampling-approaches" class="scroll-mt-8 group">
  Negative Sampling Approaches
  
    <a href="#negative-sampling-approaches"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<p>Xu et al.&rsquo;s (2022) <a href="https://arxiv.org/abs/2206.00212">review paper</a> organizes negative sampling approaches into the following taxonomy and distills 4 principles of what makes a good negative sampler:</p>
<figure><img src="https://www.dropbox.com/scl/fi/dez9g6p5enhzq7eagllom/Screenshot-2024-09-01-at-11.16.27-PM.png?rlkey=ye85jjtq09rhythgb6cmfqo9x&amp;st=ltmulfdi&amp;raw=1" width="800">
</figure>

<ol>
<li><strong>Efficient</strong>: The negative sampler should have low time and space complexity;</li>
<li><strong>Effective</strong>: Informative negative samples allow for fast model convergence;</li>
<li><strong>Stable</strong>: The negative sampler should still work when we switch datasets;</li>
<li><strong>Data-independent</strong>: The negative sampler should not rely too much on side information (e.g., query/item metadata, contexts), which may not always be available in all training examples or generalize to other examples.</li>
</ol>
<h3 id="random-negative-sampling-rns" class="scroll-mt-8 group">
  Random Negative Sampling (RNS)
  
    <a href="#random-negative-sampling-rns"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>The simplest approach is random negative sampling (RNS) from a uniform distribution. We draw an integer $i$ from $\mathcal{U}_{[1, M]}$ and put the $i$-th item from the corpus $M$ into our random negative set. We repeat this process until the desired negative sample size is reached. If a sampler doesn&rsquo;t outperform RNS, it is practically useless.</p>
<blockquote>
<p><span style="background-color: #FDB515"><b>Performance review:</b></span> Good baseline, but don&rsquo;t use it alone</p>
<ol>
<li><strong>Efficient</strong> ❌: We have to go through the entire corpus to sample items</li>
<li><strong>Effective</strong> ❌: May not always return informative negatives</li>
<li><strong>Stable</strong> ✅: No additional setup to make RNS work in new corpuses</li>
<li><strong>Data-independent</strong> ✅: RNS doesn&rsquo;t rely on side information</li>
</ol>
</blockquote>
<h3 id="batch-negative-sampling-bns" class="scroll-mt-8 group">
  Batch Negative Sampling (BNS)
  
    <a href="#batch-negative-sampling-bns"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>To avoid sampling from the entire corpus, we can sample from the mini-batch: For each $(x_i, y_i)$ pair, treat all other items in the current batch as sampled negatives.</p>
<figure><img src="https://www.dropbox.com/scl/fi/dbuhiukd4ugmrwe39z8cm/Screenshot-2024-09-01-at-2.45.08-PM.png?rlkey=6099axb8yp3rjq6qbsodlo4ez&amp;st=cqum67rs&amp;raw=1" width="800">
</figure>

<p>Batch negative sampling (BNS) is extremely efficient since we don&rsquo;t have to get negatives elsewhere and works well when the batch size is reasonable (e.g., 2048). As such, BNS is extremely commonly used in the industry. Naively, we can rewrite the loss function as follows, where the corpus $M$ is replaced with the batch $B$ &mdash;</p>
<p>$$\mathcal{l(\theta)} = -r_i \cdot \log(p(y_i | x_i; \theta)) = -r_i \cdot \log(\frac{e^{s (x_i, y_i)}}{\sum_{j \in [B]} e^{s (x_i, y_j)}}).$$</p>
<p>BNS suffers from selection bias as popular items appear in many mini-batches. The probability of selecting an item follows a <a href="https://aclanthology.org/2021.findings-acl.326.pdf">unigram distribution</a>, where each point on the x-axis represents an item and the y-axis its probability density (perhaps the most boring distribution 😂). The related <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipfian distribution</a> is fascinating &mdash; in many languages, the top few popular words dominate word occurrences; for example, &ldquo;the&rdquo; accounts for ~7% of English word occurrences. American linguist <a href="https://en.wikipedia.org/wiki/George_Kingsley_Zipf">George Kingsley Zipf</a> observed that the most common word often occurs twice as often as the second most common, three times as often as the third, dubbed as the Zipf&rsquo;s law:</p>
<p>$$\text{word frequency} \propto \frac{1}{\text{word rank}}.$$</p>
<p>Drawing yet another NLP analogy to recommender systems: The top few popular items dominate engagement logs. As a result, they often serve as negatives for many $(x_i, y_i)$ pairs, while long-tail negative items don&rsquo;t get demoted enough. Consequently, models trained on batch negatives alone often return strange long-tail items irrelevant to the users (false positives) or filter out popular items (false negatives).</p>
<p>There are 2 popular ways to correct for sampling bias in BNG:</p>
<ol>
<li><strong>logQ correction</strong>: Correct the model logit to $s^c(x_i, y_j) = s(x_i, y_i) - \log Q(i)$, where $Q(i)$ is the sampling probability of item $j$, given by $\frac{\mathrm{count}(i)}{\sum_j \mathrm{count}(i)}$ 👉 <strong>motivation</strong>: Avoid over-penalizing popular items, or at least to a lesser degree</li>
<li><strong>Mixed Negative Sampling</strong>: Sample additional negatives and combine them with BNS 👉 <strong>motivation</strong>: The model gets to see more negatives from other distributions</li>
</ol>
<blockquote>
<p><span style="background-color: #FDB515"><b>Performance review:</b></span> Super popular, but requires bias correction</p>
<ol>
<li><strong>Efficient</strong> ✅: Can get all negatives from the mini-batch</li>
<li><strong>Effective</strong> 🤔: Over-penalizes popular items without correction</li>
<li><strong>Stable</strong> ✅: No additional setup to make BNS work in new corpuses</li>
<li><strong>Data-independent</strong> ✅: BNS doesn&rsquo;t rely on side information</li>
</ol>
</blockquote>
<h3 id="mixed-negative-sampling-mns" class="scroll-mt-8 group">
  Mixed Negative Sampling (MNS)
  
    <a href="#mixed-negative-sampling-mns"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>As mentioned, Mixed Negative Sampling (MNS) mitigates the sampling bias in BNS. In the Google paper (<a href="https://research.google/pubs/mixed-negative-sampling-for-learning-two-tower-neural-networks-in-recommendations/">Yang et al., 2020</a>) that  introduced MNS, it was done by combining RNS and BNS: For a batch $B$, randomly draw $B&rsquo;$ negatives from a uniform distribution.</p>
<figure><img src="https://www.dropbox.com/scl/fi/d6svgsxna1aif9wcgx8l1/Screenshot-2024-09-01-at-2.45.27-PM.png?rlkey=iigixzkybw4xbotpu440aslzi&amp;st=f897vrvf&amp;raw=1" width="800">
</figure>

<p>Under MNS, the loss function now becomes the following &mdash;</p>
<p>$$\mathcal{l(\theta)} = -r_i \cdot \log(p(y_i | x_i; \theta)) = -r_i \cdot \log(\frac{e^{s (x_i, y_i)}}{\sum_{j \in [B + B&rsquo;]} e^{s (x_i, y_j)}}).$$</p>
<p>$B&rsquo;$ is a hyperparameter we can tune to achieve the best eval and A/B performances:</p>
<ul>
<li><strong>$B&rsquo;$ is too large</strong>: Then sampling distribution is close to the uniform distribution, which deviates from the true distribution at serving time, and also, too large a batch $B + B&rsquo;$ makes training inefficient;</li>
<li><strong>$B&rsquo;$ is too small</strong>: Then sampling distribution is close to the unigram, where long-tail items don&rsquo;t get many chances to be learned by the model</li>
</ul>
<blockquote>
<p><span style="background-color: #FDB515"><b>Performance review:</b></span> A good trade-off between efficiency and effectiveness</p>
<ol>
<li><strong>Efficient</strong> ✅: Can cache or share negatives across batches for $B'$</li>
<li><strong>Effective</strong> ✅: Corrects sampling bias in the popular BNS</li>
<li><strong>Stable</strong> 🤔: Need additional indexing work in new corpuses</li>
<li><strong>Data-independent</strong> ✅: MNS doesn&rsquo;t rely on side information</li>
</ol>
</blockquote>
<h3 id="online-hard-negative-mining-hnm" class="scroll-mt-8 group">
  Online Hard Negative Mining (HNM)
  
    <a href="#online-hard-negative-mining-hnm"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<p>The 3 sampling approaches above (DNS, BNS, MNS) are all static samplers, since the sampling probability for each item $j$ is fixed and independent of the model&rsquo;s learning progress. Researchers (e.g., <a href="https://arxiv.org/abs/1904.03626">Hacohen and Weinshal, 2019</a>) found that introducing hard negatives (e.g., negative items with high predicted scores) can accelerate model convergence. This approach is &ldquo;curriculum learning,&rdquo; and using model-generated scores to select hard negatives is &ldquo;online Hard Negative Mining (HNM)&rdquo;. Starting with hard negatives may confuse the model, but after sufficient training on easy negatives from a static sampler, adding hard negatives helps the model align its &ldquo;learned hypothesis&rdquo; with the &ldquo;target hypothesis,&rdquo; leading to faster convergence.</p>
<blockquote>
<p><span style="background-color: #FDB515"><b>Performance review:</b></span> Great addition to MNS, but don&rsquo;t start with it</p>
<ol>
<li><strong>Efficient</strong> ✅: Can use the model itself to rank items in mini-batch</li>
<li><strong>Effective</strong> ✅: Accelerates convergence of a sufficiently trained model</li>
<li><strong>Stable</strong> ❌: Sample selection is highly reliant on the model</li>
<li><strong>Data-independent</strong> ❌: The model <em>is</em> the sampler and may use &ldquo;side info&rdquo;</li>
</ol>
</blockquote>
<h2 id="case-studies" class="scroll-mt-8 group">
  Case Studies
  
    <a href="#case-studies"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="social-media" class="scroll-mt-8 group">
  Social Media
  
    <a href="#social-media"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<h4 id="facebook-people-search-kdd-2020httpsdlacmorgdoiabs10114533944863403305" class="scroll-mt-8 group">
  Facebook People Search (<a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403305">KDD 2020</a>)
  
    <a href="#facebook-people-search-kdd-2020httpsdlacmorgdoiabs10114533944863403305"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<blockquote>
<p><span style="background-color: #0081FB"><b>Highlight:</b></span> If Google popularized embedding-based retrieval (EBR) in recommender systems, the Facebook Search paper sets an inspiring example for applying EBR in personalized search. Apart from standard two-tower training, it discussed <em>hard negative mining</em> and <em>serving challenges</em> in detail.</p>
</blockquote>
<figure><img src="https://www.dropbox.com/scl/fi/ulxyqvmwt0puycqdnr9q4/Screenshot-2024-09-02-at-5.47.39-PM.png?rlkey=up336nmsvuxyk0bhk1954quzg&amp;st=wqnhynoj&amp;raw=1" width="1500">
</figure>

<p>Search retrieval based on <em>semantic</em> embeddings has been used in CV (e.g,, visual search/discovery) and NLP (e.g., passage retrieval) for years, but searching people on Facebook is not a pure semantic problem: When you and I both search for &ldquo;John Smith&rdquo;, we each want to see our friends, rather than the same set of profiles.</p>
<p>To personalize retrieval, the authors trained a two-tower network, where the query and the item towers both encode textual (e.g., search query, profile texts), contextual (e.g., location), and social (e.g., social graph embeddings) features.</p>
<figure><img src="https://www.dropbox.com/scl/fi/1hwfalp8enjs46yb6ay1f/Screenshot-2024-09-02-at-6.13.10-PM.png?rlkey=0wmz220if10ug86tstg4rjwdw&amp;st=u14u338g&amp;raw=1" width="500">
</figure>

<p>They explored various positive and negative label definitions. For positives, it didn&rsquo;t matter whether clicks or impressions were treated as positives &mdash; the results were the same. For negatives, the authors compared only training on hard negatives, pure BNS, and combining the two &mdash; the hybrid sampling approach turned out the best.</p>
<ul>
<li><strong>All hard negatives</strong> ❌: Sampling impressed but  unclicked results as negatives led to <em>55% absolute recall drop</em> 👉 <strong>why</strong>: impressed results are often partial matches (e.g., a &ldquo;John Smith&rdquo;, but not my John Smith); treating them as negatives teaches the model such matches are useless, or worse, hurt relevance</li>
<li><strong>BNS + hard negatives</strong> ✅: BNS was a good baseline; adding hard negatives (a &ldquo;John Smith&rdquo;, but not my John Smith) to batch negatives improved performance 👉 <strong>why</strong>: forcing the model to use social features to predict for hard negatives</li>
</ul>
<h4 id="linkedin-job-search-kdd-2024httpsarxivorgabs240213435" class="scroll-mt-8 group">
  LinkedIn Job Search (<a href="https://arxiv.org/abs/2402.13435">KDD 2024</a>)
  
    <a href="#linkedin-job-search-kdd-2024httpsarxivorgabs240213435"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<blockquote>
<p><span style="background-color: #0e76a8"><b>Highlight:</b></span> Interesting paper in 3 regards &mdash; 1) the same EBR service is <em>shared by ads and organic search</em>, which have different objectives (ads: job posters care the most about if applicants are qualified; organic: job seekers are driven by interests); 2) the authors applied <em>curriculum training</em>, warming up the model with easy negatives and then switching to online hard negatives; 3) they got <em>exact KNN</em> search to work (via GPU inference)!</p>
</blockquote>
<figure><img src="https://www.dropbox.com/scl/fi/jkmg0ici348di9cl24god/Screenshot-2024-09-02-at-7.03.23-PM.png?rlkey=0pa4bs5zq587aj7gz5szzq39y&amp;st=lob080d4&amp;raw=1" width="1500">
</figure>

<p>Unlike other social media where viral contents are celebrated, LinkedIn has to strike a delicate balance: For each job posting, they need to deliver <em>sufficient</em> candidates, but not <em>overwhelmingly</em> so &mdash; a tight match between seekers and jobs is key. At LinkedIn, seekers $S$ and jobs $J$ both have a rich set of metadata (e.g., job title, seniority, company, etc.). From confirmed hire data, they learned important links between seeker and job metadata. This graph powered LinkedIn retrieval.</p>
<figure><img src="https://www.dropbox.com/scl/fi/vdubss5sl610zfnin213l/Screenshot-2024-09-02-at-7.23.37-PM.png?rlkey=c5309d5i9fh1a02do3uz0gytl&amp;st=cvn1w3ot&amp;raw=1" width="1500">
</figure>

<p>When first deployed, EBR failed to beat the baseline. The team found 2 solutions:</p>
<ul>
<li><strong>Rules</strong>: Enforce term-based matching rules to address relevance defects</li>
<li><strong>Exact KNN search</strong>: Due to the sheer volume of documents and strict latency requirements, modern recommender systems and search engines have embraced Approximate Nearest Neighbor search, which sacrifices accuracy for speed (see my post for a <a href="https://www.yuan-meng.com/posts/ebr/">review</a>) 👉 LinkedIn, somehow, got exact KNN to work and saw relevance improvements; this was achieved by mixing term-based retrieval (TBR) with EBR and using GPUs to speed up matrix operations behind EBR</li>
</ul>
<h3 id="e-commerce" class="scroll-mt-8 group">
  E-Commerce
  
    <a href="#e-commerce"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<h4 id="jdcom-product-search-sigir-2020httpsdlacmorgdoiabs10114533972713401446" class="scroll-mt-8 group">
  JD.com Product Search (<a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401446">SIGIR 2020</a>)
  
    <a href="#jdcom-product-search-sigir-2020httpsdlacmorgdoiabs10114533972713401446"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<blockquote>
<p><span style="background-color: #E31D1A"><b>Highlight:</b></span> The author crafted a multi-head design for the query tower to capture <em>polysemy</em> (one word can have different semantics &mdash; e.g., &ldquo;apple&rdquo; can either be an electronics brand or a fruit) and generalized the scoring function in the two-tower network output from dot product to attention</p>
</blockquote>
<figure><img src="https://www.dropbox.com/scl/fi/fliq5gl9po3863xmn7kom/Screenshot-2024-09-02-at-7.44.08-PM.png?rlkey=9xkry47poqxy9zuqov49e34ru&amp;st=l3kmiqpy&amp;raw=1" width="1500">
</figure>

<p>The two-tower output scoring function $s(x, y) = \langle u(x, \theta), v(y, \theta) \rangle$ makes 2 assumptions:</p>
<ul>
<li><strong>Output quantity</strong>: Each tower outputs a single embedding for scoring</li>
<li><strong>Scoring method</strong>: We apply dot product on embeddings to get the final logit</li>
</ul>
<p>Both assumptions were modified to improve product search at JD.com &mdash;</p>
<ul>
<li><strong>Output quantity</strong>: The query tower outputs multiple embeddings, to capture semantics of polysemous queries 👉 the authors indeed found one head retrieved fruits and the other cellphones when the user searched for &ldquo;apple&rdquo; on JD.com</li>
<li><strong>Scoring method</strong>: Correspondingly, the authors changed the scoring function to a weighted sum of $m$ dot products between $m$ query embeddings and 1 item embedding and the loss function to attention loss</li>
</ul>
<p>The new architecture, Deep Personalized and Semantic Retrieval (DPSR), led to a +10.03% CVR gain in long-tail queries and has been in production at JD since 2019.</p>
<h4 id="amazon-product-search-kdd-2019httpswwwamazonsciencepublicationssemantic-product-search" class="scroll-mt-8 group">
  Amazon Product Search (<a href="https://www.amazon.science/publications/semantic-product-search">KDD 2019</a>)
  
    <a href="#amazon-product-search-kdd-2019httpswwwamazonsciencepublicationssemantic-product-search"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h4>
<blockquote>
<p><span style="background-color: #ff9900"><b>Highlight:</b></span> Amazon&rsquo;s two-tower model couldn&rsquo;t distinguish between positive vs. negative items, until the authors separated out impressed but not purchased items</p>
</blockquote>
<figure><img src="https://www.dropbox.com/scl/fi/7f8euj8cbalwzlq216ef5/Screenshot-2024-09-02-at-8.15.12-PM.png?rlkey=z09lqbphotiwg2ffhrqj5yjyw&amp;st=2zpvqvpz&amp;raw=1" width="1500">
</figure>

<h2 id="learn-more" class="scroll-mt-8 group">
  Learn More
  
    <a href="#learn-more"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h2>
<h3 id="papers" class="scroll-mt-8 group">
  Papers
  
    <a href="#papers"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<ol>
<li><a href="https://research.google/pubs/deep-neural-networks-for-youtube-recommendations/"><em>Deep Neural Networks for YouTube Recommendations</em></a> (2016) by Covington, Adams, and Sargin, <em>RecSys</em>.</li>
<li><a href="https://research.google/pubs/sampling-bias-corrected-neural-modeling-for-large-corpus-item-recommendations/"><em>Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations</em></a> (2019) by Yi et al., <em>RecSys</em>.</li>
<li><a href="https://research.google/pubs/mixed-negative-sampling-for-learning-two-tower-neural-networks-in-recommendations/"><em>Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations</em></a> (2020) by Yang et al., <em>WWW</em>.</li>
<li><a href="https://arxiv.org/abs/2206.00212"><em>Negative Sampling for Contrastive Representation Learning: A Review</em></a> (2022) by Xu et al., <em>arXiv</em>.</li>
<li><a href="https://arxiv.org/abs/1412.2007"><em>On Using Very Large Target Vocabulary for Neural Machine Translation</em></a> (2014) by Jean, Cho, Memisevic, and Bengio, <em>arXiv</em>.</li>
<li><a href="https://aclanthology.org/W16-1617.pdf"><em>Learning Text Similarity with Siamese Recurrent Networks</em></a> (2016) by Neculoiu, Versteegh, and Rotaru, <em>Rep4NLP@ACL</em>.</li>
<li><a href="https://arxiv.org/abs/1904.03626"><em>On The Power of Curriculum Learning in Training Deep Networks</em></a> (2019) by Hacohen and Weinshall, <em>ICML</em>.</li>
<li><a href="https://arxiv.org/html/2402.17238v1"><em>Does Negative Sampling Matter? A Review with Insights into its Theory and Applications</em></a> (2024) by Yang et al., <em>PAMI</em>.</li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403305"><em>Embedding-based Retrieval in Facebook Search</em></a> (2020) by Huang et al., <em>KDD</em>.</li>
<li><a href="https://arxiv.org/abs/2402.13435"><em>Learning to Retrieve for Job Matching</em></a> (2024) by Shen et al., <em>KDD</em>.</li>
<li><a href="https://www.amazon.science/publications/semantic-product-search"><em>Semantic Product Search</em></a> (2019) by Nigam et al., <em>KDD</em>.</li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401446"><em>Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning</em></a> (2020) by Zhang et al., <em>SIGIR</em>.</li>
</ol>
<h3 id="blogposts--repos" class="scroll-mt-8 group">
  Blogposts + Repos
  
    <a href="#blogposts--repos"
        class="no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block"
        aria-hidden="true" title="Link to this heading" tabindex="-1">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  width="16"
  height="16"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-link w-4 h-4 block"
  viewBox="0 0 24 24"
>
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" />
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" />
</svg>

    </a>
  
</h3>
<ol>
<li><a href="https://www.tensorflow.org/extras/candidate_sampling.pdf"><em>What is Candidate Sampling</em></a>, tutorial by TensorFlow</li>
<li><a href="https://github.com/RUCAIBox/Negative-Sampling-Paper#curriculum-learning"><em>Negative-Sampling-Paper</em></a>, SOTA paper collection by <a href="https://github.com/RUCAIBox">RUCAIBox</a> 👉 more on recommender systems: <a href="https://github.com/RUCAIBox/Awesome-RSPapers">Awesome-RSPapers</a> and <a href="https://github.com/RUCAIBox/RecSysDatasets">RecSysDatasets</a></li>
<li><a href="https://www.yuan-meng.com/posts/ebr/"><em>An Introduction to Embedding-Based Retrieval</em></a>, blogpost by Yuan</li>
</ol>
    </div>
  </article>

  
    <aside class="not-prose flex flex-col space-y-8 border-t pt-6">
    <section class="flex flex-col space-y-4">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-shapes h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="M8.3 10a.7.7 0 0 1-.626-1.079L11.4 3a.7.7 0 0 1 1.198-.043L16.3 8.9a.7.7 0 0 1-.572 1.1Z"
  />
  <rect width="7" height="7" x="3" y="14" rx="1" />
  <circle cx="17.5" cy="17.5" r="3.5" />
</svg>

        <span>Categories</span>
      </h2>

      <ul class="ml-6 flex flex-row flex-wrap items-center space-x-2">
          <li>
            <a href="/categories/negative-sampling/" class="taxonomy category">negative sampling</a>
          </li>
          <li>
            <a href="/categories/recommender-system/" class="taxonomy category">recommender system</a>
          </li>
      </ul>
    </section>
    <section class="flex flex-col space-y-4" aria-hidden="true">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-chart-network h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="m13.11 7.664 1.78 2.672M14.162 12.788l-3.324 1.424M20 4l-6.06 1.515M3 3v16a2 2 0 0 0 2 2h16"
  />
  <circle cx="12" cy="6" r="2" />
  <circle cx="16" cy="12" r="2" />
  <circle cx="9" cy="15" r="2" />
</svg>

        <span>Graph</span>
      </h2>

      <content-network-graph
  class="h-64 ml-6"
  data-endpoint="/graph/index.json"
  page="/posts/negative_sampling/"
></content-network-graph>

    </section>
    <section class="flex flex-col space-y-4">
      <h2 class="flex flex-row items-center space-x-2 text-lg font-semibold">
        <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-newspaper h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path
    d="M4 22h16a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v16a2 2 0 0 1-2 2Zm0 0a2 2 0 0 1-2-2v-9c0-1.1.9-2 2-2h2M18 14h-8M15 18h-5"
  />
  <path d="M10 6h8v4h-8V6Z" />
</svg>

        <span>Posts</span>
      </h2>
        <section class="flex flex-col space-y-1">
          <h3 class="flex flex-row items-center space-x-2 text-sm font-semibold">
            <svg
  xmlns="http://www.w3.org/2000/svg"
  fill="none"
  stroke="currentColor"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  class="lucide lucide-arrow-down-to-dot h-4 w-4"
  viewBox="0 0 24 24"
  aria-hidden="true"
>
  <path d="M12 2v14M19 9l-7 7-7-7" />
  <circle cx="12" cy="21" r="1" />
</svg>

            <span>Incoming</span>
          </h3>

          <ol class="not-prose ml-6">
    <li>
      <article class="flex flex-row items-center">
        <header class="grow">
          <h3>
            <a
              href="/posts/seq_user_modeling/"
              class="truncate text-sm underline decoration-slate-300 decoration-2 underline-offset-4 hover:decoration-inherit"
              title="Down the Rabbit Hole: Sequential User Modeling"
              >Down the Rabbit Hole: Sequential User Modeling</a
            >
          </h3>
        </header>
          <ul class="flex flex-row items-center justify-end space-x-2">
              <li>
                <a
                  href="/categories/recommender-systems/"
                  class="taxonomy"
                  title="Posts and notes on Recommender systems"
                  >Recommender systems</a
                >
              </li>
              <li>
                <a
                  href="/categories/information-retrieval/"
                  class="taxonomy"
                  title="Posts and notes on Information retrieval"
                  >Information retrieval</a
                >
              </li>
          </ul>
      </article>
    </li>
</ol>

        </section>
    </section>
</aside>

      </main>
      <footer class="mt-20 border-t border-neutral-100 pt-2 text-xs">
        
<section class="items-top flex flex-row justify-between opacity-70">
  <div class="flex flex-col space-y-2">
      <p>Copyright &copy; 2024, Yuan Meng.</p>
      <div
        xmlns:cc="https://creativecommons.org/ns#"
        xmlns:dct="http://purl.org/dc/terms/"
        about="https://creativecommons.org"
      >
        Content is available under
        <a href="https://creativecommons.org/licenses/by-sa/4.0/" rel="license" class="inline-block" title="Creative Commons Attribution-ShareAlike 4.0 International"
          >CC BY-SA 4.0</a
        >
        unless otherwise noted.
      </div>
        <div
          class="mt-2 flex items-center space-x-2 fill-slate-400 hover:fill-slate-600 motion-safe:transition-colors"
        >
          <div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
  aria-hidden="true"
>
  <title>Creative Commons</title>
  <circle fill="transparent" cx="37.785" cy="28.501" r="28.836" />
  <path
    d="M37.441-3.5c8.951 0 16.572 3.125 22.857 9.372 3.008 3.009 5.295 6.448 6.857 10.314 1.561 3.867 2.344 7.971 2.344 12.314 0 4.381-.773 8.486-2.314 12.313-1.543 3.828-3.82 7.21-6.828 10.143-3.123 3.085-6.666 5.448-10.629 7.086-3.961 1.638-8.057 2.457-12.285 2.457s-8.276-.808-12.143-2.429c-3.866-1.618-7.333-3.961-10.4-7.027-3.067-3.066-5.4-6.524-7-10.372S5.5 32.767 5.5 28.5c0-4.229.809-8.295 2.428-12.2 1.619-3.905 3.972-7.4 7.057-10.486C21.08-.394 28.565-3.5 37.441-3.5zm.116 5.772c-7.314 0-13.467 2.553-18.458 7.657-2.515 2.553-4.448 5.419-5.8 8.6a25.204 25.204 0 0 0-2.029 9.972c0 3.429.675 6.734 2.029 9.913 1.353 3.183 3.285 6.021 5.8 8.516 2.514 2.496 5.351 4.399 8.515 5.715a25.652 25.652 0 0 0 9.943 1.971c3.428 0 6.75-.665 9.973-1.999 3.219-1.335 6.121-3.257 8.713-5.771 4.99-4.876 7.484-10.99 7.484-18.344 0-3.543-.648-6.895-1.943-10.057-1.293-3.162-3.18-5.98-5.654-8.458-5.146-5.143-11.335-7.715-18.573-7.715zm-.401 20.915-4.287 2.229c-.458-.951-1.019-1.619-1.685-2-.667-.38-1.286-.571-1.858-.571-2.856 0-4.286 1.885-4.286 5.657 0 1.714.362 3.084 1.085 4.113.724 1.029 1.791 1.544 3.201 1.544 1.867 0 3.181-.915 3.944-2.743l3.942 2c-.838 1.563-2 2.791-3.486 3.686-1.484.896-3.123 1.343-4.914 1.343-2.857 0-5.163-.875-6.915-2.629-1.752-1.752-2.628-4.19-2.628-7.313 0-3.048.886-5.466 2.657-7.257 1.771-1.79 4.009-2.686 6.715-2.686 3.963-.002 6.8 1.541 8.515 4.627zm18.457 0-4.229 2.229c-.457-.951-1.02-1.619-1.686-2-.668-.38-1.307-.571-1.914-.571-2.857 0-4.287 1.885-4.287 5.657 0 1.714.363 3.084 1.086 4.113.723 1.029 1.789 1.544 3.201 1.544 1.865 0 3.18-.915 3.941-2.743l4 2c-.875 1.563-2.057 2.791-3.541 3.686a9.233 9.233 0 0 1-4.857 1.343c-2.896 0-5.209-.875-6.941-2.629-1.736-1.752-2.602-4.19-2.602-7.313 0-3.048.885-5.466 2.658-7.257 1.77-1.79 4.008-2.686 6.713-2.686 3.962-.002 6.783 1.541 8.458 4.627z"
  />
</svg>
</div><div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
>
  <title>Credit must be given to the creator</title>
  <circle fill="transparent" cx="37.637" cy="28.806" r="28.276" />
  <path
    d="M37.443-3.5c8.988 0 16.57 3.085 22.742 9.257C66.393 11.967 69.5 19.548 69.5 28.5c0 8.991-3.049 16.476-9.145 22.456-6.476 6.363-14.113 9.544-22.912 9.544-8.649 0-16.153-3.144-22.514-9.43C8.644 44.784 5.5 37.262 5.5 28.5c0-8.761 3.144-16.342 9.429-22.742C21.101-.415 28.604-3.5 37.443-3.5zm.114 5.772c-7.276 0-13.428 2.553-18.457 7.657-5.22 5.334-7.829 11.525-7.829 18.572 0 7.086 2.59 13.22 7.77 18.398 5.181 5.182 11.352 7.771 18.514 7.771 7.123 0 13.334-2.607 18.629-7.828 5.029-4.838 7.543-10.952 7.543-18.343 0-7.276-2.553-13.465-7.656-18.571-5.104-5.104-11.276-7.656-18.514-7.656zm8.572 18.285v13.085h-3.656v15.542h-9.944V33.643h-3.656V20.557c0-.572.2-1.057.599-1.457.401-.399.887-.6 1.457-.6h13.144c.533 0 1.01.2 1.428.6.417.4.628.886.628 1.457zm-13.087-8.228c0-3.008 1.485-4.514 4.458-4.514s4.457 1.504 4.457 4.514c0 2.971-1.486 4.457-4.457 4.457s-4.458-1.486-4.458-4.457z"
  />
</svg>
</div><div class="flex-none cursor-help"><svg
  version="1.0"
  xmlns="http://www.w3.org/2000/svg"
  viewBox="5.5 -3.5 64 64"
  xml:space="preserve"
  class="w-5 h-5 block"
>
  <title>Adaptations must be shared under the same terms</title>
  <circle fill="transparent" cx="36.944" cy="28.631" r="29.105" />
  <path
    d="M37.443-3.5c8.951 0 16.531 3.105 22.742 9.315C66.393 11.987 69.5 19.548 69.5 28.5c0 8.954-3.049 16.457-9.145 22.514-6.437 6.324-14.076 9.486-22.912 9.486-8.649 0-16.153-3.143-22.514-9.429C8.644 44.786 5.5 37.264 5.5 28.501c0-8.723 3.144-16.285 9.429-22.685C21.138-.395 28.643-3.5 37.443-3.5zm.114 5.772c-7.276 0-13.428 2.572-18.457 7.715-5.22 5.296-7.829 11.467-7.829 18.513 0 7.125 2.59 13.257 7.77 18.4 5.181 5.182 11.352 7.771 18.514 7.771 7.123 0 13.334-2.609 18.629-7.828 5.029-4.876 7.543-10.99 7.543-18.343 0-7.313-2.553-13.485-7.656-18.513-5.067-5.145-11.239-7.715-18.514-7.715zM23.271 23.985c.609-3.924 2.189-6.962 4.742-9.114 2.552-2.152 5.656-3.228 9.314-3.228 5.027 0 9.029 1.62 12 4.856 2.971 3.238 4.457 7.391 4.457 12.457 0 4.915-1.543 9-4.627 12.256-3.088 3.256-7.086 4.886-12.002 4.886-3.619 0-6.743-1.085-9.371-3.257-2.629-2.172-4.209-5.257-4.743-9.257H31.1c.19 3.886 2.533 5.829 7.029 5.829 2.246 0 4.057-.972 5.428-2.914 1.373-1.942 2.059-4.534 2.059-7.771 0-3.391-.629-5.971-1.885-7.743-1.258-1.771-3.066-2.657-5.43-2.657-4.268 0-6.667 1.885-7.2 5.656h2.343l-6.342 6.343-6.343-6.343 2.512.001z"
  />
</svg>
</div>
        </div>

  </div>
    <div>
      <a
        href="https://github.com/michenriksen/hugo-theme-til"
        title="Today I Learned &#8212; A Hugo theme by Michael Henriksen"
        data-theme-version="0.4.0"
        >theme: til</a
      >
    </div>
</section>

      </footer>
    </div>
    
  </body>
</html>
