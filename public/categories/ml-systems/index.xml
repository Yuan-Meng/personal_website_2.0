<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml systems on Yuan Meng</title>
    <link>http://localhost:1313/categories/ml-systems/</link>
    <description>Recent content in Ml systems on Yuan Meng</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>mycaptainmy@gmail.com (Yuan Meng)</managingEditor>
    <webMaster>mycaptainmy@gmail.com (Yuan Meng)</webMaster>
    <copyright>Yuan Meng</copyright>
    <lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/ml-systems/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hardware-Aware Attention for Long Sequence Modeling</title>
      <link>http://localhost:1313/posts/hardware_aware_transformers/</link>
      <pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/hardware_aware_transformers/</guid>
      <description>&lt;h2 id=&#34;attention-is-all-you-need-----if-you-can-afford-the-on2-complexity&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Attention Is All You Need &amp;mdash; if You Can Afford the $O(N^2)$ Complexity&#xA;  &#xA;    &lt;a href=&#34;#attention-is-all-you-need-----if-you-can-afford-the-on2-complexity&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Attention is key to the success of modern large language models (LLMs), which overcomes RNNs&amp;rsquo; difficulty in modeling long-range dependencies by attending to every token in the input sequence at once, without suffering from vanishing or exploding gradients.&lt;/p&gt;&#xA;&lt;p&gt;With the power to &amp;ldquo;see all&amp;rdquo; comes a time complexity of &lt;span class=&#34;sidenote&#34;&gt;&#xA;  &lt;input&#xA;    aria-label=&#34;Show sidenote&#34;&#xA;    type=&#34;checkbox&#34;&#xA;    id=&#34;sidenote-checkbox-01&#34;&#xA;    class=&#34;sidenote-checkbox hidden&#34;&#xA;  /&gt;&#xA;  &lt;label&#xA;    tabindex=&#34;0&#34;&#xA;    role=&#34;mark&#34;&#xA;    aria-details=&#34;sidenote-01&#34;&#xA;    for=&#34;sidenote-checkbox-01&#34;&#xA;    class=&#34;sidenote-mark&#34;&#xA;    &gt;$O(N^2d)$&lt;/label&#xA;  &gt;&#xA;  &lt;small id=&#34;sidenote-01&#34; class=&#34;sidenote-content&#34;&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt; (sidenote: &lt;/span&gt;Here&#39;s the trick for counting matrix multiplication complexity: The outer dimensions tell us how many inner products are performed. In step 3, for instance, we got $N \times N$. The inner dimension tells us the complexity of each inner product. In step 3, it&#39;s $2d - 1$ = $d$ (element-wise products) + $(d-1)$ (additions). The total complexity of step 3 is therefore $O(N^2d)$. Turns out steps 3 and 5 are equally expensive, dominating the final time complexity.&lt;span class=&#34;sr-only&#34;&gt;)&lt;/span&gt;&#xA;  &lt;/small&gt;&#xA;&lt;/span&gt;&#xA; and a space complexity of $O(N^2)$, where $N$ is the length of the input sequence (# tokens) and $d$ the hidden embedding dimension. Such complexity makes vanilla attention hard to scale with input length ðŸ’€.&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://www.dropbox.com/scl/fi/m8vdwmpqwt40c896ty24v/Screenshot-2025-03-15-at-11.37.40-PM.png?rlkey=t6852oqzse600dc48gjg7rfal&amp;amp;st=r3h14cla&amp;amp;raw=1&#34;&#xA;    alt=&#34;In vanilla attention, writing $\mathbf{S}$, $\mathbf{A}$, and $\mathbf{O}$ to memory has an $O(N^2)$ IO complexity.&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;p&gt;In vanilla attention, writing $\mathbf{S}$, $\mathbf{A}$, and $\mathbf{O}$ to memory has an $O(N^2)$ IO complexity.&lt;/p&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;</description>
    </item>
  </channel>
</rss>
