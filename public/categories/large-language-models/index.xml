<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large language models on Yuan Meng</title>
    <link>http://localhost:1313/categories/large-language-models/</link>
    <description>Recent content in Large language models on Yuan Meng</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>mycaptainmy@gmail.com (Yuan Meng)</managingEditor>
    <webMaster>mycaptainmy@gmail.com (Yuan Meng)</webMaster>
    <copyright>Yuan Meng</copyright>
    <lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/large-language-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race</title>
      <link>http://localhost:1313/posts/rlhf/</link>
      <pubDate>Sun, 05 Oct 2025 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/rlhf/</guid>
      <description>&lt;p&gt;Coming soon in October&amp;hellip;&lt;/p&gt;&#xA;&lt;h2 id=&#34;references&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  References&#xA;  &#xA;    &lt;a href=&#34;#references&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;stable-literature-10&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Stable Literature 1.0&#xA;  &#xA;    &lt;a href=&#34;#stable-literature-10&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Nathan Lambert&amp;rsquo;s &lt;a href=&#34;https://rlhfbook.com/&#34;&gt;RLHF book&lt;/a&gt;, a valiant attempt to capture a &amp;ldquo;stable literature&amp;rdquo; in an evolving LLM post-training battlefield&lt;/li&gt;&#xA;&lt;li&gt;Kevin Murphy&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2412.05265&#34;&gt;Reinforcement Learning: An Overview&lt;/a&gt; ðŸ‘‰ Chapter 6 talks about RL &amp;amp; LLM&lt;/li&gt;&#xA;&lt;li&gt;Sutton and Barto&amp;rsquo;s &lt;a href=&#34;https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf&#34;&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt; ðŸ‘‰ the OG RL book updated with new stuff&lt;/li&gt;&#xA;&lt;li&gt;Norvig and Russell&amp;rsquo;s &lt;a href=&#34;https://aima.cs.berkeley.edu/&#34;&gt;Artificial Intelligence: A Modern Approach&lt;/a&gt; ðŸ‘‰ Berkeley CS 188 textbook &amp;amp; the most popular one in the world &amp;mdash; an overview of intelligent agents with a heavy emphasis on RL&lt;/li&gt;&#xA;&lt;li&gt;Meta&amp;rsquo;s comprehensive lit review on RL algorithms for LLMs ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.04501&#34;&gt;&lt;em&gt;Understanding Reinforcement Learning for Model Training, and future directions with GRAPE&lt;/em&gt;&lt;/a&gt; (2025) by Patel, &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Extensive review on reasoning models ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.08827&#34;&gt;&lt;em&gt;A Survey of Reinforcement Learning for Large Reasoning Models&lt;/em&gt;&lt;/a&gt; (2025) by Zhang et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;evolving-literature&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Evolving Literature&#xA;  &#xA;    &lt;a href=&#34;#evolving-literature&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;rlhf-for-language-models&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  RLHF for Language Models&#xA;  &#xA;    &lt;a href=&#34;#rlhf-for-language-models&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h4&gt;&#xA;&lt;ol start=&#34;7&#34;&gt;&#xA;&lt;li&gt;The hottest AI startup Thinking Machines sells post-training as a service ðŸ‘‰ product: &lt;a href=&#34;https://thinkingmachines.ai/&#34;&gt;Tinker&lt;/a&gt;; blog: &lt;a href=&#34;https://thinkingmachines.ai/blog/lora/&#34;&gt;LoRA Without Regret&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Cursor&amp;rsquo;s blogpost on their wickedly effective RL models ðŸ‘‰ &lt;a href=&#34;https://cursor.com/en-US/blog/tab-rl&#34;&gt;&lt;em&gt;Improving Cursor Tab with online RL&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Nathan Lambert&amp;rsquo;s blogpost &lt;a href=&#34;https://www.interconnects.ai/&#34;&gt;&lt;em&gt;Interconnects&lt;/em&gt;&lt;/a&gt; ðŸ‘‰ frequently writes about reasoning models, agents, post-training, etc.&lt;/li&gt;&#xA;&lt;li&gt;Hugging Face&amp;rsquo;s post-training course &lt;a href=&#34;https://huggingface.co/learn/smol-course/en/unit0/1&#34;&gt;smol-course&lt;/a&gt; ðŸ‘‰ hands-on course on language model fine-tuning basics&lt;/li&gt;&#xA;&lt;li&gt;Fei-Fei Li and team&amp;rsquo;s AI agent paper ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2401.03568&#34;&gt;&lt;em&gt;Agent AI: Surveying the Horizons of Multimodal Interaction&lt;/em&gt;&lt;/a&gt; (2024) by Durante et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;NVIDIA uses RL in pretraining ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2510.01265&#34;&gt;&lt;em&gt;RLP: Reinforcement as a Pretraining Objective&lt;/em&gt;&lt;/a&gt; (2025) by Hatamizadeh et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Meta&amp;rsquo;s new self-play training strategy for Llama ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.07414&#34;&gt;&lt;em&gt;Language Self-Play For Data-Free Training&lt;/em&gt;&lt;/a&gt; (2025) by Kuba et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;rlfh-for-recsys&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  RLFH for RecSys&#xA;  &#xA;    &lt;a href=&#34;#rlfh-for-recsys&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h4&gt;&#xA;&lt;ol start=&#34;14&#34;&gt;&#xA;&lt;li&gt;OneRec-V2 improves &lt;a href=&#34;http://localhost:1313/posts/generative_recommendation/&#34; class=&#34;backlink&#34;&gt;Generative Recommendation&lt;/a&gt;&#xA;  &#xA;   post-training ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2508.20900&#34;&gt;&lt;em&gt;OneRec-V2 Technical Report&lt;/em&gt;&lt;/a&gt; (2025) by Zhou et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Shopee&amp;rsquo;s OnePiece integrates RL with traditional cascade RecSys ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.18091&#34;&gt;&lt;em&gt;OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System&lt;/em&gt;&lt;/a&gt; (2025) by Dai et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Douyin improves user understanding with reasoning ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.18864&#34;&gt;&lt;em&gt;Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling&lt;/em&gt;&lt;/a&gt; (2025) by Li et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Pinterest uses RL to tune value models of multi-objective rankers ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.05292&#34;&gt;&lt;em&gt;Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest&lt;/em&gt;&lt;/a&gt; (2025) by Xiao et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Is Generative Recommendation the ChatGPT Moment of RecSys?</title>
      <link>http://localhost:1313/posts/generative_recommendation/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/generative_recommendation/</guid>
      <description>&lt;h2 id=&#34;has-the-tide-turned-from-dlrm-to-gr&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Has the Tide Turned? From DLRM to GR&#xA;  &#xA;    &lt;a href=&#34;#has-the-tide-turned-from-dlrm-to-gr&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;For nearly a decade, recommender systems have remained largely &lt;span class=&#34;sidenote&#34;&gt;&#xA;  &lt;input&#xA;    aria-label=&#34;Show sidenote&#34;&#xA;    type=&#34;checkbox&#34;&#xA;    id=&#34;sidenote-checkbox-01&#34;&#xA;    class=&#34;sidenote-checkbox hidden&#34;&#xA;  /&gt;&#xA;  &lt;label&#xA;    tabindex=&#34;0&#34;&#xA;    role=&#34;mark&#34;&#xA;    aria-details=&#34;sidenote-01&#34;&#xA;    for=&#34;sidenote-checkbox-01&#34;&#xA;    class=&#34;sidenote-mark&#34;&#xA;    &gt;the same&lt;/label&#xA;  &gt;&#xA;  &lt;small id=&#34;sidenote-01&#34; class=&#34;sidenote-content&#34;&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt; (sidenote: &lt;/span&gt;It used to be (still is?) the case that if you&#39;re familiar with the cascade pipeline and the most popular L1 (e.g., two-tower models and embedding-based retrieval) and L2 (e.g., &#34;Embedding-MLP&#34; style `pAction` models, sequence modeling) architectures, you&#39;re golden in almost every ML system design interview. Perhaps a year from now, GenRec talents and experience will be what top companies seek instead.&lt;span class=&#34;sr-only&#34;&gt;)&lt;/span&gt;&#xA;  &lt;/small&gt;&#xA;&lt;/span&gt;&#xA;. It&amp;rsquo;s hard to even imagine a system without a cascade pipeline in the iconic &lt;a href=&#34;https://research.google.com/pubs/archive/45530.pdf&#34;&gt;YouTube paper&lt;/a&gt;, which retrieves tens of thousands of candidates from a massive corpus, trims them down to hundreds of relevant items using a lightweight ranker (L1), selects the top dozen using a heavy ranker (L2), and makes adjustments based on policy and business logic (L3). Architecture-wise, the L2 ranker hasn&amp;rsquo;t drifted far from the seminal &lt;a href=&#34;https://arxiv.org/abs/1606.07792&#34;&gt;Deep &amp;amp; Wide network&lt;/a&gt;, which embeds input features, passes them through interaction modules, and transforms representations for task heads (e.g., clicks, purchase, video watch). Upgrades to feature interaction (e.g., &lt;a href=&#34;https://arxiv.org/abs/2008.13535&#34;&gt;DCN-v2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2102.07619&#34;&gt;MaskNet&lt;/a&gt;) and multi-task learning (e.g., &lt;a href=&#34;https://arxiv.org/abs/2311.09580&#34;&gt;MMoE&lt;/a&gt;, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3383313.3412236&#34;&gt;PLE&lt;/a&gt;) culminated in Meta&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2203.11014&#34;&gt;DHEN&lt;/a&gt;, which combines multiple interaction modules and experts to push the limits of this &amp;ldquo;Deep Learning Recommender System&amp;rdquo; (DLRM) paradigm.&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://www.dropbox.com/scl/fi/96m8zb5yps9ffz9geheu7/Screenshot-2025-07-20-at-11.07.10-PM.png?rlkey=q4xtbxt3r50okrs2zo9vac2xq&amp;amp;st=fzobjxgt&amp;amp;raw=1&#34;&#xA;    alt=&#34;Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style &amp;lsquo;Embedding &amp;amp; Interaction &amp;amp; Expert&amp;rsquo; model architectures.&#34; width=&#34;1800&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;p&gt;Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style &amp;lsquo;Embedding &amp;amp; Interaction &amp;amp; Expert&amp;rsquo; model architectures.&lt;/p&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;In 2025, the tide seems to have finally turned after Meta&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2402.17152&#34;&gt;HSTU&lt;/a&gt; delivered perhaps the biggest offline/online metric and serving efficiency gains in recent years &amp;mdash; other top companies such as &lt;span class=&#34;sidenote&#34;&gt;&#xA;  &lt;input&#xA;    aria-label=&#34;Show sidenote&#34;&#xA;    type=&#34;checkbox&#34;&#xA;    id=&#34;sidenote-checkbox-03&#34;&#xA;    class=&#34;sidenote-checkbox hidden&#34;&#xA;  /&gt;&#xA;  &lt;label&#xA;    tabindex=&#34;0&#34;&#xA;    role=&#34;mark&#34;&#xA;    aria-details=&#34;sidenote-03&#34;&#xA;    for=&#34;sidenote-checkbox-03&#34;&#xA;    class=&#34;sidenote-mark&#34;&#xA;    &gt;Google&lt;/label&#xA;  &gt;&#xA;  &lt;small id=&#34;sidenote-03&#34; class=&#34;sidenote-content&#34;&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt; (sidenote: &lt;/span&gt;Google DeepMind published TIGER a year before HSTU, but it was used for retrieval only. Meta might have been the major influence behind using Generative Recommendation for both retrieval and ranking.&lt;span class=&#34;sr-only&#34;&gt;)&lt;/span&gt;&#xA;  &lt;/small&gt;&#xA;&lt;/span&gt;&#xA;, Kuaishou, Meituan, Alibaba, Netflix, Xiaohongshu, ByteDance, Tencent, Baidu, and JD.com are starting to embrace a new &amp;ldquo;Generative Recommendation&amp;rdquo; (GR) paradigm for retrieval and ranking, reframing the discriminative &lt;code&gt;pAction&lt;/code&gt; prediction task as a generative task, akin to token predictions in language modeling.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
