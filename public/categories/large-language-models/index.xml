<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large language models on Yuan Meng</title>
    <link>http://localhost:1313/categories/large-language-models/</link>
    <description>Recent content in Large language models on Yuan Meng</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>mycaptainmy@gmail.com (Yuan Meng)</managingEditor>
    <webMaster>mycaptainmy@gmail.com (Yuan Meng)</webMaster>
    <copyright>Yuan Meng</copyright>
    <lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/large-language-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race</title>
      <link>http://localhost:1313/posts/rlhf/</link>
      <pubDate>Sun, 05 Oct 2025 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/rlhf/</guid>
      <description>&lt;p&gt;Coming soon in October&amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Is Generative Recommendation the ChatGPT Moment of RecSys?</title>
      <link>http://localhost:1313/posts/generative_recommendation/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/generative_recommendation/</guid>
      <description>&lt;h2 id=&#34;has-the-tide-turned-from-dlrm-to-gr&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Has the Tide Turned? From DLRM to GR&#xA;  &#xA;    &lt;a href=&#34;#has-the-tide-turned-from-dlrm-to-gr&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;For nearly a decade, recommender systems have remained largely &lt;span class=&#34;sidenote&#34;&gt;&#xA;  &lt;input&#xA;    aria-label=&#34;Show sidenote&#34;&#xA;    type=&#34;checkbox&#34;&#xA;    id=&#34;sidenote-checkbox-01&#34;&#xA;    class=&#34;sidenote-checkbox hidden&#34;&#xA;  /&gt;&#xA;  &lt;label&#xA;    tabindex=&#34;0&#34;&#xA;    role=&#34;mark&#34;&#xA;    aria-details=&#34;sidenote-01&#34;&#xA;    for=&#34;sidenote-checkbox-01&#34;&#xA;    class=&#34;sidenote-mark&#34;&#xA;    &gt;the same&lt;/label&#xA;  &gt;&#xA;  &lt;small id=&#34;sidenote-01&#34; class=&#34;sidenote-content&#34;&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt; (sidenote: &lt;/span&gt;It used to be (still is?) the case that if you&#39;re familiar with the cascade pipeline and the most popular L1 (e.g., two-tower models and embedding-based retrieval) and L2 (e.g., &#34;Embedding-MLP&#34; style `pAction` models, sequence modeling) architectures, you&#39;re golden in almost every ML system design interview. Perhaps a year from now, GenRec talents and experience will be what top companies seek instead.&lt;span class=&#34;sr-only&#34;&gt;)&lt;/span&gt;&#xA;  &lt;/small&gt;&#xA;&lt;/span&gt;&#xA;. It&amp;rsquo;s hard to even imagine a system without a cascade pipeline in the iconic &lt;a href=&#34;https://research.google.com/pubs/archive/45530.pdf&#34;&gt;YouTube paper&lt;/a&gt;, which retrieves tens of thousands of candidates from a massive corpus, trims them down to hundreds of relevant items using a lightweight ranker (L1), selects the top dozen using a heavy ranker (L2), and makes adjustments based on policy and business logic (L3). Architecture-wise, the L2 ranker hasn&amp;rsquo;t drifted far from the seminal &lt;a href=&#34;https://arxiv.org/abs/1606.07792&#34;&gt;Deep &amp;amp; Wide network&lt;/a&gt;, which embeds input features, passes them through interaction modules, and transforms representations for task heads (e.g., clicks, purchase, video watch). Upgrades to feature interaction (e.g., &lt;a href=&#34;https://arxiv.org/abs/2008.13535&#34;&gt;DCN-v2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2102.07619&#34;&gt;MaskNet&lt;/a&gt;) and multi-task learning (e.g., &lt;a href=&#34;https://arxiv.org/abs/2311.09580&#34;&gt;MMoE&lt;/a&gt;, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3383313.3412236&#34;&gt;PLE&lt;/a&gt;) culminated in Meta&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2203.11014&#34;&gt;DHEN&lt;/a&gt;, which combines multiple interaction modules and experts to push the limits of this &amp;ldquo;Deep Learning Recommender System&amp;rdquo; (DLRM) paradigm.&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://www.dropbox.com/scl/fi/96m8zb5yps9ffz9geheu7/Screenshot-2025-07-20-at-11.07.10-PM.png?rlkey=q4xtbxt3r50okrs2zo9vac2xq&amp;amp;st=fzobjxgt&amp;amp;raw=1&#34;&#xA;    alt=&#34;Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style &amp;lsquo;Embedding &amp;amp; Interaction &amp;amp; Expert&amp;rsquo; model architectures.&#34; width=&#34;1800&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;p&gt;Since 2016, web-scale recommender systems mostly use the cascade pipeline and DLRM-style &amp;lsquo;Embedding &amp;amp; Interaction &amp;amp; Expert&amp;rsquo; model architectures.&lt;/p&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;In 2025, the tide seems to have finally turned after Meta&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2402.17152&#34;&gt;HSTU&lt;/a&gt; delivered perhaps the biggest offline/online metric and serving efficiency gains in recent years &amp;mdash; other top companies such as &lt;span class=&#34;sidenote&#34;&gt;&#xA;  &lt;input&#xA;    aria-label=&#34;Show sidenote&#34;&#xA;    type=&#34;checkbox&#34;&#xA;    id=&#34;sidenote-checkbox-03&#34;&#xA;    class=&#34;sidenote-checkbox hidden&#34;&#xA;  /&gt;&#xA;  &lt;label&#xA;    tabindex=&#34;0&#34;&#xA;    role=&#34;mark&#34;&#xA;    aria-details=&#34;sidenote-03&#34;&#xA;    for=&#34;sidenote-checkbox-03&#34;&#xA;    class=&#34;sidenote-mark&#34;&#xA;    &gt;Google&lt;/label&#xA;  &gt;&#xA;  &lt;small id=&#34;sidenote-03&#34; class=&#34;sidenote-content&#34;&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt; (sidenote: &lt;/span&gt;Google DeepMind published TIGER a year before HSTU, but it was used for retrieval only. Meta might have been the major influence behind using Generative Recommendation for both retrieval and ranking.&lt;span class=&#34;sr-only&#34;&gt;)&lt;/span&gt;&#xA;  &lt;/small&gt;&#xA;&lt;/span&gt;&#xA;, Kuaishou, Meituan, Alibaba, Netflix, Xiaohongshu, ByteDance, Tencent, Baidu, and JD.com are starting to embrace a new &amp;ldquo;Generative Recommendation&amp;rdquo; (GR) paradigm for retrieval and ranking, reframing the discriminative &lt;code&gt;pAction&lt;/code&gt; prediction task as a generative task, akin to token predictions in language modeling.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
