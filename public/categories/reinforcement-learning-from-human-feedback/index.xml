<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement learning from human feedback on Yuan Meng</title>
    <link>http://localhost:1313/categories/reinforcement-learning-from-human-feedback/</link>
    <description>Recent content in Reinforcement learning from human feedback on Yuan Meng</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>mycaptainmy@gmail.com (Yuan Meng)</managingEditor>
    <webMaster>mycaptainmy@gmail.com (Yuan Meng)</webMaster>
    <copyright>Yuan Meng</copyright>
    <lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/reinforcement-learning-from-human-feedback/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race</title>
      <link>http://localhost:1313/posts/rlhf/</link>
      <pubDate>Sun, 05 Oct 2025 00:00:00 +0000</pubDate><author>mycaptainmy@gmail.com (Yuan Meng)</author>
      <guid>http://localhost:1313/posts/rlhf/</guid>
      <description>&lt;p&gt;Coming soon in October&amp;hellip;&lt;/p&gt;&#xA;&lt;h2 id=&#34;references&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  References&#xA;  &#xA;    &lt;a href=&#34;#references&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;stable-literature-10&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Stable Literature 1.0&#xA;  &#xA;    &lt;a href=&#34;#stable-literature-10&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Nathan Lambert&amp;rsquo;s &lt;a href=&#34;https://rlhfbook.com/&#34;&gt;RLHF book&lt;/a&gt;, a valiant attempt to capture a &amp;ldquo;stable literature&amp;rdquo; in an evolving LLM post-training battlefield&lt;/li&gt;&#xA;&lt;li&gt;Kevin Murphy&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2412.05265&#34;&gt;Reinforcement Learning: An Overview&lt;/a&gt; ðŸ‘‰ Chapter 6 talks about RL &amp;amp; LLM&lt;/li&gt;&#xA;&lt;li&gt;Sutton and Barto&amp;rsquo;s &lt;a href=&#34;https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf&#34;&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt; ðŸ‘‰ the OG RL book updated with new stuff&lt;/li&gt;&#xA;&lt;li&gt;Norvig and Russell&amp;rsquo;s &lt;a href=&#34;https://aima.cs.berkeley.edu/&#34;&gt;Artificial Intelligence: A Modern Approach&lt;/a&gt; ðŸ‘‰ Berkeley CS 188 textbook &amp;amp; the most popular one in the world &amp;mdash; an overview of intelligent agents with a heavy emphasis on RL&lt;/li&gt;&#xA;&lt;li&gt;Meta&amp;rsquo;s comprehensive lit review on RL algorithms for LLMs ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.04501&#34;&gt;&lt;em&gt;Understanding Reinforcement Learning for Model Training, and future directions with GRAPE&lt;/em&gt;&lt;/a&gt; (2025) by Patel, &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Extensive review on reasoning models ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.08827&#34;&gt;&lt;em&gt;A Survey of Reinforcement Learning for Large Reasoning Models&lt;/em&gt;&lt;/a&gt; (2025) by Zhang et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;evolving-literature&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  Evolving Literature&#xA;  &#xA;    &lt;a href=&#34;#evolving-literature&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;rlhf-for-language-models&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  RLHF for Language Models&#xA;  &#xA;    &lt;a href=&#34;#rlhf-for-language-models&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h4&gt;&#xA;&lt;ol start=&#34;7&#34;&gt;&#xA;&lt;li&gt;The hottest AI startup Thinking Machines sells post-training as a service ðŸ‘‰ product: &lt;a href=&#34;https://thinkingmachines.ai/&#34;&gt;Tinker&lt;/a&gt;; blog: &lt;a href=&#34;https://thinkingmachines.ai/blog/lora/&#34;&gt;LoRA Without Regret&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Cursor&amp;rsquo;s blogpost on their wickedly effective RL models ðŸ‘‰ &lt;a href=&#34;https://cursor.com/en-US/blog/tab-rl&#34;&gt;&lt;em&gt;Improving Cursor Tab with online RL&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Nathan Lambert&amp;rsquo;s blogpost &lt;a href=&#34;https://www.interconnects.ai/&#34;&gt;&lt;em&gt;Interconnects&lt;/em&gt;&lt;/a&gt; ðŸ‘‰ frequently writes about reasoning models, agents, post-training, etc.&lt;/li&gt;&#xA;&lt;li&gt;Hugging Face&amp;rsquo;s post-training course &lt;a href=&#34;https://huggingface.co/learn/smol-course/en/unit0/1&#34;&gt;smol-course&lt;/a&gt; ðŸ‘‰ hands-on course on language model fine-tuning basics&lt;/li&gt;&#xA;&lt;li&gt;Fei-Fei Li and team&amp;rsquo;s AI agent paper ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2401.03568&#34;&gt;&lt;em&gt;Agent AI: Surveying the Horizons of Multimodal Interaction&lt;/em&gt;&lt;/a&gt; (2024) by Durante et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;NVIDIA uses RL in pretraining ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2510.01265&#34;&gt;&lt;em&gt;RLP: Reinforcement as a Pretraining Objective&lt;/em&gt;&lt;/a&gt; (2025) by Hatamizadeh et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Meta&amp;rsquo;s new self-play training strategy for Llama ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.07414&#34;&gt;&lt;em&gt;Language Self-Play For Data-Free Training&lt;/em&gt;&lt;/a&gt; (2025) by Kuba et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;rlfh-for-recsys&#34; class=&#34;scroll-mt-8 group&#34;&gt;&#xA;  RLFH for RecSys&#xA;  &#xA;    &lt;a href=&#34;#rlfh-for-recsys&#34;&#xA;        class=&#34;no-underline hidden opacity-50 hover:opacity-100 !text-inherit group-hover:inline-block&#34;&#xA;        aria-hidden=&#34;true&#34; title=&#34;Link to this heading&#34; tabindex=&#34;-1&#34;&gt;&#xA;        &lt;svg&#xA;  xmlns=&#34;http://www.w3.org/2000/svg&#34;&#xA;  width=&#34;16&#34;&#xA;  height=&#34;16&#34;&#xA;  fill=&#34;none&#34;&#xA;  stroke=&#34;currentColor&#34;&#xA;  stroke-linecap=&#34;round&#34;&#xA;  stroke-linejoin=&#34;round&#34;&#xA;  stroke-width=&#34;2&#34;&#xA;  class=&#34;lucide lucide-link w-4 h-4 block&#34;&#xA;  viewBox=&#34;0 0 24 24&#34;&#xA;&gt;&#xA;  &lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; /&gt;&#xA;  &lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; /&gt;&#xA;&lt;/svg&gt;&#xA;&#xA;    &lt;/a&gt;&#xA;  &#xA;&lt;/h4&gt;&#xA;&lt;ol start=&#34;14&#34;&gt;&#xA;&lt;li&gt;OneRec-V2 improves &lt;a href=&#34;http://localhost:1313/posts/generative_recommendation/&#34; class=&#34;backlink&#34;&gt;Generative Recommendation&lt;/a&gt;&#xA;  &#xA;   post-training ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2508.20900&#34;&gt;&lt;em&gt;OneRec-V2 Technical Report&lt;/em&gt;&lt;/a&gt; (2025) by Zhou et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Shopee&amp;rsquo;s OnePiece integrates RL with traditional cascade RecSys ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.18091&#34;&gt;&lt;em&gt;OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System&lt;/em&gt;&lt;/a&gt; (2025) by Dai et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Douyin improves user understanding with reasoning ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.18864&#34;&gt;&lt;em&gt;Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling&lt;/em&gt;&lt;/a&gt; (2025) by Li et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Pinterest uses RL to tune value models of multi-objective rankers ðŸ‘‰ &lt;a href=&#34;https://arxiv.org/abs/2509.05292&#34;&gt;&lt;em&gt;Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest&lt;/em&gt;&lt;/a&gt; (2025) by Xiao et al., &lt;em&gt;arXiv&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
