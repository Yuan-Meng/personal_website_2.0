---
title: "Elicit Hidden Powers: RLHF is the Aerodynamics for the LLM F1 Race"
date: 2025-10-05
math: true
categories: ["reinforcement learning from human feedback", "large language models", "recommender systems"]
toc: true
---

Coming soon in October...

<!--more-->

# References

## Stable Literature 1.0
1. Nathan Lambert's [RLHF book](https://rlhfbook.com/), a valiant attempt to capture a "stable literature" in an evolving LLM post-training battlefield 
2. Kevin Murphy's [Reinforcement Learning: An Overview](https://arxiv.org/abs/2412.05265) ðŸ‘‰ Chapter 6 talks about RL & LLM
3. Sutton and Barto's [Reinforcement Learning: An Introduction (2nd Edition)](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) ðŸ‘‰ the OG RL book updated with new stuff
4. Norvig and Russell's [Artificial Intelligence: A Modern Approach](https://aima.cs.berkeley.edu/) ðŸ‘‰ Berkeley CS 188 textbook & the most popular one in the world --- an overview of intelligent agents with a heavy emphasis on RL
5. Meta's comprehensive lit review on RL algorithms for LLMs ðŸ‘‰ [*Understanding Reinforcement Learning for Model Training, and future directions with GRAPE*](https://arxiv.org/abs/2509.04501) (2025) by Patel, *arXiv*.
6. Extensive review on reasoning models ðŸ‘‰ [*A Survey of Reinforcement Learning for Large Reasoning Models*](https://arxiv.org/abs/2509.08827) (2025) by Zhang et al., *arXiv*.

## Evolving Literature
### RLHF for Language Models
7. The hottest AI startup Thinking Machines sells post-training as a service ðŸ‘‰ product: [Tinker](https://thinkingmachines.ai/); blog: [LoRA Without Regret](https://thinkingmachines.ai/blog/lora/)
8. Cursor's blogpost on their wickedly effective RL models ðŸ‘‰ [*Improving Cursor Tab with online RL*](https://cursor.com/en-US/blog/tab-rl)
9. Nathan Lambert's blogpost [*Interconnects*](https://www.interconnects.ai/) ðŸ‘‰ frequently writes about reasoning models, agents, post-training, etc.
10. Hugging Face's post-training course [smol-course](https://huggingface.co/learn/smol-course/en/unit0/1) ðŸ‘‰ hands-on course on language model fine-tuning basics
11. Fei-Fei Li and team's AI agent paper ðŸ‘‰ [*Agent AI: Surveying the Horizons of Multimodal Interaction*](https://arxiv.org/abs/2401.03568) (2024) by Durante et al., *arXiv*.
12. NVIDIA uses RL in pretraining ðŸ‘‰ [*RLP: Reinforcement as a Pretraining Objective*](https://arxiv.org/abs/2510.01265) (2025) by Hatamizadeh et al., *arXiv*.
13. Meta's new self-play training strategy for Llama ðŸ‘‰ [*Language Self-Play For Data-Free Training*](https://arxiv.org/abs/2509.07414) (2025) by Kuba et al., *arXiv*.


### RLFH for RecSys
14. OneRec-V2 improves {{< backlink "generative_recommendation" "Generative Recommendation">}} post-training ðŸ‘‰ [*OneRec-V2 Technical Report*](https://arxiv.org/abs/2508.20900) (2025) by Zhou et al., *arXiv*.
15. Shopee's OnePiece integrates RL with traditional cascade RecSys ðŸ‘‰ [*OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System*](https://arxiv.org/abs/2509.18091) (2025) by Dai et al., *arXiv*.
16. Douyin improves user understanding with reasoning ðŸ‘‰ [*Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling*](https://arxiv.org/abs/2509.18864) (2025) by Li et al., *arXiv*.
17. Pinterest uses RL to tune value models of multi-objective rankers ðŸ‘‰ [*Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest*](https://arxiv.org/abs/2509.05292) (2025) by Xiao et al., *arXiv*.
